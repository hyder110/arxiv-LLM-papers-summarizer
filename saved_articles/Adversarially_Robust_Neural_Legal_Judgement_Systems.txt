Adversarially Robust Neural Legal Judgement
Systems
Rohit Raj1[0000−0003−1421−6286]and V Susheela Devi1
Dept. of CSA, Indian Institute of Science, Bangalore, Karnataka, India
{rohitr,susheela }@iisc.ac.in
Abstract. Legal judgment prediction is the task of predicting the out-
come of court cases on a given text description of facts of cases. These
tasks apply Natural Language Processing (NLP) techniques to predict le-
gal judgment results based on facts. Recently, large-scale public datasets
and NLP models have increased research in areas related to legal judg-
ment prediction systems. For such systems to be practically helpful, they
should be robust from adversarial attacks. Previous works mainly focus
on making a neural legal judgement system; however, significantly less
or no attention has been given to creating a robust Legal Judgement
Prediction(LJP) system. We implemented adversarial attacks on early
existing LJP systems and found that none of them could handle attacks.
In this work, we proposed an approach for making robust LJP systems.
Extensive experiments on four legal datasets show significant improve-
ments in our approach over the state-of-the-art LJP system in handling
adversarial attacks. To the best of our knowledge, we are the first to
increase the robustness of early-existing LJP systems
Keywords: Natural Language Processing ·Legal Judgement Prediction
·Robust Models
1 Introduction
Legal information is mainly in the form of text, so legal text processing is a grow-
ing area of research in NLP, such as crime classification [10], judgment prediction
[3], and summarization [6]. Countries like India, which are highly populated, have
many pending legal cases (approx 41 million ). In Brazil, only in the financial
domain, three hundred thirty-two thousand cases are in progress [20]. It is due
to multiple factors, including the unavailability of judges. Here legal judgment
prediction system can help in several steps like finding articles or the history of
a case, deciding penalty terms, etc. Also, legal judgment prediction is critical,
so a small error in the system may drastically affect judicial fairness.
Most of the researchers focused on making LJP systems by training NLP mod-
els(LSTM, BERT [1], legal-BERT [2]) on legal datasets. At the same time, very
little or no attention has been given to the robustness of these models.arXiv:2308.00165v1  [cs.CL]  31 Jul 20232 R. Raj et al.
We summarise our contribution as follows:
1. We implemented adversarial attacks on existing baseline models after fine-
tuning them on legal datasets and found that their performance decreased dras-
tically.
2. We suggested an algorithm for adversarial training for making robust legal
models.
3. We implemented training using data augmentation and adversarial training
methods to improve the model’s robustness.
2 Related Work
2.1 Legal Judgement System
Earlier legal judgment prediction systems involved linear models like SVM with
a bag of words as feature representation. In recent years, neural network [3]
methods have been used for legal domains due to the availability of NLP models
like RNN and BERT [1].
Most researchers used BiGRU-att [3], HAN [3], BERT [1] and Hier-BERT [4]
architecture to predict article violation on ECtHR [5] dataset. Legal-BERT [2]
is a domain-specific BERT pretrained on legal-documents corpora of approx 11.5
GB, used for legal judgement prediction. A number of other tasks like legal sum-
marization [6],prior case retrieval [8],legal QA [7] have been introduces.
In legal judgment prediction, the model must predict the final decision based on
case facts. Several datasets are introduced for training so that model can learn
specific words (for example, ‘restrictive covenant’, ‘promissory estoppel’, ‘tort’
and ‘novation’) that are being used in legal documents which are not used for
general purposes; for example, ECtHR [5], a multilabel dataset containing vio-
lated articles as the label. SCOTUS [5] contains cases of the American Supreme
Court and ILDC [9] contains cases of the Indian supreme court. All of these are
English datasets. However, datasets from different languages are also introduced
like Chinese [10], Swedes [11], Vietnamese [12].
2.2 Adversarial Training
Several adversarial training methods have been explored in NLP models to in-
crease their robustness. The models are trained on a dataset containing aug-
mented adversarial examples with the original dataset in adversarial training.
These adversarial examples are generated by applying adversarial attacks on pre-
trained models such that generated examples should be similar to the original
example, and the average human user cannot differentiate it from the natu-
ral one. Several adversarial attack mechanisms are being used in NLP, such as
BERT-Attack [15], BAE [14], A2T [16], TextFooler [13]. In these attacks, the
model finds essential words in the original text and replaces them with semanti-
cally similar words such that the label of the original text changes and generates
adversarial text that looks similar to the original text.Adversarially Robust Neural Legal Judgement Systems 3
2.3 Why adversarial training ?
To motivate the necessity of adversarial training, we implemented adversarial
attacks on existing baseline models(BERT [1], Legal-BERT [2], RoBERTa [18])
to check their robustness. We found that the performance of these models de-
creased drastically, as these models could not handle the adversarial attack. We
also implemented data augmentation using back-translation during training, but
the model’s performance was not improved much.
Legal judgment prediction is critical, so a slight variation in the input may af-
fect judgment fairness. So during deployment, if someone intentionally perturbs
the input sequence, prediction may change drastically. It is the main reason for
adversarial training.
3 Problem Formulation
Given a legal dataset, which contains a collection of legal documents, L=
{(X1, y1)..(XN, yN)}, where Xiis a legal text extracted from a legal document
andyi={1,2,3..K}. Here the length of each Xiis very large, and yiis a label
corresponding to that text.
The task is to design an LJP model M(.) that can:
–Predict correct class on legal documents of even large length.
–Perform correct prediction even if data is perturbed. Let X′be a perturbed
text, which may be perturbed intentionally or by mistake, then M(X′)→y,
where yis the correct label of that legal text.
4 Methods
In this section, we present our training workflow. We implemented three meth-
ods for training. These are 1) Fine-tuning Baseline models, 2) Training baseline
models with data augmentation 3) Adversarial training using augmenting ad-
versarial examples with natural examples. At the end of each method, we tested
our model’s robustness with adversarial attacks.
4.1 Fine tuning baseline model
In this approach, we have taken baseline models( BERT [1], Legal-BERT [2],
RoBERTa [18], Hierarchical Version of BERT [4], we have used a modified version
of Hier-BERT, denoting as H-BERT) and fine-tuned them on our downstream
tasks for legal judgment predictions. For BERT, Legal-BERT, and RoBERTa,
we have taken the last 512 tokens of each input text for training, as this approach
gave a better result. For H-BERT(modified Hierarchical Version of BERT), we
have divided text into chunks of 510 tokens such that two consecutive chunks
overlapped each other, here RoBERTa is taken as encoder, shown in Figure 2,4 R. Raj et al.
as it gives the best result. We have used cross-entropy as a loss function for
updating the gradient and evaluated model performance on accuracy.
4.2 Training using data-augmentation
In this approach, we first generated data using back-translation[24] and then
augmented it with training data. The algorithm for training is similar to Algo-
rithm 2, where in place of an adversarial example generator, we are using a data
augmenter.
We use the transformer model implemented by HuggingFace [21] for back-translation.
We first translate English to French and then translate it back to English from
French. We augment newly generated data such that it does not have any du-
plicate instances, and training is done similarly to approach 1.
4.3 Adversarial Training
In this approach, we generate adversarial examples from original legal document
datasets, then further augment these examples with legal document datasets and
train the model on this new dataset, i.e., Dnew=Dnat∪Dadv.
For generating an adversarial example from a text sample, first, we find the
importance score of each word in that sample using greedy search with word
importance ranking mechanism [25], where the importance of the word is de-
termined by how much heuristic score changes when a word is deleted from the
original input. i.e.,
Iwi=

My(X)−My(X/wi), ifM(X) =M(X/wi) =y,
(My(X)−My(X/wi)) + ( M′
y(X/wi)−M′
y(X)),ifM(X) =y, M(X/wi) =y′
andy̸=y′
(1)
Here we have followed the deletion approach for finding word importance be-
cause we are considering a common black-box setup which is usually followed
in a real-world scenario. We denote sentence after deletion of word wiasX/wi
={w1, ..., w i−1, wi+1, ...w n}and use My(.) to denote prediction score of model
for label y. Here Iwidenote importance score of word wiwhich is defined in
equation 1.
As shown in Algorithm 1, in lines 3-4, we select top-k words according to im-
portance score using equation 1 and generate ‘m’ synonyms for each word using
cosine-similarity and counter-fitted-word-embedding [23]. We then replace orig-
inal words with synonyms and make an adversarial example X′. Further, to
find the similarity of an adversarial sample X′to the original sample X, we useAdversarially Robust Neural Legal Judgement Systems 5
Universal Sentence Encoder(USE) [19]. We ignore the examples below a certain
threshold value. We have taken 0.5 as the threshold value for all of our exper-
iments. We have implemented all of our algorithms on top of the Textattack
framework.
Algorithm 1 Adversarial Example Generation from legal Sample
1:Input: Legal judgement prediction model M(θ), legal sample sentence X=
(w1, w2, ..wn), Perturbation Generator P(X, i) which replace wiwith certain per-
turbed word using counter-fitted-word-embedding
2:Output: Adversarial legal sample Xadv
3: Calculate importance score I(wi) of each word wiusing equation 1.
4: Take top-k words and rank them in decreasing order according to I(wi) and store
them in set R= (r1, r2..rk)
5:X′←X
6:fori=r1, ..rninRdo,
7: Xp←perturb the sentence X′using P(X′, i)
8: ifM(Xp)̸=ythen
9: ifsim(Xp, X)> threshold then ▷Check similarity of XandX′
10: X′←Xp
11: end if
12: end if
13:end for
14:return X′asXadv
For adversarial training, we first train the model using natural legal dataset
Dnatfor some iterations,i.e., nnat, after that we augment adversarial example
Dadvby using adversarial example generator, i.e., Dnew=Dnat∪Dadv. Further,
train the model on Dnewfor some iterations, i.e., nadv. Here adversarial loss
function is used to train the model.
LetLnatbe the loss function used for natural training, which is defined as a
cross-entropy loss function, i.e.,
Lnat=Lθ(X, y) (2)
Where Xis the input text and yis the label corresponding to it. If Aθ(X, y) is
the adversarial example generator, then the loss function for adversarial training
is defined as a cross-entropy loss function,i.e
Ladv=Lθ(Aθ(X, y), y) (3)
So our final loss function will be the combination of these two cross entropy loss
functions, i.e.,
L=argmin θ(Lnat+γLadv) (4)
Where γis a hyper-parameter that is used to change the importance of adver-
sarial training.6 R. Raj et al.
As shown in Algorithm 2, lines 3-6 represent the natural training of the model.
Lines 7-18 represent the adversarial training of the model, which is pre-trained in
lines 3-6. In lines 10-15, adversarial examples are generated. Line 16 represents
the augmentation of adversarial examples with natural data. Line 17 represents
the adversarial training step.
Algorithm 2 Adversarial Training of legal Models
1:Input: Legal judgement prediction model Mθ(.), Adversarial example generator
algorithm Aθ(X, y), legal dataset Dnat={X, y}m
i=1, natural training epochs nnat,
adversarial training epochs nadv
2:Output: Adversarially trained model
3: Randomly initialize θ
4:fori= 1,2..nnatdo,
5: Train Mθ(.) on dataset Dnatusing loss function from Equation (2).
6:end for
7:fori= 1,2..nadvdo,
8: Initialize set of adversarial legal dataset Dadv← {}
9: K←fraction of adversarial samples to be generated of natural dataset
10: for i = 1,2.. size(Dnat)do,
11: if size( Dadv)< K∗Dnatthen
12: Xadv←Aθ(X, y)
13: Dadv←Dadv∪ {Xadv, y}
14: end if
15: end for
16: Dnew←Dnat∪Dadv
17: Train Mθ(.) onDnewusing loss function from Equation (4).
18:end for
5 Experiments and Results
5.1 Datasets and Models
Datasets
ECHR [3]: It contains cases of the European Council of Human Rights
(ECHR). The dataset has 11.5k cases, of which 7100 cases are used for training,
1380 for development, and 2998 for the test set. The training and development
set contains cases from 1959-2013, and the test set contains cases from 2014-2018.
Total ECHR articles are 66; however, we have taken binary representation of
the ECHR dataset, in which label 1 is assigned if any article is violated; other-
wise, 0 is assigned.
SCOTUS[5] : It is a dataset of the US Supreme Court, which hears only com-
plex cases not well solved by lower courts. SCOTUS is a multi-class single labelAdversarially Robust Neural Legal Judgement Systems 7
dataset containing 14 classes, broad areas like Civil Rights, Criminal Procedure,
Economic Activity, etc. The SCOTUS cases are split into a 5k(1946-1982) train-
ing set, 1.4k(1982-1991) development set, and 1.4k(1991-2016) test set.
ILDC : Indian Legal Document Corpus (ILDC) is introduced by [9], which con-
tains cases of the Supreme court of India(SCI) from 1947 to 2020. It is a binary
classification dataset contain labels {0,1}. It has two versions. 1)ILDC-single
contains cases of a single petition filed, label 1 is assigned to cases whose petition
is accepted, and 0 is for not accepted. 2)ILDC-multi contains cases with mul-
tiple petitions filed. Here label 1 is assigned to cases with at least one petition
accepted; otherwise, label 0 is assigned. We have taken ILDC-multi for all of our
experiments.
As shown in Figure 1, legal text datasets have a substantial length. Here the
average length of samples of ECtHR is 1619 words, SCOTUS is 5853 words,
and ILDC is 3208 words. So it is far greater than a normal BERT architecture
input size. Therefore, we have implemented the modified Hierarchical Variant of
BERT (H-BERT) architecture.
Fig. 1. Text length distribution of different datasets, here horizontal axis shows length
of input texts and vertical axis show number of inputs
Models
BERT [1] is a pre-trained transformer-based language model. It is pre-trained
to perform masked language modeling and next sentence prediction.
Legal-BERT [2] is BERT pre-trained on English legal corpora, which contains
legislation, contracts, and court cases. Its configuration is the same as the origi-
nal BERT configuration. The sub-word vocabulary of Legal-BERT is built from
scratch.
Hierarchical Variant of BERT (H-BERT) Legal documents are usually of
large text length (shown in Figure 1), for example, ECtHR, ILDC, and SCO-
TUS. Transformer-based models can handle up to 512 sub-word units. So we
implemented an architecture similar to [4] in which we divided the text into the
chunk of 510 tokens such that two consecutive chunks have 100 overlapping to-
kens. Each chunk is sent through a BERT-Encoder to generate CLS embedding.8 R. Raj et al.
As shown in Figure 2, CLS embedding is passed to 1-dimensional convolution
and max-pooling layers. A further output of the max-pooling layer is passed to
Bi-directional LSTM and then the Dense layer. We have taken RoBERTa as an
encoder here as it gave best result among all other BERT-based models.
5.2 Implementation Details
For all tasks, we use pre-trained transformer-based BERT models from Hug-
gingface implementation. Each model output a 768-dimension vector regarding
each input text. The batch size is set to 8. Models trained using Adam optimizer
with 1e-5 learning rate for overall 10 epochs, which includes 3 epochs of natural
training and 7 epochs of adversarial training. We used LSTM of 100 units and
1-D CNN with 32 filters for H-BERT.
Fig. 2. Adversarially Robust Neural Legal Judgement Model (H-BERT model)
5.3 Results
Results after fine tuning
We have fine-tuned models naturally, i.e., without any augmentation (shown in
Table 1). For BERT, Legal-BERT, and RoBERTa, we have taken the last 512Adversarially Robust Neural Legal Judgement Systems 9
tokens of each sample as input, and for H-BERT, we have divided the text into
chunks, as mentioned in section ??. From empirical results, we can say that H-
BERT performs better than other models as H-BERT takes whole text examples,
whereas other models take only the last 512 tokens. Legal-BERT performs better
on ECHR and SCOTUS datasets as it is pre-trained on legal documents of Eu-
rope and America. The performance of RoBERTa on the ILDC dataset is better
than Legal-BERT because Legal-BERT is not pre-trained on the Indian-origin
legal dataset, whereas RoBERTa is pre-trained on general English datasets.
Table 1. Accuracy of Naturally trained models, ( FT) : Fine Tuning
Models ECHR SCOTUS ILDC multi
BERT( FT) 81.21 68.33 67.24
Legal-BERT( FT)83.42 76.47 63.37
RoBERTa( FT)79.27 71.69 71.26
H-BERT( FT) 81.03 78.02 74.89
Results after adversarial attack on naturally trained models
We feed 1000 adversarial examples generated from the adversarial examples
generator to naturally trained models to check their robustness against adver-
sarial attacks. As shown in Table 2, naturally trained models could not han-
dle adversarial attacks as their accuracy decreased drastically. The accuracy of
BERT decreased the most because it is not pre-trained on domain-specific (legal
domain) datasets, whereas, in the case of H-BERT, accuracy decreased least be-
cause H-BERT’s RoBERTa is pre-trained on general English datasets as well as
during training, it is considering whole legal text documents. In contrast, other
models consider only the last 512 words of each example. Legal-BERT is more
robust than BERT as it is pre-trained on legal datasets. RoBERTa is pre-trained
on a large corpus, so it can able to handle adversarial attacks better than Legal-
BERT.
Table 2. Accuracy of Naturally trained Models after attack, ( FT) : Fine Tuning
Models ECHR SCOTUS ILDC multi
BERT( FT) 33.12 36.42 22.59
Legal-BERT( FT)36.27 41.67 25.26
RoBERTa( FT)36.05 41.91 38.92
H-BERT( FT) 39.18 43.19 37.2110 R. Raj et al.
Results after adversarial attack on model trained using data-augmentation
We feed 1000 adversarial examples to a model trained using data augmentation
to check their robustness. As shown in Table 3, the accuracy of models is less
than that of naturally trained models but better than the accuracy of models
after the adversarial attack on naturally trained models. This is because we are
augmenting extra data, which is very similar to the original data except for a
few words for training. So due to this, the model is more diverse and can handle
some adversarial attacks. In most cases, H-BERT performs better than others
because it considers whole text data instead of the last 512 tokens.
Table 3. Accuracy after adversarial attack , ( DA) : Data Augmentation
Models ECHR SCOTUS ILDC multi
BERT( DA)(Ours) 38.03 41.12 32.56
Legal-BERT( DA)(Ours) 39.36 43.15 41.66
RoBERTa( DA)(Ours) 40.21 45.09 38.71
H-BERT( DA)(Ours) 46.10 45.02 42.03
Results after adversarial training
We implemented adversarial training using our Algorithm 2. As shown in Table
4, sometimes, the accuracy of an adversarially trained model is better than the
naturally trained model. The increase in accuracy is due to the augmentation
of adversarial examples, which creates more diversity during training. The per-
formance of the H-BERT model is best, while Legal-BERT is performing better
on ECHR and the SCOTUS dataset because it is pretrained on European and
American legal documents. Figure 3 shows an adversarial example on the ILDC
dataset during adversarial training. As we can see, slight change perturbation in
the text can change the label of an input. Due to the large length of text input,
we have shown only a small snippet of an example where an example is being
perturbed.
Table 4. Accuracy after adversarial training, ( AT) : Adversarial Training
Models ECHR SCOTUS ILDC multi
BERT( AT)(Ours) 79.23 69.07 65.56
Legal-BERT( AT)(Ours) 82.01 77.02 61.02
RoBERTa( AT)(Ours) 81.73 70.03 69.97
H-BERT( AT)(Ours) 83.67 78.09 71.53Adversarially Robust Neural Legal Judgement Systems 11
Fig. 3. original and adversarial examples of ILDC dataset while training BERT.
Results after adversarial attack on adversarially trained model
We feed 1000 adversarial examples, as earlier, to check the robustness of the
adversarially trained model. The results are surprising, as shown in Table 5.
Our models can handle most adversarial attacks. Accuracy is far better than
accuracy after the attack on naturally trained models. This is because, during
adversarial training, the model came across a diverse set of words that were not
present earlier.
As shown in Table 5, H-BERT is performing better than other models because it
is trained on the whole dataset. The BERT model performs worst as it is not pre-
trained on legal documents. The performance of Legal-BERT is not satisfactory
on ILDC because it is pre-trained on European and American legal documents,
which may contain words that are different from Indian legal documents.
Table 5. Accuracy after attack , ( AT) : Adversarial Training
Models ECHR SCOTUS ILDC multi
BERT( AT)(Ours) 58.96 52.38 54.46
Legal-BERT( AT)(Ours) 64.07 52.71 51.96
RoBERTa( AT)(Ours) 64.97 50.09 55.91
H-BERT( AT)(Ours) 69.32 61.53 58.29
6 Conclusion and Future work
In this work, we empirically proved that early existing legal models are not
adversarially robust, which is a significant risk for deploying them in work. We
also presented an adversarially robust model, which is trained on our adversarial
training algorithm for legal judgment prediction, which performs better than
state-of-the-art models in the presence of adversarial examples.
For future work, we suggest making robust legal models which can be applied to
Legal documents that are different from English. Also, one can work on zero-shot
and few-shot learning in legal domains, where very few resources are available
for legal documents.12 R. Raj et al.
