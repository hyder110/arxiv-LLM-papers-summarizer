Summary:
The paper presents SkipDecode, a method for efficient inference in autoregressive large language models (LLMs). LLMs incur high computational cost due to token-by-token generation, and existing early exit strategies cannot be readily applied for batch inference and Key-Value caching. SkipDecode overcomes these limitations by setting up a singular exit point for every token in a batch and ensures a monotonic decrease in exit points. It bypasses lower layers and allocates more computational resources to upper layers, allowing later tokens to benefit from earlier tokens. Experimental results show that SkipDecode can achieve 2x to 5x inference speedups with negligible regression in various tasks. It is compatible with batching and KV caching optimization techniques.

Bullet Points:
1. Autoregressive large language models (LLMs) have high computation cost and latency.
2. Early exit strategies have been proposed to reduce computational cost, but they have limitations with batch inference and Key-Value caching.
3. SkipDecode is a token-level early exit method that works seamlessly with batch inference and KV caching.
4. SkipDecode sets up a singular exit point for every token in a batch and ensures a monotonic decrease in exit points.
5. It bypasses lower layers and allocates more computational resources to upper layers.
6. Experimental results show that SkipDecode can achieve 2x to 5x inference speedups with negligible regression.
7. SkipDecode is directly compatible with batching and KV caching optimization techniques.
8. The method can be used to make LLMs more efficient on devices with limited resources.
9. SkipDecode helps to democratize AI by making LLMs more accessible and sustainable.
10. Future research can explore alternative decay functions and investigate the effect of the decaying policy on the prompt.

Keywords:
- Autoregressive large language models
- Early exit strategies
- Batch inference
- Key-Value caching
- SkipDecode
- Monotonic decrease
- Inference speedup
- Computational resources
- Batching optimization
- KV caching optimization