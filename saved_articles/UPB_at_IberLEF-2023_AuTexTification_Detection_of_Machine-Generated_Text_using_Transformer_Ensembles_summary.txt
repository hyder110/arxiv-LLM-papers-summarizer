Summary:
This paper presents the solutions developed by the UPB team for the AuTexTification shared task at IberLEF-2023. The task focused on detecting machine-generated text, specifically that generated by large language models (LLMs). The team participated in the first subtask of binary classification, aiming to distinguish between human-written and LLM-generated documents. They applied deep learning models based on Transformers and utilized techniques such as multi-task learning and virtual adversarial training to enhance their performance. The team submitted ensemble models, with the best-performing model achieving macro F1-scores of 66.63% for English and 67.10% for Spanish datasets.

Bullet points:
1. The UPB team participated in the AuTexTification shared task at IberLEF-2023.
2. The task focused on detecting machine-generated text by large language models.
3. The team developed deep learning models based on Transformers.
4. They utilized techniques like multi-task learning and virtual adversarial training.
5. The team submitted ensemble models for the task.
6. Their best-performing model achieved macro F1-scores of 66.63% for English and 67.10% for Spanish datasets.
7. The team experimented with shallow learning models like XGBoost and k-Nearest Neighbors.
8. They also explored features like readability scores and string kernels.
9. The results showed that Transformer-based models outperformed classical machine learning classifiers.
10. Future work could include optimizing hyperparameters, improving overfitting prevention, and increasing training epochs.

Keywords:
1. Machine-Generated Text
2. Transformer
3. Multi-Task Learning
4. Virtual Adversarial Training
5. Large Language Models
6. Ensemble Models
7. Deep Learning
8. Binary Classification
9. Shallow Learning
10. Text Detection