Beyond Generic: Enhancing Image Captioning with Real-World
Knowledge using Vision-Language Pre-Training Model
Kanzhi Cheng
National Key Laboratory for Novel
Software Technology,
Nanjing University,
Nanjing, China
chengkz@smail.nju.edu.cnWenpo Song
National Key Laboratory for Novel
Software Technology,
Nanjing University,
Nanjing, China
songwp@smail.nju.edu.cnZheng Ma
National Key Laboratory for Novel
Software Technology,
Nanjing University,
Nanjing, China
maz@smail.nju.edu.cn
Wenhao Zhu
National Key Laboratory for Novel
Software Technology,
Nanjing University,
Nanjing, China
zhuwh@smail.nju.edu.cnZixuan Zhu
University of Glasgow
Glasgow, Scotland
zzx349313@gmail.comJianbing Zhang∗
National Key Laboratory for Novel
Software Technology,
Nanjing University,
Nanjing, China
zjb@nju.edu.cn
ABSTRACT
Current captioning approaches tend to generate correct but "generic"
descriptions that lack real-world knowledge, e.g., named entities
and contextual information. Considering that Vision-Language
Pre-Training (VLP) models master massive such knowledge from
large-scale web-harvested data, it is promising to utilize the gen-
eralizability of VLP models to incorporate knowledge into image
descriptions. However, using VLP models faces challenges: zero-
shot inference suffers from knowledge hallucination that leads
to low-quality descriptions, but the generic bias in downstream
task fine-tuning hinders the VLP model from expressing knowl-
edge. To address these concerns, we propose a simple yet effective
method called Knowledge-guided Replay (K-Replay), which en-
ables the retention of pre-training knowledge during fine-tuning.
Our approach consists of two parts: (1) a knowledge prediction
task on automatically collected replay exemplars to continuously
awaken the VLP model’s memory about knowledge, thus prevent-
ing the model from collapsing into the generic pattern; (2) a knowl-
edge distillation constraint to improve the faithfulness of gener-
ated descriptions hence alleviating the knowledge hallucination. To
evaluate knowledge-enhanced descriptions, we construct a novel
captioning benchmark KnowCap, containing knowledge of land-
marks, famous brands, special foods and movie characters. Exper-
imental results show that our approach effectively incorporates
knowledge into descriptions, outperforming strong VLP baseline by
20.9 points (78.7→99.6) in CIDEr score and 20.5 percentage points
(34.0%→54.5%) in knowledge recognition accuracy. Our code and
data is available at https://github.com/njucckevin/KnowCap.
CCS CONCEPTS
•Information systems →Multimedia content creation .
KEYWORDS
Image Captioning; Vision-Language Pre-Training; Knowledge
∗Corresponding author.ACM Reference Format:
Kanzhi Cheng, Wenpo Song, Zheng Ma, Wenhao Zhu, Zixuan Zhu, and Jian-
bing Zhang. 2023. Beyond Generic: Enhancing Image Captioning with
Real-World Knowledge using Vision-Language Pre-Training Model. In Pro-
ceedings of the 31st ACM International Conference on Multimedia (MM ’23),
October 29-November 3, 2023, Ottawa, ON, Canada. ACM, New York, NY,
USA, 12 pages. https://doi.org/10.1145/3581783.3611987
1 INTRODUCTION
Image captioning aims to automatically describe the content of an
image using natural language [ 3,15,57]. It has a variety of applica-
tions such as assisting visually impaired people and multi-modal
content understanding for social media. Most existing approaches
generate image descriptions in a "generic" manner [ 43,69], i.e.,
describing only the common objects in an image, while lacking
real-world knowledge such as named entities and contextual infor-
mation. However, in many situations, such specific knowledge is
the key to understanding the image. Taking Figure 1 as an example,
the knowledge-enhanced description containing visual entity Daisy
Duck and contextual knowledge Disneyland might evoke the mem-
ory of a wonderful journey. In contrast, the generic description
generated by the advanced VLP+fine-tuning model seems tedious.
There have been several efforts attempt to incorporate knowl-
edge into image descriptions [ 43,55,62]. Most of them follow a
retrieve-and-generate methodology, first retrieving visual entities
in images using external resources (e.g. entity recognition model or
image metadata), and then adding the retrieved entities into descrip-
tions. Moreover, they require the collection of additional caption
data for supervised training. Such approaches are limited by the
capacity of external resources and the availability of annotated cap-
tion data. By contrast, powerful VLP models learn from large-scale
web-harvested data in an unsupervised manner, with the potential
to master infinite real-world knowledge. In this paper, we pave a
new way to leverage the VLP model’s generalizability to recognize
real-world knowledge and incorporate them into image description
generation. Using VLP models has significant advantages over the
aforementioned methods: (1) not limited by the capacity of externalarXiv:2308.01126v1  [cs.CV]  2 Aug 2023MM ’23, October 29-November 3, 2023, Ottawa, ON, Canada Kanzhi Cheng, Wenpo Song, Zheng Ma, Wenhao Zhu, Zixuan Zhu, and Jianbing Zhang
VLP:The mascots of the 2020Disneylandhalf marathon.VLP+fine-tuning:Acouple of people that are standing in the street.Knowledge-enriched:Daisy Duckand a fairy tale princess parade in Disneyland.
😴
😄
🤔VLPzero-shotThe mascots of the 2020Disneylandhalf marathon.
🤔VLP+fine-tuningAcouple of people that are standing in the street.
😴Knowledge-enhancedDaisy Duckand a fairy tale princess parade in Disneyland.
😄
Figure 1: Comparison of the result of VLP zero-shot, VLP+
fine-tuning and knowledge-enhanced description. We ex-
pect to properly integrate knowledge into descriptions while
avoiding knowledge hallucination caused by pre-training
noise. Knowledge words are marked by font colors and hal-
lucination words are marked by blue font background.
resources; (2) no extra data collection for training; (3) no specific
model architecture design.
Despite its potential, using the VLP model still faces some chal-
lenges. As depicted in Figure 1, the zero-shot inference of the VLP
model yields low-quality description, and the noisy correspondence
in pre-training image-text pairs causes the generation of harmful
knowledge hallucinations. Therefore, fine-tuning the VLP model
on a general captioning task is indispensable. However, we identi-
fied that fine-tuning on a captioning dataset with limited semantic
diversity leads to the generic bias, which further markedly inhibits
the VLP model’s ability to express knowledge.
To tackle the above challenges, we propose the Knowledge-
guided Replay (K-Replay), which guides the VLP model to retain
pre-training knowledge during downstream task fine-tuning. We
first filter a handful of images containing knowledge from the
pre-training data as knowledge-related replay exemplars, then a
sentence-level knowledge coverage loss is applied to evoke the VLP
model’s memory about knowledge thus preventing the model from
collapsing into the generic pattern. In addition, we implement a
knowledge distillation constraint using a fine-tuned VLP model
to encourage the generation of faithful descriptions, thus alleviat-
ing the knowledge hallucination problem. Notably, K-Replay has
a significant performance improvement in the replay unseen sce-
nario, demonstrating that it is not only learning the knowledge
from replay exemplars, but activating the VLP model to express the
knowledge it has mastered during pre-training.
To evaluate the quality of the generated knowledge-enhanced
descriptions, we constructed a new captioning benchmark, the
KnowCap dataset. The dataset contains 1400+ images and 4100+
descriptions, covering 240 knowledge categories of landmarks, fa-
mous brands, special foods and movie characters. We extensively
tested a series of representative captioning models on KnowCap.
Our approach outperforms strong VLP baselines by a large margin
in CIDEr score and knowledge recognition accuracy, establishing a
new state-of-the-art on KnowCap.
The main contributions of this work are summarized as follows:
•We propose to exploit VLP model’s generalizability for knowledge-
enhanced image captioning, which is more efficient compared to
previous retrieve-and-generate methods.•We find that the generic bias in downstream task fine-tuning
inhibits VLP models from expressing knowledge, thus designing
the K-Replay to continuously evoke the model’s memory about
knowledge, while reducing knowledge hallucination through
knowledge distillation constraint.
•We constructed the novel KnowCap dataset for evaluation, con-
sisting of more than 1400 images containing various types of
knowledge.
•Experimental results show that our approach can effectively
retain the knowledge mastered by the pre-trained model during
downstream task fine-tuning, finally outperforming a series of
strong baselines on the KnowCap dataset.
2 RELATED WORK
2.1 Vision-Language Pre-Training
Pre-training technique has revolutionized the NLP and CV re-
search community in recent years [ 7,18,22,48]. Meanwhile, Vision-
Language Pre-Training has been shown to significantly improve
performance on a wide range of uni-modal and multi-modal tasks
[30,31,47,60,61,68,70]. Recently, several studies show that the
Large Language Models (LLMs) can store and predict knowledge
about the world [ 20,26,41,46]. However, the real-world knowledge
contained in VLP is largely overlooked by existing research, with
only [ 10] exploring using a knowledge base to assist pre-training.
In this paper, we focus on generating knowledge-enhanced image
descriptions with the knowledge of VLP.
2.2 knowledge-enhanced Image Captioning
Previous image captioning systems adopted the encoder-decoder
approach and achieved success through carefully designed model
architectures [ 3,15,28,57,66] and training methods [ 5,13,38,
50]. Benefiting from pre-training techniques, the pre-training and
fine-tuning paradigm [ 30,32,61,68,70] further takes the model
capability to another level. Among them, autoregressive generative
models [ 14,59,60] achieved remarkable performance through the
simple sequence-to-sequence learning framework.
However, [ 38,58,62,69] revealed that existing captioning ap-
proaches suffer from the over-generic problem, which limits the
informativeness of descriptions. A line of effort attempted to incor-
porate knowledge into descriptions to alleviate this shortcoming.
These works adopt a retrieve-and-generate methodology, first re-
trieving entities contained in images using additional visual entity
recognition model [ 55,69] or image metadata [ 43,62], followed by
supervised training to integrate these entities into the decoding
process [ 43,54,69] or template caption [ 6,37]. These methods are
limited by the ability of external resources and the difficulty of label-
ing data. This paper solves these difficulties through VLP model’s
generalizability. Recently, Universal Captioner [ 14] and GIT [ 59]
have shown that VLP models have certain generalizability to in-
the-wild entities but lack quantitative analysis. In this paper, we
construct a benchmark for knowledge-enhanced image captioning
task and demonstrate the superiority of our approach.
2.3 Catastrophic Forgetting
We argue that the difficulty of expressing knowledge in VLP models
is associated with catastrophic forgetting [ 21,40] when fine-tuningBeyond Generic: Enhancing Image Captioning with Real-World Knowledge MM ’23, October 29-November 3, 2023, Ottawa, ON, Canada
VLPModelVLPModel
A group of people posing for a picture in front of the Taj Mahal.InferenceFine-tuningagroupofpeople······abuildingReferenceforvanillafine-tuningVLPModelfourpeopletakepicture······abuildingPseudo-captionforunsupervisedtraining𝑪𝑬𝑳𝒐𝒔𝒔
𝑲𝑫𝑳𝒐𝒔𝒔𝑲𝒏𝒐𝒘𝒍𝒆𝒅𝒈𝒆𝑪𝒐𝒗𝒆𝒓𝒂𝒈𝒆𝑳𝒐𝒔𝒔𝑷(“𝑪𝒂𝒑𝒊𝒕𝒐𝒍𝑯𝒊𝒍𝒍”)!
ReplayExemplars
COCOSamples
K=“EiffelTower”K=“CapitolHill”···K-Replay
Figure 2: Illustration of our K-Replay method. K-Replay is performed simultaneously with downstream task fine-tuning to
activate the model’s memory about knowledge. Notice that the knowledge generated in the inference stage (Taj Mahal) does
not need to appear in the replay exemplars.
downstream tasks. Methods designed to alleviate catastrophic for-
getting fall into three categories [ 42]:regularization ,replay , and
dynamic architecture . Regularization methods mitigate the forget-
ting of prior knowledge by limiting the change in model parameters,
using regular terms [ 2,11,27] or knowledge distillation [ 8,63]. Re-
play methods store exemplars from the past task and continuously
review them while learning new tasks [ 8,42,49]. Dynamic archi-
tecture methods use different modules to learn different tasks and
thus avoid forgetting [ 17,39,51]. Other efforts seek an alternative
strategy to alleviate the forgetting of pre-trained models by fine-
tuning part of the parameters to enhance generalizability , such as
adapter [ 19,24], prompt tuning [ 29,33] and child tuning [ 65]. In
Section 6.2 we systematically compare the effects of these methods.
3 APPROACH
3.1 Overview
Visual-Language Pre-Training allows the VLP model to learn a large
amount of knowledge from web-scale data. However, knowledge
hallucination and the generic bias in downstream task fine-tuning
invisibly inhibit the expression of such knowledge. To address these
concerns, we propose K-Replay, which continuously stimulates the
model to express knowledge while downstream task fine-tuning,
and reduces the hallucination through knowledge distillation con-
strain. Note that K-Replay does not introduce additional model
design, but rather seeks to guide the behavior of the VLP model
through learning tasks, thus enabling the retention of pre-trained
knowledge during downstream task fine-tuning.
Given a VLP model 𝑃and a downstream captioning dataset
𝐷𝐶={(𝑥𝑖,𝑦𝑖)|𝑖}, with the𝑖-th image𝑥𝑖along with its correspond-
ing caption𝑦𝑖, vanilla fine-tuning trains 𝑃on dataset𝐷𝐶to obtain
an image captioning model 𝑃𝐶. In addition, we automatically filter
a small portion of knowledge-related samples using knowledge
keywords from the pre-training data to constitute the replay ex-
emplars set 𝐷𝐾={(𝑥𝑖,𝑘𝑖)|𝑖}, with the𝑖-th image𝑥𝑖along withits knowledge keyword 𝑘𝑖(details in Section 3.3). Next, our ap-
proach uses 𝐷𝐶and𝐷𝐾to train the VLP model 𝑃for generating
knowledge-enhanced image descriptions.
The overview of proposed K-Replay method is presented in Fig-
ure 2. It is implemented in three main specially designed parts: (1)
a knowledge prediction task on the replay exemplar to awaken the
knowledge of the VLP model (in Section 3.3); (2) a knowledge dis-
tillation constraint to alleviate the knowledge hallucination caused
by pre-training noise (in Section 3.4); and (3) simultaneous training
on𝐷𝐶and𝐷𝐾achieved by constructing pseudo-caption data (in
Section 3.5).
3.2 Vanilla Fine-tuning
We start by introducing the process of vanilla fine-tuning the VLP
model𝑃to the downstream captioning task. Formally, given the
captioning dataset 𝐷𝐶={(𝑥𝑖,𝑦𝑖)|𝑖}, where𝑦={𝑤1,𝑤2,...𝑤𝑇}rep-
resents the word sequence of the reference description of length T,
by omitting the sample subscript 𝑖, as default in the later. Then, we
model the image captioning task as an end-to-end sequence gener-
ation paradigm, by maximizing the probability of image-caption
pairs(𝑥,𝑦)in𝐷𝐶:
arg max
𝜃𝐸(𝑥,𝑦)∼𝐷𝐶𝑇Ö
𝑡=1𝑝(𝑤𝑡|𝑤𝜏<𝑡,𝑥;𝜃), (1)
where𝜃is the parameters of VLP model 𝑃. Finally, the loss of vanilla
fine-tuning is the cross-entropy between generated caption and
groundtruth:
L𝑐𝑒=−1
𝑇𝑇∑︁
𝑡=1𝑙𝑜𝑔𝑝(𝑤𝑡|𝑤𝜏<𝑡,𝑥;𝑃). (2)
3.3 Knowledge Prediction Task
We aim to preserve the knowledge learned by the VLP model during
pre-training while downstream task fine-tuning. A straightforwardMM ’23, October 29-November 3, 2023, Ottawa, ON, Canada Kanzhi Cheng, Wenpo Song, Zheng Ma, Wenhao Zhu, Zixuan Zhu, and Jianbing Zhang
idea is to retrain the pre-training data while fine-tuning. However,
the noisy correspondence in web-harvested image-text pairs causes
the failure of joint training. To this end, our K-Replay method
excludes the negative effect of noise by filtering knowledge-related
exemplars and elaborating a knowledge prediction task on them,
finally enabling continuous review of pre-trained knowledge while
fine-tuning, thus preventing the VLP model from forgetting.
We first select representative knowledge-related images from nu-
merous pre-training image-text pairs by detecting whether the text
contains knowledge keyword 𝑘, where𝑘belongs to a predefined set
of knowledge keywords 𝐾={𝑘1,𝑘2,...,𝑘𝑀}of size𝑀, eventually
compose the replay exemplar set 𝐷𝐾={(𝑥𝑖,𝑘𝑖)|𝑖},𝑘𝑖∈𝐾. The ef-
fect of keyword number 𝑀and sample size of 𝐷𝐾on performance
will be discussed in Section 6.3.
Next we use a knowledge prediction task to stimulate the model
to express knowledge. For the image-keyword pair (𝑥,𝑘)in𝐷𝐾,
where𝑘={𝑤𝑘
1,𝑤𝑘
2,...,𝑤𝑘
𝑁}represent BPE tokenization, we en-
courage the model to include the knowledge keyword 𝑘in the
generated sentence ˆ𝑦=𝑃(𝑥). To do so, we design a sentence-level
knowledge coverage loss, by formalizing the coverage of knowl-
edge keyword as the multi-label classification problem. We first
accumulate the probability distributions of decoding process to
obtain the generation probability of each BPE token in 𝑘:
𝑝(𝑤𝑘
𝑖)=𝑇∑︁
𝑡=1𝑝(𝑤𝑘
𝑖|𝑤𝜏<𝑡,𝑥;𝑃). (3)
Then we calculate the cross-entropy of the knowledge keyword:
L𝑐𝑜𝑣=−𝑁∑︁
𝑖=1𝑙𝑜𝑔𝜎
𝑝(𝑤𝑘
𝑖)
, (4)
where𝜎is the sigmoid normalization. In practice, we add a degen-
eration penalty to limit the accumulation of keyword probability,
thus preventing the generation of repetitive knowledge keywords:
L𝑟𝑒𝑝=𝑁∑︁
𝑖=1
1−𝑝(𝑤𝑘
𝑖)2. (5)
Finally, the loss of knowledge prediction task is:
L𝑘𝑛𝑜𝑤 =L𝑐𝑜𝑣+L𝑟𝑒𝑝. (6)
3.4 Knowledge Distillation Constraint
The knowledge prediction task activates the model’s memory of
pre-training knowledge, but also brings the harmful knowledge
hallucination caused by pre-training noise [ 16,25], as shown in
Figure 1. In contrast, the vanilla fine-tuned model tends to be free of
hallucination but generic. In this regard, we introduce a knowledge
distillation constraint to suppress the hallucination by distilling the
output of the vanilla fine-tuned model 𝑃𝐶.
We adopt the widely used response-based knowledge distillation
[23,36]. Given the vanilla fine-tuned teacher model 𝑃𝐶and the
student model 𝑃,𝑧𝑡and𝑧𝑠denote the logits of the teacher model 𝑃𝐶
and student model 𝑃at each moment, respectively. The knowledge
distillation loss is calculated as follows:
L𝑘𝑑=𝐷𝑘𝑙
𝜑(𝑧𝑡),𝜑(𝑧𝑠)
, 𝜑(𝑧𝑖)=𝑒𝑥𝑝(𝑧𝑖/𝑇)Í
𝑗𝑒𝑥𝑝(𝑧𝑗/𝑇), (7)Algorithm 1: Knowledge-guided Replay
Input: mini-batch training samples 𝑑𝑚∈𝐷𝐶∪𝐷𝐾,
𝑑𝑚={(𝑥𝑖,𝑦𝑖)|𝑖}∪{(𝑥𝑗,𝑘𝑗)|𝑗}; VLP model 𝑃𝜃,
vanilla fine-tuned VLP model 𝑃𝐶; hyperparameters
𝜆𝑘𝑛𝑜𝑤 and𝜆𝑘𝑑.
Output: updated model parameter 𝜃′.
1for(𝑥,𝑘)in{(𝑥𝑗,𝑘𝑗)|𝑗}do
2 generate pseudo caption 𝑦𝑝=𝑃𝜃(𝑥)
3 teacher model forward ˆ𝑧=𝑃𝐶(𝑥,𝑦𝑝)
4end
5𝑑𝑚={(𝑥𝑖,𝑦𝑖)|𝑖}∪{(𝑥𝑗,𝑦𝑝
𝑗,𝑘𝑗)|𝑗}feeding into model for
forward: z=𝑃𝜃(x,y)
6for
𝑥,𝑦,(𝑘),𝑧
in𝑑𝑚,zdo
7 if𝑥∈𝐷𝐶then
8 calculateL𝑐𝑒(𝑧,𝑦)
9 end
10 if𝑥∈𝐷𝐾then
11 calculateL𝑘𝑛𝑜𝑤(𝑧,𝑘)andL𝑘𝑑(𝑧,ˆ𝑧)
12 end
13end
14𝑙𝑜𝑠𝑠=L𝑐𝑒+𝜆𝑘𝑛𝑜𝑤L𝑘𝑛𝑜𝑤+𝜆𝑘𝑑L𝑘𝑑
15𝜃′←𝑢𝑝𝑑𝑎𝑡𝑒(𝑙𝑜𝑠𝑠)
where𝐷𝑘𝑙is the Kullback-Leibler divergence between two prob-
ability distributions, 𝑧𝑖is the logit for the 𝑖-th class and 𝑇is the
temperature hyperparameter.
In this way, the knowledge distillation loss acts as a regulariza-
tion term, guiding the model to generate faithful image descriptions,
thus alleviating the knowledge hallucination problem.
3.5 Pseudo-Caption Training
One question remains here, how can replay data 𝐷𝐾={(𝑥𝑗,𝑘𝑗)|𝑗}
without reference sentences be trained together with captioning
data𝐷𝐶={(𝑥𝑖,𝑦𝑖)|𝑖}containing reference sentences? We design a
simple and effective scheme, Pesudo-Caption Training, that enables
joint training using both supervised caption data 𝐷𝐶and weakly
supervised data 𝐷𝐾. We first generates pseudo-captions 𝑦𝑝for
images𝑥∈𝐷𝐾before forward propagation, after which we are
able to integrate pseudo-captions 𝑦𝑝and real reference sentences 𝑦
together as input to decoder for forward propagation. Finally, we
calculate the corresponding losses for each the two types of samples.
In practice, we generate pseudo-caption using greedy search.
Finally, our overall objective function is as follows:
L=L𝑐𝑒+𝜆𝑘𝑛𝑜𝑤L𝑘𝑛𝑜𝑤+𝜆𝑘𝑑L𝑘𝑑, (8)
where𝜆𝑘𝑛𝑜𝑤 and𝜆𝑘𝑑are hyperparameters that balance the losses.
Algorithm 1 gives the procedures of K-Replay for updating the
model parameters within a mini-batch.
4 THE KNOWCAP DATASET
For the purposes of evaluating the ability of different methods to in-
corporate knowledge into image descriptions, we require a dataset
of images with knowledge-enhanced captions. Standard image cap-
tioning datasets such as MSCOCO [ 35], Flickr [ 67] and nocaps [ 1]Beyond Generic: Enhancing Image Captioning with Real-World Knowledge MM ’23, October 29-November 3, 2023, Ottawa, ON, Canada
Table 1: High-frequency knowledge categories of KnowCap.
Categories High Frequency Keywords
landmarks White House, Eiffel Tower, Grand Canyon, ...
famous brands iPhone, Ford, Chevrolet, FIFA World Cup, ...
special foods tacos, sushi, ramen, dumplings, fried rice, ...
movie characters Batman, Joker, Barbie, Superman, Spider-Man, ...
contain images of common objects, yet little real-world knowledge
like named entities are involved. There are several datasets con-
sider descriptions with specific knowledge, however, these datasets
focus on specific domains, such as geographic knowledge [ 43] and
news domain [ 6]. More importantly, their task requires additional
image metadata to be provided along with image [ 43,69], and thus
is not suitable for evaluating the ability of generating knowledge-
enhanced image descriptions via VLP model’s generalizability.
We introduce KnowCap, a new dataset for the evaluation of
knowledge-enhanced image captioning. To collect images contain-
ing various types of common knowledge, we first guided chatgpt1
to give some keywords of world-famous landmarks, brands, special
foods and movie characters, and finally 240 categories (89 land-
marks, 71 famous brands, 35 special foods and 45 movie characters)
were obtained after filtering. By inspection, more than 96% of them
appear over 50 times in the most frequently used Vision-Language
Pre-training dataset CC12M [ 9], and thus have the potential to be
mastered by VLP models. Table 1 displays a sample of the high-
frequency knowledge categories. Next, we used these 240 keywords
to crawl over 20,000 images from the Internet, from which three
expert annotators eventually filtered 1424 images suitable for the
image captioning task (e.g., containing multiple objects, complex
scenes).
For each image, we collected 3 reference descriptions carefully
written by human annotators. The annotation process is similar to
[12], except that we provide the knowledge keyword corresponding
to the image. We finally obtained 4156 reference descriptions. Sta-
tistically, the average length of these sentences is 12.27 (compared
to 11.30 in MSCOCO), and more than 95% of them contain at least
one knowledge keyword, which is almost absent in MSCOCO. We
provide sample annotations in Appendix A.
5 EXPERIMENTS
5.1 Experimental Settings
To demonstrate the effectiveness of our K-Replay method, we con-
ducted experiments on three baselines VLP models: (1) OFA [ 60]
unifies various tasks of different modalities into a simple sequence-
to-sequence learning framework; (2) BLIP [ 31] transfers flexibly
to both vision-language understanding and generation tasks and
proposes to filter and augment the noisy web data; (3) GIT [ 59]
designs a generative image-to-text transformer and verifies its supe-
rior generalizability. We apply K-Replay in the fine-tuning stage of
these VLP models and compare it with vanilla fine-tuning baselines.
1https://chat.openai.com/We evaluate the performance on MSCOCO and KnowCap datasets.
The MSCOCO dataset contains 123287 images, each with 5 human-
annotated reference sentences. Following previous work, we split
each 5000 for validation and testing and the rest for training. Our
KnowCap dataset consists of 1424 images covering 240 classes of
knowledge, each with 3 human-annotated reference sentences. We
randomly split into 424 for validation and 1000 for testing.
To make fair comparison, we use the 𝑙𝑎𝑟𝑔𝑒 versions of OFA, BLIP
and GIT, which have similar model sizes. For vanilla fine-tuning
baselines, we use the official checkpoints provided for testing. For
our K-Replay method, we report the test results for the highest-
scoring models on the KnowCap validation set. In knowledge dis-
tillation, the temperature 𝑇is16.0and loss weight 𝜆𝑘𝑑is1.0. The
weight𝜆𝑘𝑛𝑜𝑤 of knowledge prediction loss are set to 1.0,0.05,0.1
in OFA, BLIP and GIT, respectively.
5.2 Evaluation Metrics
We first evaluate the accuracy of generated captions using stan-
dard image captioning metrics, including BLEU [ 45], METEOR
[4], ROUGE [ 34] and CIDEr [ 56]. To measure whether the gener-
ated descriptions contain knowledge, we additionally calculate the
knowledge recognition accuracy (RecogAcc) on KnowCap, which
represents the proportion of generated descriptions that contain
valid knowledge keywords.
6 RESULTS AND ANALYSIS
6.1 Results on KnowCap and MSCOCO
In this section, we first test the performance of a series of existing
image captioning methods on the KnowCap dataset, and then we
analyze the improvement introduced by our K-Replay method. We
report the main results in Table 2.
Only the VLP model can incorporate knowledge into descrip-
tions. We compare the capabilities of a range of image captioning
methods on KnowCap: (1) traditional image captioning models NIC
[57], SAT [ 64] and the recently proposed CLIP-Trans [ 52]; (2) zero-
shot image captioning approaches MAGIC [ 53] and CapDec [ 44]; (3)
Vision-Language Pre-training models OFA [ 60], BLIP [ 31] and GIT
[59], including both zero-shot inference and downstream task fine-
tuning versions. The results are shown in Table 2, the traditional
image captioning models fail to express knowledge due to the limi-
tations of training data, resulting in extremely low RecogAcc. The
two zero-shot image captioning approaches MAGIC and CapDec
also do not demonstrate generalizability on KnowCap, which may
be because they both train the language model on MSCOCO text
data thus limiting the generation of knowledge.
In this situation, Vision-Language Pre-training becomes the only
way to incorporate knowledge into the image descriptions. VLP
zero-shot inference is able to generate knowledge (as reflected by
RecogAcc), but its noisy language model leads to relatively low
standard metrics such as CIDEr. In contrast, the VLP+fine-tuning
models could generate fluent descriptions (as evidenced by the
increased standard metrics). However the inhibition of expressing
knowledge by downstream task fine-tuning leads to a decrease in
RecogAcc. For example, the RecogAcc of OFA+fine-tuning decreases
from 43.8%to34.0%compared to OFA zero-shot, and BLIP similarly
drops from 34.6%to21.9%. GIT is an exception, which we believeMM ’23, October 29-November 3, 2023, Ottawa, ON, Canada Kanzhi Cheng, Wenpo Song, Zheng Ma, Wenhao Zhu, Zixuan Zhu, and Jianbing Zhang
Table 2: Results of different methods on KnowCap and MSCOCO. We use the official released checkpoints for all methods
except NIC, SAT and CLIP-Trans. B-n, M, R, C and Rec are abbreviations for BLEU-n, METEOR, ROUGE, CIDEr and RecogAcc,
respectively. The best results in each column are underlined , and the improvements achieved by our method are bold.
MethodKnowCap MSCOCO
B1 B2 B3 B4 M R C Rec B1 B2 B3 B4 M R C
Traditional
Image
CaptioningNIC [57] 40.3 22.2 13.1 8.1 10.3 27.6 13.9 2.8% 71.6 54.2 39.5 28.6 24.2 52.1 91.7
SAT [64] 43.4 25.8 16.3 10.7 11.6 30.2 20.4 3.7% 72.7 55.5 40.9 30.1 25.3 53.3 98.4
CLIP-Trans [52] 47.3 29.3 19.3 13.0 13.4 33.3 28.1 4.6% 76.3 60.2 46.7 36.1 28.0 56.8 116.4
Zero-shot
Image
CaptioningMAGIC [53] 36.5 17.0 8.6 4.5 9.0 24.4 12.4 3.4% 56.4 35.2 21.0 12.6 17.3 39.6 48.5
CapDec [44] 42.4 24.6 15.2 9.7 12.3 30.5 20.7 3.5% 68.3 50.7 36.9 26.8 25.2 51.3 92.5
OFA [60] zero-shot 39.2 25.7 18.3 13.4 13.3 30.0 50.0 43.8% 53.0 37.2 26.2 18.5 19.1 38.9 63.2
BLIP [31] zero-shot 44.6 31.6 23.3 17.3 15.6 38.2 58.1 32.7% 55.7 46.3 36.7 28.3 23.4 53.1 97.6
GIT [59] zero-shot 33.9 21.6 14.8 10.6 12.2 29.7 41.9 34.6% 37.3 28.0 21.2 16.3 17.4 39.2 65.8
VLP+
fine-tuning
(+K-Replay)OFA+fine-tuning 58.0 41.0 30.3 22.7 19.6 42.8 78.7 34.0% 80.4 65.7 52.1 40.9 31.4 61.0 139.3
+K-Replay (ours) 59.7 43.6 32.8 25.1 21.3 45.1 99.6 54.5% 79.8 65.0 51.4 40.1 31.6 60.8 138.1
BLIP+fine-tuning 56.4 39.1 28.2 20.4 17.9 40.8 62.7 21.9% 79.6 64.8 51.4 40.7 30.9 60.2 135.6
+K-Replay (ours) 57.4 40.9 30.0 22.3 19.6 43.3 81.8 50.3% 80.6 65.8 52.2 41.1 30.5 60.2 135.9
GIT+fine-tuning 46.5 32.1 23.3 17.1 16.7 37.0 70.5 55.6% 80.0 65.3 52.1 41.3 30.8 60.5 137.1
+K-Replay (ours) 48.1 33.6 24.3 17.8 17.4 37.6 78.4 68.0% 80.9 66.1 52.7 41.5 30.9 60.7 138.5
is because GIT zero-shot strongly prefers short sentences, leading
to a significant underestimation of its RecogAcc.
In addition, an interesting finding is that the RecogAcc of GIT+fine-
tuning ( 55.6%) is significantly higher than OFA and BLIP, especially
considering that GIT uses less pre-training data. We suggest that
this is because GIT uses CLIP [ 47] as the visual encoder, and the
powerful zero-shot transfer capability of CLIP contributes to the
excellent generalizability of GIT.
K-Replay further significantly improves VLP model’s abil-
ity to express knowledge. As displayed in Table 2, applying our
K-Replay method on all three baselines VLP models resulted in
significant performance boosts on KnowCap, where OFA improved
by20.9on CIDEr and BLIP improved by up to 28.4%on RecogAcc.
To verify the effectiveness of our method under the unseen scenario
of the replay exemplars, we divided a part of the KnowCap test
(240 categories, 1000 images) that never appears in the replay data
as KnowCap Unseen (120 categories, 520 images). The results are
shown in Table 3, where K-Replay still has a considerable perfor-
mance gain in the unseen scenario. This indicates that K-Replay
is not merely learning knowledge on replay exemplars, but awak-
ening the inherent power of VLP models to express knowledge
through the learning task on replay exemplars.
It is worth noting that the VLP models maintain its performance
on MSCOCO after using the K-Replay , which indicates that K-
Replay guided the model learns to automatically determine whether
to express knowledge based on the image content. Ultimately, our
K-Replay method is able to both accurately describe the image
content and appropriately incorporate its acquired knowledge.
6.2 Comparison with Prior Methods
The inhibition of the VLP model’s ability to express knowledge
can be seen as a result of catastrophic forgetting. Namely, VLP
models forget the real-world knowledge acquired by pre-trainingTable 3: Results of K-Replay in replay unseen scenario. The
best results in each column are underlined , and the improve-
ments achieved by our method are bold.
MethodKnowCap Unseen
B1 B4 M R C Rec
OFA [60]+fine-tuning 58.0 23.6 19.5 42.7 77.1 33.7%
+K-Replay (ours) 59.0 25.4 20.8 44.3 94.0 52.9%
BLIP [31]+fine-tuning 56.2 20.5 17.8 40.6 64.5 25.6%
+K-Replay (ours) 57.2 21.3 19.1 42.8 74.4 47.9%
GIT [59]+fine-tuning 46.3 17.6 16.6 36.7 67.1 59.2%
+K-Replay (ours) 47.4 17.8 17.1 37.0 71.9 68.8%
when learning a new downstream task (image captioning), which
eventually limits the model’s generalizability. In this section, we
review and compare some previous work on catastrophic forgetting.
•EWC [ 27] adds a regularization term for parameters that are
important for the old task when learning a new task, thus
maintaining performance on older tasks.
•Recall and Learn [ 11] is similar to EWC but uses consis-
tent regularization weights to mitigate forgetting when fine-
tuning the pre-trained language model.
•Child-Tuning [ 65] only fine-tunes a small subset of parame-
ters of large pre-trained models that are highly relevant to
the downstream task and thus improve generalizability.
•Adapter [ 19] adds an additional bottleneck layer while keep-
ing most of the parameters the same to prevent overfitting.
Here we take the pre-training image-conditioned language mod-
eling task as the older task and the downstream image captioning
task as the new task.
We experiment the effect of the above methods on the VLP model
OFA and compare them with our K-Replay method, the results areBeyond Generic: Enhancing Image Captioning with Real-World Knowledge MM ’23, October 29-November 3, 2023, Ottawa, ON, Canada
Figure 3: Effect of the number (left two figures) and categories (right two figures) of replay exemplar set on the performance of
K-Replay. The results on KnowCap and KnowCap Unseen are shown in blue and red, respectively. The shaded area represents
the standard deviation and the dashed line at bottom represents the performance of the vanilla fine-tuning baseline. K-Replay
achieves considerable performance gains using only 120 samples or 20 categories of replay exemplars.
Table 4: Results of different approaches for overcoming cat-
astrophic forgetting. The best results obtained by our K-
Replay method are bold.
MethodKnowCap
B1 B4 M R C Rec
vanilla fine-tuning 58.0 22.7 19.6 42.8 78.7 34.0%
EWC [27] 56.9 21.8 19.1 42.0 73.6 30.4%
Recall and Learn [11] 53.7 20.1 18.1 39.3 70.6 37.2%
Child-Tuning [65] 55.8 21.7 18.8 41.5 74.7 33.8%
Adapter [19] 54.4 20.5 17.6 40.1 63.7 30.4%
K-Replay (ours) 59.7 25.1 21.3 45.1 99.6 54.5%
shown in Table 4. These methods fail to achieve improvements on
CIDEr or RecogAcc, indicating their inability to preserve the mem-
ory of VLP model’s knowledge while fine-tuning the downstream
task. Why do these methods fail? We believe that VLP models have
a wide range of capabilities and the ones we want to preserve are
mixed in it, so it is difficult to activate the model’s ability to express
knowledge simply by limiting the VLP model’s parameter changes.
In contrast, our K-Replay approach finds a new path to guide the
behavior of the VLP model through carefully designed learning
tasks, ultimately enabling the retention of pre-training model’s
knowledge during downstream task fine-tuning.
6.3 Effect of Replay Exemplar Set
The K-Replay method relies on a replay exemplar set 𝐷𝐾={(𝑥𝑖,𝑘𝑖)|𝑖},
𝑘𝑖∈{𝑘1,𝑘2,...,𝑘𝑀}, which contains a total of 𝑁samples from 𝑀
knowledge categories filtered from the pre-training data. In this
section, we systematically analyze the impact of the number 𝑁
and the categories 𝑀of replay exemplars on the performance of
K-Replay.
We apply the K-Replay method on OFA for our experiments. The
results in Table 2 use a collection of 5000 samples of 120 knowledge
categories as the replay exemplar set. Now we attempt to change
the number of samples and knowledge categories.
Effect of the number of Replay Exemplar. We experimented
with gradually reducing the number of the replay exemplar set
from 5000 to 120 (i.e.,retaining only 1 sample per category), whileconsistently including 120 knowledge categories. As depicted in
Figure 3 left, with the number of replay exemplars decreasing, there
is a clear drop in CIDEr and RecogAcc of the K-Replay method,
which indicates that the model’s capacity to express knowledge
declines. However, it is worth noting that even with a very limited
number of replay exemplars (e.g., 120), the K-Replay method still
has a significant performance gain compared to the vanilla fine-
tuning baseline, suggesting that a handful of exemplars is sufficient
to activate the memory of the VLP model.
Effect of the categories of Replay Exemplar. We explored de-
creasing the knowledge categories contained in the replay exemplar
set from 120 to 10, while keeping a total of 5000 samples. The re-
sults are shown in Figure 3 right, both CIDEr and RecogAcc of
K-Replay decrease as the knowledge categories of replay exemplars
are reduced. The K-Replay method still has a favorable performance
improvement over the vanilla fine-tuning baseline when only 20
categories are used. However, when only 10 categories of knowl-
edge are selected as replay exemplars, the model performance drops
sharply, which we believe is caused by too few categories leading to
model’s overfitting and thus forgetting other types of knowledge.
6.4 Analysis
Ablation Study. In Table 5, we conduct ablation experiments to
study the effects of the proposed knowledge prediction task and
knowledge distillation constraint. Experiments were performed on
OFA. We replace the elaborate knowledge prediction task with a
straightforward calculation of the cross-entropy loss of the web-
crawled text in the pre-training corpus, denoted as w/o Pred . The
noise in the web-crawled text causes substantial performance degra-
dation, which validates the necessity of designing the knowledge
prediction task to evoke knowledge. We also observe that the model
without knowledge distillation constraint (denoted as w/o KD ) per-
forms worse on standard metrics such as BLEU and CIDEr, which
Table 5: Ablation study result.
MethodKnowCap
B1 B4 M R C Rec
K-Replay 59.7 25.1 21.3 45.1 99.6 54.5%
w/o KD 59.2 24.6 21.1 44.7 98.4 54.5%
w/o Pred 58.0 22.6 19.7 43.0 78.8 33.6%MM ’23, October 29-November 3, 2023, Ottawa, ON, Canada Kanzhi Cheng, Wenpo Song, Zheng Ma, Wenhao Zhu, Zixuan Zhu, and Jianbing Zhang
VLP+fine-tuning: A group of people walking in front of a store.K-Replay: A group of people walking in front of a hsbcbank.VLP+fine-tuning: A woman standing in front of a white car.K-Replay: A woman standing in front of a Bentleycar in Dubai.
VLP+fine-tuning: A group of people sitting on a rock near the water with a bridge in the background.K-Replay: A group of people sitting on rocks near the Brooklyn Bridge.VLP+fine-tuning: A group of people standing in front of a building.K-Replay: A group of people posing for a picture in front of the Acropolis.VLP+fine-tuning: A group of people walking around a building with a ball on top of it.K-Replay: A group of people walking around a Oriental Pearl Tower.
VLP+fine-tuning: A group of cartoon animals sitting on top of a hill.K-Replay:A bunch of Winnie the Poohcartoon characters.VLP+fine-tuning: A rabbit and a man are playing with each other.K-Replay: A Bugs Bunnyand boy playing with each other.VLP+fine-tuning: A man and a woman sitting in a living room.K-Replay: A man and a Spider-Manstanding in a living room.VLP+fine-tuning: A black plate with rice and chicken stir fry and chopsticks.K-Replay: A black plate of Kung Pao Chickenwith chopsticks.VLP+fine-tuning: A plate with two croissantsand a cup of coffee.K-Replay: A plate of croissantsnext to a cup of coffee.VLP+fine-tuning: A bowl of noodles and vegetables with a white spoon.K-Replay: A bowl of ramennoodles with a white spoon and chopsticks.
VLP+fine-tuning: A group of people standing in front of a large screen.K-Replay: Two men standing in front of a Huaweidisplay.
Figure 4: Sample descriptions generated by baseline and our K-Replay method. Knowledge of landmarks, famous brands, movie
characters and special foods are marked in color.
表格 10.250.500.751Vanialla Fine-tuning30.732.737.933.7K-Replay43.947.357.966.3
017.53552.570
Occurrence Frequency in Pre-training Data0.250.500.751
66.3
57.9
47.3
43.9
33.7
37.9
32.7
30.7Vanialla Fine-tuningK-Replay
表格 1-1foodsbrandscharacters landmarksVanialla Fine-tuning20.023.040.245.7K-Replay25.553.362.362.7RecogAcc (%)0.017.535.052.570.0
Various scenario categoriesfoodsbrandscharacters landmarks
62.7
62.3
53.3
25.5
45.7
40.2
23.0
20.0Vanialla Fine-tuningK-Replay
1
Figure 5: RecogAcc under various scenario categories (left).
The relationship between RecogAcc and occurrence fre-
quency in pre-training data (right).
demonstrate that proposed knowledge distillion loss contributes to
alleviating the knowledge hallucination.
What type of knowledge can be expressed by the VLP model?
KnowCap contains a series of 240 types of knowledge, which can
be divided into four major categories: landmarks, famous brands,
special foods and movie characters. Figure 5 left displays the per-
formance of vanilla fine-tuning and K-Replay (for OFA) on these
four categories. K-Replay method can evoke the knowledge under
each scenario category, especially on famous brands. The RecogAcc
of special food is relatively low, which we suggest is because the
features of food are not as recognizable as other scenario.
The knowledge mastered by the VLP model is what it learns
during the pre-training process. An interesting question is whether
the knowledge acquired by the VLP model is related to its occur-
rence frequency in the pre-training data. Figure 5 right shows how
the knowledge that appears in the pre-training data with different
frequencies is expressed by the VLP model. Somewhat surprisingly,
for the vanilla fine-tuning model, the knowledge with a higheroccurrence frequency is not easier to be expressed. However, after
using K-Replay, the knowledge with higher occurrence frequency
has a higher RecogAcc. In fact, this result corroborates our con-
jecture that knowledge with high occurrence frequency is actually
mastered by the VLP model, but the generic bias in downstream
task fine-tuning inhibits its expression, while the K-Replay method
reactivates the expression of such knowledge.
Figure 4 provide sample captions generated by K-Replay and
the VLP+fine-tuning baseline (See Appendix B for more samples).
K-Replay appropriately incorporates various types of knowledge
into the image descriptions, thereby improving the informativeness
and quality of the descriptions.
7 CONCLUSION
In this paper, we propose to exploit VLP model’s generalizability for
knowledge-enhanced image captioning. We find that VLP model’s
ability to inject knowledge is greatly inhibited by the knowledge
hallucination and the generic bias during fine-tuning. To solve these
issues, we proposed K-Replay which enables the retention of pre-
trained knowledge during fine-tuning. Our approach consists of: (1)
a knowledge prediction task to awaken the model’s memory about
knowledge, and (2) a knowledge distillation constraint to enhance
faithfulness. We constructed the KnowCap dataset for evaluation,
containing over 1400 images covering various types of knowledge.
Experimental results demonstrate that K-Replay significantly im-
proves VLP model’s ability to incorporate knowledge compared to
the VLP+fine-tuning baseline. We finally conduct extensive analy-
ses to understand the effectiveness of our method.
ACKNOWLEDGMENTS
We thank Fei Zhao, Yawen Ouyang, Shi Zong, Siyu Liao, and Yihan
Wang for useful help. This work was supported by Natural Science
Foundation of China (NSFC) under Grant No.62176115.Beyond Generic: Enhancing Image Captioning with Real-World Knowledge MM ’23, October 29-November 3, 2023, Ottawa, ON, Canada
