THEFORM AI D ATASET : GENERATIVE AIINSOFTWARE
SECURITY THROUGH THE LENS OF FORMAL VERIFICATION∗
Norbert Tihanyi
Technology Innovation Institute
Abu Dhabi
UAE
tihanyi.pgp@gmail.comTamas Bisztray
The University of Oslo
Oslo
Norway
tamasbi@ifi.uio.noRidhi Jain
Technology Innovation Institute
Abu Dhabi
UAE
ridhij@iiitd.ac.in
Mohamed Amine Ferrag
Technology Innovation Institute
Abu Dhabi
UAE
mohamed.amine.ferrag@gmail.comLucas C. Cordeiro
University of Manchester
Manchester
UK
lucas.cordeiro@manchester.ac.ukVasileios Mavroeidis
The University of Oslo
Oslo
Norway
vasileim@ifi.uio.no
ABSTRACT
This paper presents the FormAI dataset, a large collection of 112 000 AI-generated compilable
and independent C programs with vulnerability classification. We introduce a dynamic zero-shot
prompting technique constructed to spawn a diverse set of programs utilizing Large Language
Models (LLMs). The dataset is generated by GPT-3.5-turbo and comprises programs with varying
levels of complexity. Some programs handle complicated tasks like network management, table
games, or encryption, while others deal with simpler tasks like string manipulation. Every program
is labeled with the vulnerabilities found within the source code, indicating the type, line number,
and vulnerable function name. This is accomplished by employing a formal verification method
using the Efficient SMT-based Bounded Model Checker (ESBMC), which exploits model checking,
abstract interpretation, constraint programming, and satisfiability modulo theories, to reason over
safety/security properties in programs. This approach definitively detects vulnerabilities and offers a
formal model known as a counterexample, thus eliminating the possibility of generating false positive
reports. This property of the dataset makes it suitable for evaluating the effectiveness of various
static and dynamic analysis tools. Furthermore, we have associated the identified vulnerabilities with
relevant Common Weakness Enumeration (CWE) numbers. We make the source code available for
the112,000programs, accompanied by a comprehensive list detailing the vulnerabilities detected in
each program, making the dataset ideal for training LLMs and machine learning algorithms.
Keywords Dataset ·Vulnerability Classification ·Large Language Models ·Formal Verification.
1 Introduction
The advent of Large Language Models (LLMs) is revolutionizing the field of computer science, heavily impact-
ing software development and programming as developers and computer scientists enthusiastically use AI tools
for code completion, generation, translation, and documentation [ Bui et al.(2023) ,Ross et al.(2023) ]. Research re-
lated to program synthesis using Generative Pre-trained Transformers (GPT) [ Chavez et al.(2023) ] is gaining signif-
icant traction, where initial studies indicate that the GPT models can generate syntactically correct yet vulnerable
code [ Ma et al.(2023a) ,Charalambous et al.(2023) ]. A recent study conducted at Stanford University suggests that
software engineers assisted by OpenAI’s codex-davinci-002 model during development were at a higher risk of introduc-
ing security flaws into their code [ Perry et al.(2022) ]. As the usage of AI-based tools for code generation continues to
∗Citation : This work is a preprint and has not yet been peer-reviewed.arXiv:2307.02192v1  [cs.DB]  5 Jul 2023The FormAI Dataset
expand, understanding their potential to introduce software vulnerabilities becomes increasingly important. Considering
that GPT models are trained on freely available data from the internet, which can include vulnerable code, AI tools can
potentially recreate the same patterns that facilitated those vulnerabilities.
Our primary objective is to explore how proficiently LLMs can produce secure code for different coding objectives
without requiring subsequent adjustments or human intervention. Additionally, we aim to uncover the most frequent
vulnerabilities that LLMs tend to introduce in the code they generate, identifying common patterns in realistic examples
to comprehend their behavior better. This brings forward the following research questions:
•RQ1 : How likely is purely LLM-generated code to contain vulnerabilities on the first output when using
simple zero-shot text-based prompts?
•RQ2 : What are LLMs’ most typical coding errors?
Figure 1: AI-driven dataset generation and vulnerability labeling framework. A random type and style combination is
selected for each prompt, instructing the LLM module to generate a C program. The compilable programs are fed to the
BMC module, which performs the classification based on formal verification techniques.
In particular, we explore these research questions in the context of GPT-3.5 generating C programs. GPT-3.5 is the most
widely used LLM available to software developers with a free web interface [ Somoye(2023) ]. Moreover, C is one of
the most popular low-level programming languages for embedded systems, critical security systems, and Internet of
Things (IoT) applications [ Avila(2022) ]. For our purposes, simply showing through a handful of empirical examples
that LLMs can produce vulnerable code is not gratifying and has been demonstrated before for various programming
languages [Charalambous et al.(2023), Perry et al.(2022), Umawing(2023)].
Two things are required to address the outlined research questions accurately. First, a large database containing a diverse
set of C programs. Second, we need to gain insight into the variety and distribution of different vulnerabilities. At the
same time, we must determine whether a vulnerability is present in the code. If we label the code as vulnerable, it should
not be a false positive. The latter is essential when creating datasets for machine learning purposes [ Picard et al.(2020) ,
Hutchinson et al.(2021) ]. On that note, deep learning applications also need large datasets of vulnerable source code
for training purposes [Chen et al.(2023)].
Here, we developed a simple yet effective prompting method to obtain a diverse dataset, prodding the LLM to tackle a
mixed bag of tasks. This resulted in a collection of 112,000C programs addressing various programming scenarios.
Manually labeling the entire dataset is unfeasible for such a large corpus of data. Therefore, we use the Efficient SMT-
based Bounded Model Checker (ESBMC) [ Gadelha et al.(2018) ], which can formally falsify the existence of certain
vulnerabilities. This state-of-the-art tool showcased exceptional performance in the SV-COMP 2023 [ Beyer(2023) ]
competition by efficiently solving many verification tasks within a limited timeframe [ Gadelha et al.(2018) ]. Although
it can only detect formally verifiable errors through symbolic execution, it does not produce false positives.
One limitation of this method is that due to its resource-intensive nature, it can only detect vulnerabilities within a
predefined search depth bounded by the available computational capacity. Suppose the complexity of the code does
not allow the module to check all the nodes in the control-flow graph (CFG) [ Aho et al.(2006) ] exhaustively under a
reasonable time. In that case, we can only know the presence or absence of vulnerabilities within the predefined bound.
2The FormAI Dataset
If we do not find any vulnerabilities up to that depth, the code might still contain some. On the upside, which is why
we use this method, we can definitively confirm the presence of the detected vulnerabilities up to a bound, as we can
provide a “counterexample” as a formal model. Such databases can be useful for various research activities, especially
in machine learning, which we remark on in our discussion.
Figure 1 illustrates the methodology employed in this paper. Initially, we provide instructions to GPT-3.5 to construct a
C program for various tasks. This step will be elaborated thoroughly in Section 5. Next, each output is fed to the GNU
C2compiler to check if the program is compilable. The compilable source code constitutes the FormAI dataset. These
programs are used as input for the ESMBC module which performs the labeling process. The labeled data is saved
in a.csvfile, which includes details such as the name of the vulnerable file, the specific line of code containing the
vulnerability, the function name, and the type of vulnerability.
To summarize, this paper holds the following original contributions:
•We present FormAI, the first AI-generated large-scale dataset consisting of 112 000 independent compilable C
programs that perform various computing tasks. Each of these programs is labeled based on the vulnerabilities
identified by formal verification, namely, the ESBMC module;
•A comprehensive analysis on the identification and prevalence of vulnerabilities affecting the safety and
security properties of C programs generated by GTP-3.5-turbo. The ESBMC module provides the detection
and categorization of vulnerabilities. We connect the identified vulnerability classes with corresponding
Common Weakness Enumeration (CWE) numbers.
The remaining sections are structured as follows: Section 2 discusses the motivation for our work. Section 3 overviews
the related literature. Section 4 presents a short introduction to formal verification and the ESBMC module. Section 5
outlines the approach we employed to create and categorize our dataset, where Section 6 provides an in-depth evaluation
of our findings. Section 7 overviews limitations related to our work. Finally, Section 8 concludes the paper with an
outlook on possible future research directions.
2 Motivation
Throughout software development, it is paramount to guarantee the created programs’ correctness, safety, and security.
Functionally correct code produces the expected output for each given input. Safety aims to produce failure tolerant and
fail-safe code, resistant against accidental or unexpected inputs that result in correct but undesired outputs, which may
cause system failure or erroneous human decisions [ rey V oas(1996) ]. Finally, software security embodies robustness
against external hazards and deliberate attacks. Our objective in this paper is to examine the safety and security
properties of AI-generated source code.
The term “generated code” signifies computer code created by an LLM, capable of using multiple forms of data as
input. Textual prompts are segmented into individual units known as tokens. LLMs generate their response one token
at a time, where a pre-defined token cap limits the output length. Due to this, as of today, LLMs cannot be used to
spawn large applications on a single prompt. The main way developers utilize AI tools is by creating small programs or
code snippets incorporated into their projects. The Stanford study mentioned earlier [ Perry et al.(2022) ] captured this
important aspect. Some assignments given to students were, for example, creating:
•two functions in Python where one encrypts and the other decrypts a given string using a given symmetric key;
•a function in Python that signs a given message using a given ECDSA signing key .
We aim to prompt the LLM to produce code for tasks with similar complexity levels. Furthermore, for the ESBMC
module, it is beneficial to have smaller independent programs. These allow the module to execute the verification process
piece by piece, adhering to a set boundary, thus making the process manageable and more efficient. If the programs
were heavily interdependent, accurately estimating the time required for the module to finish the verification process
would be hardly feasible. The main area of interest in LLM-based code generation has been related to correctness.
Datasets such as HumanEval provide programming challenges to assess the performance of models. For example,
GPT-4 achieves a 67% success rate in solving tasks compared to 48.1%for GPT-3.5 [ OpenAI(2023) ]. Measuring
correctness is not our goal with the FormAI dataset. For example, if the prompt says “Create a board game using the C
programming language in an artistic style” , correctness would be difficult to verify, especially for a large dataset. The
only requirement is that the program should be syntactically correct and it must be possible to compile it. To restate our
research objective, we aim to uncover the proportion and type of frequent coding errors in C source code generated by
2https://gcc.gnu.org
3The FormAI Dataset
GPT-3.5 when prompted to perform simple tasks using natural language. The following real-life example demonstrates
and underscores the necessity of this research question.
Imagine a situation where a programmer submits the following prompt to GPT-3.5: “Provide a small C program that
adds two numbers together. ” .
Figure 2: Insecure code generated by gpt-3.5-turbo. The program reads two numbers, where the addition can result in a
value outside of the range “int” can represent, which may lead to integer overflow.
The resulting code shown in Figure 2 is vulnerable, as it contains an integer overflow on the scanf() function. In
32-bit computing architectures, integers are commonly stored as 4 bytes (32 bits), which results in a maximum integer
value of 2147483647 , equivalent to 231−1. If one attempts to add 2147483647 + 1 using this small program, the result
will be incorrect due to integer overflow. The incorrect result will be -2147483648 instead of the expected 2147483648 .
The addition exceeds the maximum representable value for a signed 32-bit integer 231−1, causing the integer to wrap
around and become negative due to the two’s complement representation.
Even when GPT-3.5 is requested to write a secure version of this code –without specifying the vulnerability– it
only attempts to verify against entering non-integer inputs by adding the following code snippet: if (scanf("%d",
&num1) != 1) {...} . Clearly, after sanitizing the input, the issue of integer overflow is still present. When prompted
to create a C program that adds two numbers, it appears that both GPT-3.5 and GPT-4 generate code with this insecure
pattern. When asked for a secure version, both models perform input sanitization. By using the ESBMC module to
verify this program, the vulnerability is immediately found through a counterexample, and the following message is
created:
In [Charalambous et al.(2023) ], the authors demonstrated that GPT-3.5 could efficiently fix errors if the output of the
ESBMC module is provided. Given only general instruction as “write secure code” , or asked to find vulnerabilities,
GPT-3.5 struggles to pinpoint the specific vulnerability accurately, let alone if multiple are present. While advanced
models might perform better for certain vulnerabilities, this provides no guarantee that all coding mistakes will be
found [ Pearce et al.(2022) ]. The main challenge is the initial detection without any prior hint or indicator. Doing this
efficiently for a large corpus of C code while avoiding false positives and false negatives is still challenging for LLMs
[Pearce et al.(2022) ]. Based on this observation, we want to create an extensive and diverse dataset of properly labeled
LLM-generated C programs. Such a dataset can reproduce coding errors often created by LLMs and serve as a valuable
resource and starting point in training LLMs for secure code generation.
3 Related Work
This section overviews automated vulnerability detection and notable existing datasets containing vulnerable code
samples for various training and benchmarking purposes.
3.1 ChatGPT in Software Engineering
In [Ma et al.(2023b) ] Me et al. assessed the capabilities and limitations of ChatGPT for software engineering (SE),
specifically in understanding code syntax and semantic structures like abstract syntax trees (AST), control flow graphs
4The FormAI Dataset
Figure 3: The counterexample provided for Figure 1. using ESBMC version 7.2.0. Note this is only part of the output
specifically for integer overflow, which we wanted to bring forward for this example.
(CFG), and call graphs (CG). ChatGPT exhibits excellent syntax understanding, indicating its potential for static code
analysis. They highlighted that the model also hallucinates when interpreting code semantics, creating non-existent
facts. This implies a need for methods to verify ChatGPT’s outputs to enhance its reliability. This study provides
initial insights into why the codes generated by language models are syntactically correct but potentially vulnerable.
Frameworks and techniques for turning prompts into executable code for Software Engineering are rapidly emerging, but
the main focus is often functional correctness omitting important security aspects [ Xing et al.(2023) ,White et al.(2023) ,
Yao et al.(2023) ,Wei et al.(2023) ]. In [ Liu et al.(2023) ], Liu et al. questions the validity of existing code evaluation
datasets, suggesting they inadequately assess the correctness of generated code.
In [Khoury et al.(2023)] the authors generated 21 small programs in five different languages: C, C++, Python, HTML
and Java. Combining manual verification with ChatGPT-based vulnerability detection, the study found that only 5
of the 21 generated programs were initially secure. A recent study by Microsoft [ Imani et al.(2023) ] found that GPT
models encounter difficulties when attempting to accurately solve arithmetic operations. This aligns with the findings
we presented in the motivation Section.
In a small study involving 50students [ Sandoval et al.(2023) ], the authors found that students using an AI coding
assistant introduced vulnerabilities at the same rate as their unassisted counterparts. Still, notably, the experiment was
limited by focusing only on a single programming scenario. Contrary to the previous study in [ Pearce et al.(2021) ]
Pearce et al. conclude that the control group, which utilized GitHub’s Copilot, incorporated more vulnerabilities
into their code. Instead of a single coding scenario like in [ Sandoval et al.(2023) ], the authors expanded the study’s
comprehensiveness by choosing a diverse set of coding tasks pertinent to high-risk cybersecurity vulnerabilities,
such as those featured in MITRE’s “Top 25” Common Weakness Enumeration (CWE) list. The study highlights an
important lesson: to accurately measure the role of AI tools in code generation or completion, it is essential to choose
coding scenarios mirroring a diverse set of relevant real-world settings, thereby facilitating the occurrence of various
vulnerabilities. This necessitates the creation of code bases replicating a wide range of settings, which is one of the
primary goals the FormAI dataset strives to achieve. These studies indicate that AI tools, and in particular ChatGPT, as
of today, can produce code containing vulnerabilities.
In a recent study, Shumailov et al. highlighted a phenomenon known as “model collapse” [Shumailov et al.(2023) ].
Their research demonstrated that integrating content generated by LLMs can lead to persistent flaws in subsequent
models when using the generated data for training. This hints that training machine learning models only on purely
AI-generated content is insufficient if one aims to prepare these models for detecting vulnerabilities in human-generated
code. This is essentially due to using a dataset during the training phase, which is not diverse enough and misrepresents
edge cases. We use our dynamic zero-shot prompting method to circumvent the highlighted issue to ensure diversity.
Moreover, our research goal is to find and highlight what coding mistakes AI models can create, which requires a
thorough investigation of AI-generated code. On the other hand, AI models themselves were trained on human-generated
content; thus, the vulnerabilities produced have roots in incorrect code created by humans. Yet, as discussed in the
next section, existing datasets notoriously include synthetic data (different from AI-generated), which can be useful for
benchmarking vulnerability scanners, but has questionable value for training purposes [Chen et al.(2023)].
5The FormAI Dataset
Table 1: Comparisons of various datasets based on their labeling classifications.
DatasetOnly
C-codeSource#Code
Snippets#Vuln.
SnippetsMultiple
Vulns/SnippetCompiles /
GranularityVuln.
Labelling#Avg Line
of CodeLabelling
Method
Big-Vul ✘ Real-World 188,636 100% ✘ ✘ /Function CVE/CVW 30 PATCH
Draper ✘ Synthetic+Real-World 1,274,366 5.62% ✔ ✘ /Function CWE 29 STAT
SARD ✘ Synthetic+Real-World 100,883 100% ✘ ✔ /Program CWE 114 BDV/STAT/MAN
Juliet ✘ Synthetic 106,075 100% ✘ ✔ /Program CWE 125 BDV
Devign ✘ Real-World 27,544 46.05% ✘ ✘ /Function CVE 112 ML
REVEAL ✘ Real-World 22,734 9.85% ✘ ✘ /Function CVE 32 PATCH
DiverseVul ✘ Real-World 379,241 7.02% ✘ ✘ /Function CWE 37 PATCH
FormAI ✔ AI-generated 112,000 51.24% ✔ ✔ /Program CWE 79 ESBMC
Legend:
PATCH : GitHub Commits Patching a Vuln. Man : Manual Verification, Stat: Static Analyser, ML: Machine Learning Based, BDV : By design vulnerable
3.2 Existing databases for Vulnerable C code
We show how the FormAI dataset compares to seven widely studied datasets containing vulnerable code. The examined
datasets are: Big-Vul [ Fan et al.(2020) ], Draper [ Russell et al.(2018) ,Kim and Russell(2018) ], SARD [ Black(2018) ],
Juliet [ Jr and Black(2012) ], Devign [ Zhou et al.(2019b) ,Zhou et al.(2019a) ], REVEAL [ Chakraborty et al.(2022) ], and
DiverseVul[ Chen et al.(2023) ]. Table 1 presents a comprehensive comparison of the datasets across various metrics.
Some of this data is derived from review papers that evaluate these datasets [Jain et al.(2023), Chen et al.(2023)].
Big-Vul, Draper, Devign, REVEAL, and DiverseVul comprise vulnerable real-world functions from open-source
applications. These five datasets do not include all dependencies of the samples; therefore, they are non-compilable.
SARD and Juliet contain synthetic, compilable programs. In their general composition, the programs contain a
vulnerable function, its equivalent patched function, and a main function calling these functions. All datasets indicate
whether a code is vulnerable. The mentioned datasets use the following vulnerability labeling methodologies:
•PATCH : Functions before receiving GitHub commits for detected vulnerabilities are treated as vulnerable.
•MAN: Manual labeling
•STAT : Static analyzers
•ML: Machine learning-based techniques
•BDV: By design vulnerable
In the latter case, no vulnerability verification tool is used. Note that the size of the datasets can be misleading, as
many of the datasets contain samples from other languages. For example, SARD contains C, C++, Java, PHP, and C#.
Moreover, newly released sets often incorporate previous datasets or scrape the same GitHub repositories, making them
redundant.
For example, Dreper contains C and C++ code from the SATE IV Juliet Test Suite, Debian Linux distribution, and
public Git repositories. Since the open-source functions from Debian and GitHub were not labeled, the authors used
a suite of static analysis tools: Clang, Cppcheck, and Flawfinder [ Russell et al.(2018) ]. However, the paper does not
mention if vulnerabilities were manually verified or if any confirmation has been performed to root out false positives. In
[Chen et al.(2023) ], on top of creating DiverseVul, Chen et al. merged all datasets that were based on GitHub commits
and removed duplicates, thus making the most comprehensive collection of GitHub commits containing vulnerable C
and C++ code.
3.3 Vulnerability Scanning and Repair
Software verification is critical to ensuring correctness, safety, and security. The primary techniques are manual
verification, static analysis, and dynamic analysis, where a fourth emerging technique is machine learning-based
detection [ Cordeiro et al.(2012) ,D’Silva et al.(2008) ,Wallace and Fujii(1989) ,Ma et al.(2023b) ]. Manual verification
techniques such as code review or manual testing rely on human effort and are not scalable. Static analysis can test
the source code without running it, using techniques such as static symbolic execution, data flow analysis, control
flow analysis, and style checking. On the other hand, dynamic analysis aims at observing software behavior while
running the code. It involves fuzzing, automated testing, run-time verification, and profiling. The fourth technique is a
promising field where LLMs can be useful in a wide range of tasks, such as code review and bug detection, vulnerability
6The FormAI Dataset
detection, test case generation, and documentation generation; however, as of today, each area has certain limitations.
Research related to the application of verification tools in analyzing code specifically generated by LLMs remains
rather limited. An earlier work from 2022 examined the ability of various LLMs to fix vulnerabilities, where the models
showed promising results, especially when combined. Still, the authors noted that such tools are not ready to be used
in a program repair framework, where further research is necessary to incorporate bug localization. They highlighted
challenges in the tool’s ability to generate functionally correct code [Pearce et al.(2022)].
4 Formal Verification
This section presents the crucial foundational knowledge required to understand the technology employed in this
research, specifically Bounded Model Checking (BMC). An intuitive question arises: Could BMC potentially introduce
false positives into our dataset? The answer is no, and understanding why is critical to our work. To clarify this theory,
we will explain counterexamples and thoroughly discuss the math behind bounded model checking.
Bounded Model Checking (BMC) is a technique used in formal verification to check the correctness of a system
within a finite number of steps. It involves modeling the system as a finite state transition system and system-
atically exploring its state space up to a specified bound or depth. The latest BMC modules can handle various
programming languages [ Sadowski and Yi(2014) ,Gadelha et al.(2019) ,White et al.(2016) ,Zhao and Huang(2018) ,
Gadelha et al.(2023) ]. This technique first takes the program code, from which a control-flow graph (CFG) is created
[Aho et al.(2006) ]. In CFG, each node signifies a deterministic or non-deterministic assignment or a conditional
statement. Each edge represents a potential shift in the program’s control position. Essentially, every node is a block
representing a “set of instructions with a singular entry and exit point” . Edges indicate possible paths to other blocks
to which the program’s control location can transition. The CFG is first transformed into Static Single Assignment
(SSA) and converted into a State Transition System (STS). This can be interpreted by a Satisfiability Modulo Theories
(SMT) solver. This solver can determine if a set of variable assignments makes a given formula true, i.e., this formula is
designed to be satisfiable if and only if there’s a counterexample to the properties within a specified bound k. If there is
no error state and the formula is unsatisfiable up to the bound k, there is no software vulnerability within that bound. If
the solver reaches termination within a bound ≤k, we can definitively prove the absence of software errors.
To be more precise, let a given program Punder verification be a finite state transition system, denoted by a triple
ST= (S, R, I ), where Srepresents the set of states, R⊆S×Srepresents the set of transitions and (sn,···, sm)∈
I⊆Srepresents the set of initial states. In a state transition system, a state denoted as s∈Sconsists of the program
counter value, referred to as pc, and the values of all program variables. The initial state denoted as s1, assigns the
initial program location within the Control Flow Graph (CFG) to pc. Each transition T= (si, si+1)∈Rbetween two
states, siandsi+1, is identified with a logical formula T(si, si+1). This formula captures the constraints governing the
values of the program counter and program variables relevant to the transition.
Within BMC (Bounded Model Checking), properties under verification are defined as follows: ϕ(s)represents a logical
formula that encodes states satisfying a safety/security property. In contrast, ψ(s)represents a logical formula that
encodes states satisfying the completeness threshold, indicating states corresponding to program termination. ψ(s),
contains unwindings so that it does not exceed the maximum number of loop iterations in the program. It is worth
noting that, in our notation, termination, and error are mutually exclusive: ϕ(s)∧ψ(s)is by construction unsatisfiable.
IfT(si, si+1)∨ϕ(s)is unsatisfiable, state sis considered a deadlock state. The bounded model checking problem,
denoted by BMC Φis formulated by constructing a logical formula, and the satisfiability of this formula determines
whether Phas a counterexample of length kor less. Specifically, the formula is satisfiable if and only if such a
counterexample exists within the given length constraint, i.e.:
BMC Φ(k) =I(s1)∧k−1^
i=1T(si, si+1)∧k_
i=1¬ϕ(si). (1)
In this context, Idenotes the set of initial states of ST, andT(si, si+1)represents the transition relation of ST, between
time steps iandi+ 1. Hence, the logical formula I(s1)∧Vk−1
i=1T(si, si+1)represents the executions of STwith a
length of kandBMC Φ(k)can be satisfied if and only if for some i≤kthere exists a reachable state at time step iin
which ϕis violated. If BMC Φ(k)is satisfiable, it implies that ϕis violated, and an SMT solver provides a satisfying
assignment from which we can extract the values of the program variables to construct a counterexample.
A counterexample, or trace, for a violated property ϕ, is defined as a finite sequence of states s1, . . . , s k, where
s1, . . . , s k∈SandT(si, si+1)holds for 0≤i < k . If equation (1) is unsatisfiable, we can conclude that no error
state is reachable within ksteps or less. This valuable information leads us to conclude that no software vulnerability
7The FormAI Dataset
exists in the program within the specified bound of k. With this methodology, we aim to classify every generated C
program as either vulnerable or not, within a given bound k. By searching for counterexamples within this bound, we
can establish, based on mathematical proofs, whether a counterexample exists and whether our program Pcontains a
security vulnerability. This approach allows us to identify security issues such as buffer overflows or access-bound
violations.
4.1 The ESBMC module
This work uses the Efficient SMT-based Context-Bounded Model Checker (ESBMC) [ Gadelha et al.(2018) ] as our
chosen BMC module. ESBMC is a mature, permissively licensed open-source context-bounded model checker for
verifying single- and multithreaded C/C++, Kotlin, and Solidity programs. It can automatically verify both predefined
safety properties and user-defined program assertions. The safety properties include out-of-bounds array access, illegal
pointer de