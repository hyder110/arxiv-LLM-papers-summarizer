Text Alignment Is An Efficient Unified Model
for Massive NLP Tasks
Yuheng Zha∗Yichi Yang∗Ruichen Li Zhiting Hu
UC San Diego
{yzha, yiy067, rul014, zhh019}@ucsd.edu
Abstract
Large language models (LLMs), typically designed as a function of next-word
prediction, have excelled across extensive NLP tasks. Despite the generality,
next-word prediction is often not an efficient formulation for many of the tasks,
demanding an extreme scale of model parameters (10s or 100s of billions) and
sometimes yielding suboptimal performance. In practice, it is often desirable to
build more efficient models—despite being less versatile, they still apply to a
substantial subset of problems, delivering on par or even superior performance
with much smaller model sizes. In this paper, we propose text alignment as an
efficient unified model for a wide range of crucial tasks involving text entailment,
similarity, question answering (and answerability), factual consistency, and so
forth. Given a pair of texts, the model measures the degree of alignment between
their information. We instantiate an alignment model ( ALIGN ) through lightweight
finetuning of RoBERTa (355M parameters) using 5.9M examples from 28 datasets.
Despite its compact size, extensive experiments show the model’s efficiency and
strong performance: (1)On over 20 datasets of aforementioned diverse tasks,
the model matches or surpasses FLAN-T5 models that have around 2x or 10x
more parameters; the single unified model also outperforms task-specific models
finetuned on individual datasets; (2)When applied to evaluate factual consistency
of language generation on 23 datasets, our model improves over various baselines,
including the much larger GPT-3.5 (ChatGPT) and sometimes even GPT-4; (3)
The lightweight model can also serve as an add-on component for LLMs such as
GPT-3.5 in question answering tasks, improving the average exact match (EM)
score by 17.94 and F1 score by 15.05 through identifying unanswerable questions.2
1 Introduction
Recent large language models (LLMs) have demonstrated exceptional generalizability in a wide
range of natural language processing (NLP) tasks. As the underlying formulation of these LLMs,
next-word prediction is proven to be a general function applicable to diverse language problems.
However, it is often not being an efficient solution for many tasks. LLMs often need to scale up to
over tens of billions of parameters to achieve meaningful performance [ 1], with popular models like
GPT-3 boasting as many as 175B parameters [ 2]. Additionally, even with their extreme scale, LLMs
sometimes still find themselves outperformed by smaller models. For example, ChatGPT/GPT-3.5
falls behind existing finetuned baselines on most classical natural language understanding tasks [3].
As a result, in many cases it is desirable to navigate the spectrum of generality-vs-efficiency tradeoff,
for example, by developing smaller but general-purpose models that excel in a substantial subset of
tasks. Despite being less versatile than the extreme-scale LLMs, these models are more efficient and
∗Equal contribution.
2Code will be available at https://github.com/yuh-zha/Align
Preprint. Under review.arXiv:2307.02729v1  [cs.CL]  6 Jul 2023Figure 1: Our alignment model (125M and 355M) achieves substantially better efficiency and
performance compared to much larger models on a wide range of tasks, including (left) diverse text
pair understanding tasks on over 20 datasets and (middle) factual consistency evaluation, and (right)
improves existing LLMs on question answering by detecting unanswerable questions. See Section 4
for more details.
provide superior performance, making them more usable on the set of tasks that they are designed
to handle. Previous work has attempted to build natural language inference (NLI) models as an
efficient solution for broad tasks [ 4–6]. But with limited NLI data (e.g., MNLI [ 7]) for training, the
models exhibit limited performance and applicability across diverse domains. Another related line of
research trains general text representation models with pretraining and multi-task learning [ 8–10].
Those models need to be specifically finetuned (with task-specific head) for each downstream task,
instead of functioning as ready-to-use solutions.
In this paper, we investigate the underlying commonalities among a broad range of NLP tasks that
concern the relationship between two texts, and propose a text alignment model ( ALIGN ) as an
efficient unified solution, following Zha et al. [11]. Given an arbitrary pair of texts, ALIGN measures
the degree of alignment between the content in the texts. We show the formulation subsumes a
substantial set of popular tasks, ranging from NLI, fact verification, semantic textual similarity,
question answering, coreference resolution, paraphrase detection, to factual consistency evaluation
of diverse language generation tasks, question answerability verification, and so forth (Figure 1).
The generality, in turn, presents an opportunity for us to use diverse data to learn the alignment
model. Specifically, we adapt and aggregate 28 datasets from the above tasks, resulting in 5.9M
training examples with diverse characteristics. We then use the data to finetune a small-scale LM
(e.g., RoBERTa [ 12]), yielding a model that directly applies to and excels in diverse problems and
domains.
We evaluate ALIGN with extensive experiments. First , we test on 25 seen and unseen datasets of the
aforementioned tasks, and show our alignment model based on RoBERTa (355M parameters) achieves
on par or even better performance than the FLAN-T5 models (780M and 3B) that are 2x or 8.5x as
large and trained with substantially more data. In addition, the single alignment model outperforms
RoBERTa specifically finetuned on each of the datasets. Second , we use ALIGN to evaluate faculty
consistency of natural language generation systems (e.g., for summarization, dialog, paraphrasing,
etc.). On 23 datasets, the small alignment model achieves substantially higher correlation with human
judgements than recent metrics, including those based on GPT-3.5 and even GPT-4. Third , we use
ALIGN as a question answerability verifier and incorporate it as an add-on component to existing
LLMs (e.g., GPT-3.5 and FLAN-T5). It significantly enhances the LLMs’ performance in three
question answering datasets, improving the average exact match score by 17.94 and F1 score by
15.05.
2 Related Work
Recent work has shown that LLMs are few-shot learners capable of generalizing to diverse tasks
[13,2,14]. These LLMs are designed based on the principle of next-word-prediction, where the joint
probability distribution of text sequences are factored into a product of conditional probabilities [ 15].
The performance of LLMs is highly correlated with their scales. Wei et al. [1]show that a scale of
2more than 1022training FLOPs (around 10B model parameters) is required for several different LLM
designs to achieve above-random performance on many language tasks.
Another line of research has tried to design models that can either learn from multiple tasks or handle
many downstream tasks. Liu et al. [8]propose to use a BERT model [ 16] with task-specific heads to
learn four types of tasks, including single-sentence classification, pairwise text classification, text
similarity scoring, and relevance ranking. Aghajanyan et al. [9]pre-finetune language models on
50 dataset to encourage learning more general representations, and show that the process improves
model performance and data-efficiency in the finetuning stage. While we also use diverse datasets to
train our model, we 1) unify language tasks into a single text pair alignment problem, 2) share all
model components across multiple tasks and do not use dataset-specific heads, and 3) our model can
be directly applied to a wide range of tasks without additional finetuning.
3 Text Alignment Model
In this section, we introduce the text pair alignment formulation. We first formally define the concept
of text pair alignment, and then discuss how the alignment function can be used to solve a set of
popular language tasks. Additionally, we cover the split-then-aggregate method used to handle long
inputs. In Section 3.1, we discuss the training process of the alignment model (A LIGN ).
Given a text pair (x1,x2), we define text x2to be aligned with text x1if all information in x2
is supported by information in x1, following Zha et al. [11]. For example, let "I have been in
Kentucky, Kirby." be text x1. Then, "I have been in the US." is aligned with x1. In con-
trast, both "I have been in Europe." and"Kentucky has the best fried chicken."
are not aligned with x1, as the former is contradicted by x1, and the latter cannot be inferred
from x1. Formally, we model alignment as a function that maps the text pair (x1,x2)to a label y
describing the level of alignment:
f: (x1,x2)→y.
In practice, the language tasks we wish to solve with the alignment function can be broadly categorized
into two groups: one that uses discrete labels, and the other that uses continuous labels (e.g., semantic
textual similarity). More specifically, tasks with discrete labels are typically formulated as either
binary classification (e.g., paraphrase detection) or three way classification (e.g., fact verification).
In order to make the alignment function more general, such that it accommodates all the above
cases, our alignment model produces three outputs: Pr(ybin),Pr(y3way), and yreg. Here, Pr(ybin)
andPr(y3way)are probability distributions over the binary ( ALIGNED ,NOT-ALIGNED ) and 3-way
(ALIGNED ,CONTRADICT ,NEUTRAL ) classification labels, respectively; yreg∈[0,1]is a real-valued
score for regression tasks.
This formulation allows us to apply the alignment function to diverse tasks:
•For tasks that naturally fit into the text pair alignment format, such as NLI [7],fact verification
[17],paraphrase detection [18], and semantic textual similarity [19], depending on the nature
of the task, we simply map one of the alignment labels to the desired label. For example, for
most NLI tasks, we interpret the corresponding y3waylabels as "entailment","contradiction", and
"neutral".
•Ininformation retrieval tasks [ 20], the goal is to find documents that can answer a given query
from a large set of candidate documents. Since relevant documents contain information related to
respective queries, we use candidate documents as text x1, and queries as text x2. Then, a higher
Pr(ybin=ALIGNED )indicates the candidate document is more likely to be useful in answering
the query.
•Inmultiple choice QA tasks [ 21], the inputs are a context, a question, and several choices (with
one of them being the correct answer). In extractive QA tasks (including ones with unanswerable
questions [22]), the inputs only consist of a context and a question. In either case, the expected
output (the correct answer) can be inferred from the question and the context, while a wrong
answer either contradicts the context or is not supported by the context. Therefore, we use the
context as text x1and the concatenation of the question a candidate answer as text x2. Here, a
higher Pr(ybin=ALIGNED )indicates the candidate answer is more likely to be correct.
3•Incoreference resolution tasks [ 23], each sample includes a context containing a pronoun, and
a list of candidate entities. The goal is to find the correct entity that the pronoun is referring to.
As the pronoun and the associated entity is equivalent in this context, we consider the context
with the pronoun replaced with the correct entity to be aligned with the original context. To
solve coreference resolution problems, we simply replace the pronoun with candidate entities and
compare the resulting contexts ( x2) with the original context ( x1). We pick the candidate that
produces the highest Pr(y3way=ALIGNED )orPr(ybin=ALIGNED )as the correct answer.
•For generative tasks like machine summarization, dialog, and paraphrasing, the alignment function
can be used to evaluate the factual consistency of generated outputs. We use the generation
context (e.g., input document) as text x1, and candidate system output (e.g., generated summary)
as text x2. In this case, the probability of Pr(y3way =ALIGNED )orPr(ybin=ALIGNED )
indicates if the candidate output faithfully reflects information in the context, without introducing
hallucinations or contradictions.
One specific challenge of applying the alignment function to downstream tasks is that text x1in some
datasets (e.g., contexts in QA or summarization datasets) tends to be significantly longer than the
input length limit of typical language models (e.g., 512 tokens for RoBERTa). As a result, naively
truncating oversized inputs could throw away important information. To alleviate this problem,
inspired by Laban et al. [24], Amplayo et al. [25], at inference time, instead of truncating the inputs,
we split text x1into a set of chunks {x(i)
1}and text x2into a set of sentences {x(j)
2}such that the
combined length of a chunk-sentence pair is slightly below that length limit. Then, we evaluate each
pair and aggregate the results as
alignment( x1,x2) = mean
jmax
if(x(i)
1,x(j)
2),
where the max operation selects the output with the highest ALIGNED probability or regression score.
Since in most downstream applications, text x2tends to be succinct (e.g., summaries) and consists of
self-contained sentences, this aggregation method can be interpreted as first finding the text x1chunk
that most strongly supports each text x2"fact", and then taking the average across all text x2"facts".
3.1 Training
Our formulation not only allows us to solve the above tasks with a single alignment function, but
also learn the alignment function from these tasks. By adapting text pair understanding tasks into
a uniform alignment format as above, we can naturally model these tasks as simple classification
and regression, allowing us to train a small model while achieving strong performance. Specifically,
we use RoBERTa [ 12] as a lightweight backbone language model, and attach three individual linear
layers to predict the three types of alignment outputs, Pr(ybin),Pr(y3way), andyreg, respectively. The
two classification heads are trained with cross entropy loss, while the regression head is trained with
mean squared error loss. The losses are aggregated as a weighted sum,
Ltotal=λ1L3way+λ2Lbin+λ3Lreg,
where we set λ1=1/log 3,λ2=1/log 2, and λ3= 1, following Aghajanyan et al. [9].
Besides the aforementioned downstream datasets, we also include synthetic data to increase the
diversity of the training set. Specifically, for QA datasets without wrong options (e.g., extractive QA
datasets like SQuAD v2 [ 22]), we first remove the ground truth answer from the context, and then
use a QA model [ 13] to generate wrong answers that can be used to create NOT-ALIGNED samples.
Additionally, we create synthetic paraphrase samples by back translating the WikiText-103 corpus
[26] using a neural machine translation model [ 27]. For the WikiHow summarization dataset, we
use an extractive summarizer [ 28] to generate synthetic summaries in additional to ground truth
summaries. Following Kryscinski et al. [29], Deng et al. [30], we create negative samples for both
WikiText-103 and WikiHow samples by randomly masking 25% of the tokens in text band infilling
with a small masked language modeling model [ 31]. In total, we collect 5.9M examples from 28
datasets to train our alignment model ALIGN . We include more details of our training setup and data
in Appendix C.
4Table 1: Performance of ALIGN and the much larger FLAN-T5 on in-domain datasets. Bold number
indicates our performance is better than either FLAN-T5-large or xlarge.
ALIGN (Ours) FLAN-T5
base large large xlarge
Model Parameters 125M 355M 780M 3B
NLIMNLI-mm [7] 87.5 90.3 88.7 90.4
MNLI-m [7] 87.8 90.3 88.8 90.5
ANLI-1 [32] 65.3 75.8 68.1 77.0
ANLI-2 [32] 48.7 52.4 48.7 60.6
ANLI-3 [32] 45.5 52.3 49.8 56.6
SNLI [33] 90.8 91.8 89.7 90.7
Fact VerificationNLI-FEVER [34, 32] 76.8 77.8 72.0 71.9
VitaminC [17] 89.8 91.8 72.9 73.9
STSSICK [19] 90.7 91.5 79.3 79.1
STSB [35] 89.0 89.8 83.9 88.2
ParaphrasePAWS [18] 92.3 92.6 94.0 94.6
PAWS-QQP [18] 91.9 93.8 88.3 90.1
QQP [36] 90.1 91.3 86.8 87.4
QARACE-m [21] 76.9 86.8 84.8 87.6
RACE-h [21] 68.8 81.6 78.3 84.6
Multi-RC [37] 82.2 87.8 84.7 88.2
BoolQ [38] 81.1 87.7 84.9 89.6
QuAIL [39] 67.8 78.6 79.1 86.3
SciQ [40] 92.4 93.7 94.9 95.7
Coreference GAP [23] 81.4 88.6 73.8 81.5
Average 79.8 84.3 79.6 83.2
4 Experiments
In this section, we experiment with applying ALIGN to multiple downstream tasks, including language
pair understanding tasks (Section 4.1), factual consistency evaluation (Section 4.2), and question
answering with unanswerable questions (Section 4.3).
4.1 Natural Language Understanding Tasks
Natural Language Understanding (NLU) is a major category of tasks for language models, and our
formulation allows us to directly use ALIGN to solve these tasks. Specifically, we include NLI, fact
verification, paraphrase detection, multiple-choice QA, STS, and coreference resolution datasets
in the experiments. We also include unseen datasets to demonstrate the generalizability of ALIGN .
Experiments show the alignment model is on par with FLAN T5 that has 8.5x as many parameters.
Additionally, without further task-specific finetuning, our model outperforms finetuned language
models of a similar size.
4.1.1 Experiment Setup
Datasets We first evaluate ALIGN on test sets of the 20 datasets used during training (in-domain
setting; see Table 1). Then, we use 9 unseen datasets for evaluation (zero-shot setting; see Table 2).
For more details about the datasets, please refer to appendix C.2. If a dataset does not have a public
test set, we use its validation set instead. For datasets that require binary, 3-way classification or
regression, we use the associated output heads, respectively, as discussed in Section 3.
Baselines To demonstrate the efficiency of ALIGN , we compare it with FLAN-T5 [ 14] and FLAN-
Alpaca3with model size ranging from 220M (FLAN-Alpaca-base) to 3B (FLAN-Alpaca-xlarge and
3https://github.com/declare-lab/flan-alpaca
5NLI FV STS Para Coref QAAverageFigure 2: The performance of the ALIGN -large
and finetuned RoBERTa-large on in-domain tasks.
NLI QA ParaAverageFigure 3: The performance of the ALIGN -large
and RoBERTa-NLI-large on zero-shot tasks.
Table 2: Zero-shot setting results from ALIGN . The bold numbers indicates a better performance of
ALIGN when comparing with similarly sized FLAN-T5. The gray number shows the specific dataset
is appeared in the training set of FLAN-T5.
ALIGN (Ours) FLAN-T5
base (125M) large (355M) base (250M) large (780M)
AXB [42] 75.1 79.6 71.7 76.2
AXG [42] 59.8 72.5 53.4 73.6
CB [42] 76.8 89.3 82.1 87.5
RTE [43] 83.4 89.9 81.6 87.0
WNLI [43] 52.1 54.9 46.5 62.0NLI
SE14T1 [44] 90.7 86.9 69.6 69.9
Paraphrase MRPC [45] 66.0 67.9 74.8 80.1
DREAM [46] 71.3 81.1 69.9 79.0QAQuartz [47] 59.7 79.2 74.4 90.2
Average 70.5 77.9 69.3 78.4
FLAN-T5-xlarge). For both models, we use the same prompts as Longpre et al. [41]. We also include
RoBERTa finetuned on individual datasets as a baseline and show that our alignment model works
well out-of-the-box, without further finetuning. Lastly, in the zero-shot setting, we also compare
with RoBERTa model finetuned on MNLI, ANLI and SNLI datasets (RoBERTa-NLI) to show the
generalizability of our proposed formulation.
4.1.2 Results
We report average Pearson Correlation coefficient for the STS tasks [ 19,35], and average accuracy
for the other tasks. As show in Table 1, ALIGN outperforms instruction-finetuned FLAN-T5 that is 2x
as large (780M) and has comparable performance to the version 8.5x as large (3B). In the zero-shot
setting (see Table 2), ALIGN achieves comparable performance with similarly sized variants of FLAN-
T5, even on datasets that exist in the FLAN-T5’s training set. We report results on FLAN-Alpaca in
Appendix D.
Furthermore, ALIGN is on par with RoBERTa finetuned on individual datasets (Figure 2). In the zero-
shot setting, ALIGN has stronger performance on average than the RoBERTa-NLI model (Figure 3),
indicating that our formulation leads to better generalizability.
4.2 Factual Consistency Evaluation for Language Generation
Studies have shown that natural generation systems (NLG) are prone to generating text that is
not consistent with the source material [ 48–52]. As a result, many automatic metrics have been
6developed with the goal of detecting factual consistency errors. As factual consistency is closely
related to our definition of text pair alignment, we can directly apply ALIGN for this purpose, using
the NLG input context as x1, and system outputs as x2. We consider a system output with higher
Pr(y3way=ALIGNED )to be more factually consistent.
4.2.1 Experiment Setup
Dataset Following Zha et al. [11], we use two popular factual consistency evaluation benchmarks,
TRUE (containing 11 datasets, including dialog, fact verification, and paraphrase detection ) [ 52]
andSummaC (consisting of 6 summarization datasets) [ 24]. We also include Other popular meta-
evaluation datasets, namely XSumFaith [ 51], SummEval [ 53], QAGS-XSum [ 54], QAGS-CNNDM
[54], FRANK [55] and SamSum [56]. This results in 23 datasets in total for our study.
Baselines We compare ALIGN with the latest LLM based automatic metrics: GPTScore [ 57],
G-EV AL [ 58] and a ChatGPT-based metric [ 59]. These metrics achieve the best performance when
using the GPT family of LLMs, which are significantly larger than our alignment model (e.g., GPT-3
has 175B parameters). GPTScore evaluates texts based on the probability of a LLM generating the
target text, while G-EV AL augments its prompt using chain-of-thoughts techniques and asks the
LLM to score the input by form-filling. Liu et al. [58] design a prompt that asks ChatGPT to score
the faithfulness of the summary on a five point scale. Additionally, we include strong, smaller-scale
(similar with our alignment model) baselines, including BERTScore [ 60], BLEURT [ 61], BARTScore
[62], CTC [30], UniEval [63] and QAFactEval [64], following Zha et al. [11].
Metrics Both the TRUE and SummaC benchmarks formulates factual consistency evaluation as
binary classification (i.e., identifying factual consistency errors). Following the common practice,
we report ROC AUC [ 65], treating each model as a classifier. For the rest of datasets, we report
instance-level Pearson, Spearman, and Kendall- τcorrelation coefficients between automatic metric
scores and human-annotated scores.
4.2.2 Results
For the LLMs-based metrics, we use the results reported by Fu et al. [57], Liu et al. [58], Gao
et al. [59], and consequently results for some model-dataset combinations are unavailable. Despite
being much smaller than ChatGPT/GPT-3.5 or GPT-4, our alignment model achieves comparable
performance on SummEval (see Table 3). When evaluated on the QAGS-XSum and QAGS-CNNDM
datasets, even our 125M alignment model outperforms both G-EV AL and GPTScore based on
GPT-3.5, while the 355M alignment model beats G-EV AL based on GPT-4. When compared with
similarly sized metrics, our method consistently outperform the strong baselines on factual consistency
benchmarks and datasets (see Figure 4). We include detailed results in Appendix D.
Table 3: The Spearman Correlation coefficient of ALIGN and GPT-based models on SummEval,
QAGS-XSUM and QAGS-CNNDM datasets. Bold number shows the best model on a specific
dataset.
G-EV AL-3.5
GPT3.5-d03G-EV AL-4
GPT4GPTScore
GPT3.5-d03ChatGPT
GPT3.5-turboALIGN -base
(Ours)ALIGN -large
(Ours)
Model Parameters — — — —- 125M 355M
DatasetsSummEval 38.6 50.7 47.5 43.3 42.0 47.9
QAGS-XSUM 40.6 53.7 22.0 — 52.7 57.4
QAGS-CNNDM 51.6 68.5 — — 56.1 71.6
4.3 Question Answering with Unanswerable Question
In question answering tasks, a system must find the correct answer to a question from a context.
When the question cannot be answered with information in the context, the system must indicate
the question is not answerable. Despite being a well-studied task, predicting whether a question is
answerable remains challenging, especially in a zero-shot setting.
A common approach to improve a system’s ability to handle unanswerable questions is to introduce
a verifier model in addition to the QA model [ 66,67]. Given a context and question pair, the QA
7Figure 4: The performance of different models on diverse factual consistency benchmarks and
datasets. The left figure includes performance on the SummaC and TRUE benchmark. The right
figure shows models measured by correlation coefficients on other datasets (Section 4.2.1).
model first predicts a candidate answer. Then, the verifier model independently predicts whether the
question is answerable by comparing the candidate answer to the question and the context. Lastly,
the outputs of the two models are aggregated to form the final prediction. In our experiments, we use
the alignment model as the verifier.
4.3.1 Experiment Setup
Datasets We experiment with two existing QA datasets with unanswerable questions,
SQuAD v2 [ 22] and ACE-whQA [ 68]. Additionally, we construct a third dataset, Simplified Natural
Questions (Simplified NQ), base on Natural Questions [ 69]. To build the dataset, for samples in Nat-
ural Questions with both short and long answers, we use the long answer as the context, and the short
answer as the ground truth answer; for samples without short and long answers, we select random
paragraphs from the articles as contexts and consider them to be unanswerable. Both ACE-whQA and
Simplified NQ are not seen by the alignment model during training (i.e., a zero-shot experiment). We
use the validation split of SQuAD v2 and Simplified NQ as their test splits are not publicly available.
Baselines We include FLAN T5 [ 14] and GPT-3.54to represent large sequence-to-sequence lan-
guage models. In addition, we experiment with using ALIGN as a verifier add-on for FLAN T5
and GPT-3.5. Here, we use 1−Pr(ybin=ALIGNED )as the unanswerable probability and use the
SQuAD v2 validation split to find the best unanswerable threshold that maximizes the F1 score. The
prompts we use and other experiment details are discussed in the appendix (Section D).
Metrics We follow Rajpurkar et al. [70] and report exact match and macro-averaged F1 score. To
evaluate each model’s performance at identifying unanswerable questions, we also formulate the
problem as a binary classification task (predicting whether the sample is answerable) and report
the ROC AUC [ 65]. A higher ROC AUC indicates the model is better at identifying unanswerable
questions. For GPT-3.5 and FLAN T5, we consider the unanswerable classifier output to be 0 if the
model predicts an answer, or 1 otherwise.
4.3.2 Results
As shown in Table 4, using ALIGN as a verifier add-on significantly improves GPT-3.5 and FLAN T5
in most cases (increases exact match score by 17.94 on average and F1 score by 15.05), suggesting
that it is effective at identifying unanswerable questions. For the Simplified NQ dataset, adding the
alignment verifier to GPT-3.5 degrades exact match and F1 score, but improves AUC. This indicates
that while ALIGN produces meaningful unanswerable probabilities on the Simplified NQ dataset,
the threshold found on the SQuAD v2 validation split is not ideal for Simplified NQ. Repeating the
experiment with the best threshold selected on the Simplified NQ validation split (see numbers in
parenthesis in Table 4) shows the potential for improvements in exact match and F1 scores, albeit this
can no longer be considered a zero-shot setting.
4gpt-3.5-turbo , see https://platform.openai.com/docs/models/gpt-3-5
8Table 4: QA experiment results. Cases where adding the ALIGN verifier improves performance are
highlighted in green. Best model for each dataset is shown in bold. For the combination of GPT-3.5 +
Verifier and Simplified NQ, we also report the exact match and F1 scores with the best unanswerable
threshold selected on the Simplified NQ validation split in parenthesis.
SQuAD v2 ACE-whQA Simplified NQ
EM F1 AUC EM F1 AUC EM F1 AUC
GPT-3.5 52.53 63.96 0.76 67.98 71.98 0.77 58.37 68.61 0.81
FLAN T5 75.72 79.01 0.83 26.29 29.24 0.51 38.24 44.98 0.58
GPT-3.5 + Verifier (Ours) 67.19 77.63 0.93 79.02 80.91 0.8456.16
(63.51)57.40
(71.83)0.86
FLAN T5 + Verifier (Ours) 83.72 86.55 0.95 75.75 77.60 0.90 64.93 67.99 0.83
4.4 Ablation Study
As discussed in Section 3.1, ALIGN is trained on datasets from a wide set of language understand
tasks. To understand their contributions to the performance of the alignment model, we conduct an
ablation study by incrementally adding subsets of tasks to the training set. Specifically, we start with
only NLI dataset, and then add the remaining tasks in the following order: 1) paraphrase detection
(para ) and fact verification ( FV) datasets; 2) coreference resolution ( coref ), summarization ( sum),
information retrieval ( IR), and STS datasets, and lastly 3) QA datasets. For simplicity, we use
RoBERTa-base as the backbone in this experiment. As shown in Table 5 each added subset improves
the overall performance of the alignment model, suggesting our training tasks are compatible and all
contributes to the model performance.
Table 5: Ablation results on in-domain natural language understanding tasks. Each row corresponds
with a model trained with data adapted from incrementally more types of tasks. For example, the
model on the second row is trained with NLI, Fact Verification and Paraphrase tasks. The model
on the last row is the same as ALIGN -base. We report the average performance for each evaluation
tasks. The last column shows the overall average for all the evaluated tasks. The best model for each
evaluated task is shown in bold .
Evaluation Tasks
Training TasksNLI Fact Verification STS Paraphrase QA Coreference Average
+NLI 69.0 65.3 53.7 69.3 56.0 70.6 64.0
+FV , Para 70.1 83.0 51.6 92.4 53.6 67.9 69.8
+Coref, Sum, IR, STS 69.9 82.9 90.3 91.9 51.8 83.2 78.3
+QA (A LIGN -base) 70.9 83.3 89.9 91.4 78.2 81.4 82.5
5 Conclusion
We propose to unify diverse language tasks into a text pair alignment problem. This framework
yields an alignment model ( ALIGN ) that, despite being less versatile than LLMs, solves a wide range
of language problems efficiently with superior performance. We show that ALIGN outperforms
task-specific models finetuned on several NLU tasks while having performance comparable to LLMs
that are orders of magnitude larger. Additionally, ALIGN excels in factual consistency evaluation,
and can be used as an add-on to augment LLMs in QA tasks by identifying unanswerable questions.
Limitations Our alignment framework uses splitting and aggregation to handle long inputs (see
Section 3), with the assumption that text x2is short and its sentences are self-contained. While we
empirically show this method works well on diverse datasets, violating this assumption has a few
implications. First, if text x2sentences are highly interrelated, splitting them discards document-level
semantic information, which could degrade performance. Second, as we need to evaluate all text x2
sentences individually, doing so will be slow for long text x2.
We use a wide collection of NLU datasets to learn the alignment function, with the assumption that
these dataset, after being adapted into the text pair alignment format, accurately reflect our definition
9of alignment. However, as with all datasets, they could contain biases that are subsequently learned by
our alignment model. Additionally, we augment the training set with synthetic data. While it proves
to improve performance in our experiments, synthetic data likely do not perfectly model real-world
data distributions.
