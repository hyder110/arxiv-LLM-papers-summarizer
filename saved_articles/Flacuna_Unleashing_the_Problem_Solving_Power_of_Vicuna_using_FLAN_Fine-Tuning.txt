FLACUNA : Unleashing the Problem Solving Power of
VICUNA using F LAN Fine-Tuning
Deepanway Ghosal‡, Yew Ken Chia‡, Navonil Majumder†, Soujanya Poria‡
‡DeCLaRe Lab, Singapore University of Technology and Design, Singapore
{deepanway_ghosal, yewken_chia}@mymail.sutd.edu.sg
{navonil_majumder,sporia}@sutd.edu.sg
CODE:https://github.com/declare-lab/flacuna
MODEL :https://huggingface.co/declare-lab/flacuna-13b-v1.0
FLAN-MINI :https://huggingface.co/declare-lab/flan-mini
Abstract
Recently, the release of INSTRUCT EVAL [Chia et al., 2023] has provided valuable
insights into the performance of large language models (LLMs) that utilize encoder-
decoder or decoder-only architecture. Interestingly, despite being introduced four
years ago, T5-based LLMs, such as FLAN-T5, continue to outperform the latest
decoder-based LLMs, such as LLAMA andVICUNA , on tasks that require general
problem-solving skills. This performance discrepancy can be attributed to three key
factors: (1) Pre-training data, (2) Backbone architecture, and (3) Instruction dataset.
In this technical report, our main focus is on investigating the impact of the third
factor by leveraging VICUNA , a large language model based on LLAMA, which
has undergone fine-tuning on ChatGPT conversations. To achieve this objective,
we fine-tuned VICUNA using a customized instruction dataset collection called
FLAN-MINI . This collection includes a subset of the large-scale instruction dataset
known as FLAN, as well as various code-related datasets and conversational datasets
derived from ChatGPT/GPT-4. This dataset comprises a large number of tasks that
Preprint. Under review.arXiv:2307.02053v1  [cs.CL]  5 Jul 2023demand problem-solving skills. Our experimental findings strongly indicate that the
enhanced problem-solving abilities of our model, FLACUNA , are obtained through
fine-tuning VICUNA on the FLAN dataset, leading to significant improvements
across numerous benchmark datasets in INSTRUCT EVAL.FLACUNA is publicly
available at https://huggingface.co/declare-lab/flacuna-13b-v1.0 .
1 Introduction
ChatGPT and its successor GPT-4 have surpassed their prior state-of-the-art models on a vast majority
of the benchmarking tasks and datasets. However, to preserve privacy, natively running a 175B+
sized model like GPT-3 is beyond the capabilities of most organizations, let alone individuals. This
has prompted many researchers to fine-tune manageable-sized LLMs — from 7B to 30B on a diverse
set of instruction examples generated by ChatGPT or GPT-4. This has birthed LLMs, such as,
Alpaca [Taori et al., 2023] and VICUNA [Chiang et al., 2023] that are fine-tuned checkpoints of
LLaMA [Touvron et al., 2023]. These models have attained close to ChatGPT-level performance on
some specific benchmarking tasks, but overall generalization still remains elusive. Recent works like
INSTRUCT EVAL [Chia et al., 2023] strongly hint that the fine-tuning datasets dictate the task-specific
performances. For instance, it has been observed that FLAN-T5— a T5checkpoint fine-tuned on
FLAN Collection instruction dataset — outperforms VICUNA and Alpaca on tasks involving strong
reasoning and problem-solving skills. This spurred us to fine-tune VICUNA onFLAN-MINI Collection
dataset, anticipating improvement on reasoning-intensive tasks in INSTRUCT EVAL [Chia et al., 2023].
To this end, we first sample a 1M-sized instruction dataset from the 15M-sized FLAN Collection
dataset [Longpre et al., 2023] and combined it with several other datasets comprising coding tasks
and ChatGPT/GPT-4 distilled conversations. The resulting smaller dataset, FLAN-MINI , is then
cast into the conversational format of VICUNA . To ensure a reasonable computational cost for the
fine-tuning process, we retrofit LoRA [Hu et al., 2021] adapter into the LLaMA [Touvron et al., 2023]
decoder-transformer of VICUNA . Following a parameter-efficient LoRA fine-tuning of the VICUNA
checkpoint on FLAN-MINI , we obtain FLACUNA . As expected, FLACUNA outperforms VICUNA by a
substantial margin on most benchmark datasets, especially for reasoning-intensive tasks. However,
the performance of FLACUNA still remains below FLAN-T5on the same reasoning benchmarks. This
could be attributed to the 15-times smaller dataset of the instruction dataset which may contain less
diverse samples. Furthermore, full fine-tuning of V ICUNA may narrow the gap with F LAN-T5.
This work overall has the following contributions:
1.Improving the problem-solving capability of VICUNA through parameter efficient fine-tuning on
FLAN-MINI .
2.Introducing an instruction tuning dataset, FLAN-MINI , comprising a diverse set of tasks and
templates.
2 Training Details
Preparing the FLAN-MINI Collection. Given the enormous size of the FLAN Collection [Longpre
et al., 2023], we opted to work with a carefully selected subset that maintains a high level of task
diversity while reducing the overall dataset size. In Table 1, we present the specific tasks included
in our subset of FLAN, along with their respective dataset sizes. As the public release of the
FLAN Collection does not include programming tasks, we augment the collection with existing
code datasets. Specifically, we include CodeContests [Li et al., 2022a], APPS [Hendrycks et al.,
2021a] and CodeSearchNet [Husain et al., 2019a]. Following the data processing pipeline of FLAN
Collection, we sample a fixed number of examples from each dataset, where each example is randomly
augmented with different prompt templates. Specifically, the examples are processed with a pool
of handcrafted prompt templates and may be used as zero-shot examples or grouped together with
few-shot demonstrations [Longpre et al., 2023].
Maintaining VICUNA ’SChatting Ability. VICUNA has demonstrated remarkable chatting abil-
ity, achieving 90% of the performance of ChatGPT. This indicates its significant potential as an
open-source alternative to closed-source large language models (LLMs) like ChatGPT. To ensure
2Dataset Name Source Dataset Size
Flan2021 F LAN 388K
Public Pool of Prompts F LAN 320K
Natural instructions v2 F LAN 200K
CoT F LAN 100K
Code Search Husain et al. [2019b] 100K
Code Contest Li et al. [2022b] 50K
Apps Hendrycks et al. [2021b] 50K
GPT4-Alpaca GPT-4 52K
Code-Alpaca ChatGPT 20K
ShareGPT ChatGPT 60K
Total - 1.34M
Table 1: The F LAN-MINI Collection, used to train F LACUNA .
thatFLACUNA retains VICUNA ’s learned knowledge and chatting ability, we incorporated various
ChatGPT datasets, including Alpaca [Taori et al., 2023], Code Alpaca [Chaudhary, 2023], and
ShareGPT [Chiang et al., 2023], into our FLAN collection. Among these three datasets, VICUNA
was originally fine-tuned using the ShareGPT dataset. The final collection was then used to train
FLACUNA .
Architecture. We employed LORAin the VICUNA model for fine-tuning on the FLAN-MINI
collection. We inserted the low-rank adapters on all the query and value projection layers, resulting
in a total trainable parameter count of 6.55M, which is only around 0.05% of the parameter count of
the original 13B VICUNA model. The maximum input sequence length was set to 1280, and efficient
training was facilitated by utilizing bf16 precision.
Hyperparameter Details. FLACUNA was trained on 4 ×A6000 GPUs for 1 epoch. We use 16
gradient accumulation steps with a per-device batch size of 2, resulting in a total batch size of 128.
We used 3000 warm-up steps and a learning rate of 2e-5.
3 Evaluation Tasks and Results
3.1 Problem Solving Evaluation
To assess the problem-solving prowess of instructed large language models (LLMs), INSTRUCT EVAL
employs a range of benchmarks encompassing real-world exams that delve into diverse topics. These
benchmarks encompass complex instructions, arithmetic problems, programming challenges, and
causal reasoning tasks. In order to excel in these benchmarks, models need to exhibit a profound
understanding of the world, demonstrate multi-hop reasoning capabilities, showcase creativity, and
employ a plethora of other cognitive skills.
World Knowledge. The Massive Multitask Language Understanding (MMLU) benchmark, intro-
duced in the work by Hendrycks et al. [2021c], serves as an assessment tool to gauge the problem-
solving aptitude and world knowledge of language models across various subjects. It offers evalua-
tions in both zero-shot and few-shot settings, presenting a more challenging and human-like evaluation
scenario. The MMLU benchmark encompasses a comprehensive range of 57 subjects spanning
STEM, humanities, social sciences, and other domains. The difficulty levels of the tasks within
the benchmark vary from elementary to advanced professional levels, providing a comprehensive
assessment of the model’s capabilities in problem-solving and domain understanding.
Complex Instructions. The subset known as BIG-Bench Hard (BBH) comprises 23 highly demand-
ing tasks carefully selected from the BIG-Bench benchmark [Srivastava et al., 2022] to specifically
target tasks that are considered to surpass the current capabilities of language models [Suzgun et al.,
2022]. BBH presents models with intricate instructions that require advanced skills in navigation,
logical deduction, and fallacy detection.
3Comprehension and Arithmetic. Discrete Reasoning Over Paragraphs (DROP) is a reading
comprehension task with a mathematical focus. It challenges systems to engage in discrete reasoning
by analyzing passages extracted from Wikipedia articles. In order to excel in the DROP task, a system
needs to adeptly navigate 