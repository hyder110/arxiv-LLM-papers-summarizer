Summary:

This paper presents a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. The proposed approach transfers knowledge from a larger, more complex model to a smaller, lightweight model using dual-view cross-correlation distillation and teacher codebook distillation. It showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting while working within on-device resource constraints.

Bullet Points:

- Large self-supervised models are effective feature extractors, but challenging to implement on-device due to resource constraints and biased datasets.
- The authors propose a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting.
- The approach uses a dual-view cross-correlation distillation and teacher codebook distillation to transfer knowledge from a larger model to a smaller, lightweight model.
- The model showed exceptional performance in normal and noisy conditions.
- The proposed approach addresses the challenges of resource constraints and biased datasets in keyword spotting.
- The experiments were conducted on an internal dataset and demonstrated the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting.
- The results showed that the dual-view cross-correlation distillation method outperforms single-view approaches.
- Integrating the teacher codebook as a training objective improved the performance, especially under noisy conditions.
- The proposed approach provides an efficient and effective on-device constrained self-supervised model for keyword spotting.
- The findings emphasize the efficiency and effectiveness of innovative knowledge distillation methods in developing on-device constrained self-supervised models for keyword spotting tasks.

Keywords:

- self-supervised learning
- knowledge distillation
- dual-view cross-correlation
- keyword spotting
- on-device