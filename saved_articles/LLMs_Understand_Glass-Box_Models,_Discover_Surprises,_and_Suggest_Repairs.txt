LLMs Understand Glass-Box Models, Discover Surprises,
and Suggest Repairs
Benjamin J. Lengerich*1,2, Sebastian Bordt†3, Harsha Nori4, Mark E. Nunnally5, Yin
Aphinyanaphongs5, Manolis Kellis‡1,2, and Rich Caruana§4
1Massachusetts Institute of Technology
2Broad Institute of MIT and Harvard
3University of Tübingen, Tübingen AI Center
4Microsoft Research
5NYU Langone Health
August 3, 2023
Abstract
We show that large language models (LLMs) are remarkably good at working with interpretable models that
decompose complex outcomes into univariate graph-represented components. By adopting a hierarchical approach
to reasoning, LLMs can provide comprehensive model-level summaries without ever requiring the entire model to
fit in context. This approach enables LLMs to apply their extensive background knowledge to automate common
tasks in data science such as detecting anomalies that contradict prior knowledge, describing potential reasons for
the anomalies, and suggesting repairs that would remove the anomalies. We use multiple examples in healthcare to
demonstrate the utility of these new capabilities of LLMs, with particular emphasis on Generalized Additive Models
(GAMs). Finally, we present the package TalkToEBM as an open-source LLM-GAM interface.
Introduction
Large language models (LLMs) offer the potential to automate data science through natural language interfaces, but
it is difficult to embed complex models or datasets in confined context windows. While GPT-4 has a context window
size of up to 32k tokens, paying equal attention to all parts of the context remains a challenge [1] and the practicality of
lengthy context windows is questionable. Machine learning models often involve billions of parameters, accentuating
the need for compact, modular function representations that more easily interface with LLMs.
In this paper, we show that LLMs pair remarkably well with interpretable models that are decomposable into
modular components. Specifically, we show that GPT-4 is able to describe, interpret and debug univariate graphs, and
by applying a form of chain-of-thought reasoning[2], GPT-4 can understand Generalized Additive Models (GAMs).
GAMs [3, 4] represent complex outcomes as sums of univariate component functions (graphs); thus, by analyzing each
of these component functions in turn, the LLM does not need to understand the entire model at once. After analyzing
and summarizing each graph, the LLM can operate on component summaries to produce model-level analyses. This
modularity simplifies the application of LLMs to data science and machine learning and enables LLM-based analyses
to scale to very large datasets while staying within small context windows.
*blengeri@mit.edu
†Work done while at Microsoft Research.
‡manoli@mit.edu
§rcaruana@microsoft.com
1arXiv:2308.01157v1  [stat.ML]  2 Aug 2023Prior KnowledgeDataInterpretMLExplain GraphExplain ModelFind Surprising Effects
TalkToEBMGlass-Box ModelFind Paradoxical Aberrations
Find Internal InconsistenciesLLMFigure 1: Our approach to connect LLMs with data interpretations. By using glass-box interpretable models that
can be decomposed into modular components, we enable the LLM to use hierarchical reasoning by considering each
modular component in turn. This enables the LLM to analyze large models and perform complex reasoning tasks
without requiring the entire model to be stored in a single context window.
Next, we show that because LLMs have large amounts of prior knowledge, LLMs can be used to provide model
interpretations that are grounded in domain expertise. Specifically, we show that LLMs can automatically detect
surprises and anomalies in models such as GAM graphs that appear to contradict expectations. By highlighting these
surprises, the LLM can suggest problems in all aspects of the analysis, including data collection, imputation, model
fitting, or model specification. This is critical for good data science because real-world datasets are universally polluted
by confounders and systemic biases that are often obvious only after close examination with domain expertise (e.g.
treatment effects in healthcare [5]); LLMs with extensive prior knowledge offer a system to automatically detect and
report these potential problems. Depending on the application, these potential problems may require users to correct
the data, correct the model, or change the underlying system.
Ultimately, LLMs offer the potential to be used as tools that automate important yet repetitive aspects of the data
science process. LLMs are especially useful when paired with statistical tools, such as glass-box interpretable models,
that break complex reasoning tasks into separable components. While LLMs such as GPT-4 are not yet able to directly
understand large tables of tabular data, they are able to interface with glass-box GAMs trained on that data, providing
a new opportunity for humans to interact and learn with their data.
Our Approach
Our approach (Figure 2) is to use model representations that provide separable components. Separable components
can be independently analyzed, summarized, and combined with hierarchical reasoning.1Glass-box models (e.g.
GAMs) that can be decomposed into univariate component graphs fit this approach because the univariate components
can be separated and later combined without approximation. Thus, we use GAMs to split the complex task of reasoning
about an entire model into a sequence of small tasks of reasoning about individual graphs. Concretely, we prompt the
LLM with a general introductory prompt about the task, model and data set, then successively provide each graph as
key-value lists. For each graph, we can ask the LLM to perform various tasks (for example, to summarize and answer
questions about the graph). To draw conclusions about the entire model, the summaries of individual graphs are
aggregated in a new query to the LLM. This approach is available in our open-source software package TalkToEBM .
1This approach of first using component-level prompts before using model-level prompts is similar to chain-of-thought reasoning [2] which
attempts to improve LLM reasoning by asking the LLM to explicitly work through a problem, progressively solving more difficult reasoning tasks.
2Related Work
The disruptive potential of using LLMs to automate tasks in data science has inspired several related approaches. Slack
et al. [6] develop a natural language interface to give practitioners conversational access to model explanations. Their
work intends to provide access to generic (potentially black-box) models, and so the LLM does not have direct access
to model internals. As a result, [6] cannot use the same chain-of-thought approach we explore with GAMs to enable
scalability and complex model-level reasoning.
On the other end of the pipeline for automated data science, Bisercic et al. [7] showed that LLMs can extract
tabular datasets from unstructured text and then train interpretable models (e.g. linear regression and small decision
trees) on top of this data. Similarly, recent works have explored the potential of LLMs for data wrangling and cleaning
[8, 9], or traditional supervised and unsupervised learning tasks like classification or density estimation [10–13]. These
works usually rely on fine-tuning, suggesting that today’s LLMs have only a limited ability to solve complex tasks
with raw tabular data in-context. All in all, these approaches to automated data preprocessing and model estimation
are complementary systems to the analytical system proposed in this paper, together suggesting a path toward fully
automated data science which includes automated data preprocessing, model fitting, and interpretations.
The Importance of Iterative Investigations in Data Science While the predictive accuracy of machine learning
tools is undoubtedly crucial, it is not sufficient as a standalone objective when employing these tools to analyze and
influence real-world systems. Instead of singularly pursuing accuracy, we need to embark on a cyclical investigative
process to comprehend the model’s nuances, the effects it has learned, and the implications it holds for the dataset
and corresponding real-world system. Taking healthcare data as an instance, we may develop a system that effectively
predicts the outcomes for hospitalized patients. However, applying this retrospectively derived knowledge to future
diagnostic policies can lead to flawed treatment decisions. This occurs because the patients identified as low-risk in
the retrospective study are those who received effective treatment, not necessarily those who inherently bore the least
risk [4]. Moreover, relying heavily on predictive accuracy can be deceptive, particularly considering the inconsistent
recording of real-world healthcare data [14], which could lead to statistical endorsement of unrelated confounding
factors [15]. Hence, using predictive accuracy as an end goal, our goal is to employ interpretable models to uncover
unexpected effects. This mirrors the philosophy proposed by [5], which demonstrated that systemic confounding
presents opportunities to enhance healthcare. This prior work relied on human experts to design explicit statistical
tests for each category of surprising pattern in GAM feature graphs, providing a handle on false-discovery rates but
limiting the breadth of potential applications. Our work builds on this insight that interpretable models reveal hidden
confounding and proposes to automate surprise discovery with LLMs.
Results
We begin by examining the ability of LLMs to understand individual graphs. We next outline how this capability can
be used to explain entire models and summarize data sets. Finally, we use the domain knowledge embedded in the
LLM to find surprising effects (i.e. hidden confounding). We are particularly interested in the use of this approach
to identify confounding factors and suboptimal treatment protocols in observational healthcare data; thus, in all of
these experiments, we use a dataset of pneumonia patients from [16] as a running example. The approach is broadly
applicable and more examples on publicly available datasets are included in the package repository TalkToEBM .
LLMs Understand Graphs
LLMs are able to understand and reason about simple graphs. The main challenge is to encode each graph in a text
description which enables straightforward reasoning. Piecewise-constant graphs, such as those estimated by tree-
based GAMs, can be exactly described as a list of regions and values. By encoding piecewise-constant graphs as
JSON objects of x-bins and y-values (Figure 2A), we pass an exact and efficient description of the graph to the LLM.
The LLM can summarize this encoding (Figure 2B) and enter into a dialogue with the user (Figure 2C).
For example, consider the following question about the influence of age on pneumonia mortality risk (Figure 2A):
Question: What would be the average risk of a 82-year old patient?
3A. Graphs can be encoded as text...
B. and understood by LLMs...
C. to enable conversational data science.
This graph represents a continuous-valued feature. The keys areintervals that represent ranges where the function predicts the samevalue.Feature Name: AgeFeature Type: continuousMeans: {"(18.0, 50.5)": -0.467, …,"(88.5, 106.0)": 0.503}Lower Bounds: {"(18.0, 50.5)": -0.727, …, "(88.5, 106.0)": 0.392}Upper Bounds: {"(18.0, 50.5)": -0.207, …, "(88.5, 106.0)": 0.614}GPT-4:"[…]The general pattern of the graph indicates that as the age increases, the log-odds contribution also increases, meaning that older age is associated with a higher risk of death due to pneumonia.In the age range from about 18 to 67.5 years, the contribution to the log-odds is negative, which means the model predicts a lower risk of death from pneumonia in this age group. Starting from around 69.5 years, the contribution becomes positive, indicating ahigher risk of death from pneumonia as the patient's age increases.It's important to note that the exact turning point where the risk increases can vary, as indicated by the lower and upper bounds for each age range. These bounds represent the 95% confidence interval, which is a measure of the uncertainty around the estimate. However, the overall trend remains the same: older age is associated with higher risk of death from pneumonia.In conclusion, age is a significant predictor of pneumonia-related mortality according to this model, with older patients generally at higher risk."SummarizationQ: Does the risk increase monotonically with age?GPT-4:"Yes, according to the provided data, the risk of death from pneumonia increases monotonically with age. [...]”ApplicationQ: What would be the average risk of an 82-year oldpatient?GPT-4:"[…] according to the model, an 82-year oldpatient would have a 57.2% probability of dying from pneumonia on average. However, remember this is a model-based prediction and actual risk could be different based on other patient-specific factors not included in this model."SimplificationQ: What is the best trendline? Express your answer as a linear equation of the form y=mx+b.GPT-4:"[…]Therefore, the best trendline(linear equation) forThe graph is:y ≈ -0.558006+0.003896x"AgeAddition to Mortality Log-Odds
AgeAddition to Mortality Log-OddsFigure 2: Graphs can be encoded into JSON text and understood by an LLM. GPT-4 successfully understands the
graph and enables conversational dialogue to speed up data science.
4GPT-4: "The average risk for an 82-year old patient can be derived from the mean log-odds contribution
for the age interval "(81.5, 85.5)", which is 0.29.
Log-odds are a way of expressing probability and can be converted into a probability with the following
formula: p = exp(log-odds) / (1 + exp(log-odds)).
Using this formula, the average risk is approximately 57.2%.
This means that, according to the model, an 82-year old patient would have a 57.2% probability of dying
from pneumonia on average. However, remember this is a model-based prediction and actual risk could
be different based on other patient-specific factors not included in this model."
This answer is correct since the relevant interval in the graph is indeed (81.5, 85.5): 0.29 and the arith-
metic translation from log-odds to probability is correct. The LLM can also do simple reasoning about this graph:
Question: Does the risk increase monotonically with age?
GPT-4: "Yes, according to the provided data, the risk of death from pneumonia increases monotonically
with age. [...]"
Question: Which patients are most at risk?
GPT-4: "According to the model’s output, the patients most at risk are those in the oldest age group,
specifically those aged 88.5 to 106 years. [...]"
More complicated analysis questions begin to push at the bounds of GPT-4’s reasoning capabilities:
Question: What is the average increase in risk due to a 5-year increase in age?
GPT-4: "To calculate the average increase in risk due to a 5-year increase in age, we need to look at the
mean log-odds contributions of adjacent 5-year age groups and calculate the difference. However, the
age groups provided are not all of equal length, so we cannot calculate an exact average increase in risk
for a 5-year increase in age.
If we wanted to estimate this, we could consider the log-odds difference between the "(77.5, 80.5)" and
"(81.5, 85.5)" age groups. The mean log-odds contribution increases from 0.216 to 0.29, a difference of
0.074 in log-odds.
To convert this difference in log-odds to a difference in probability, we would use the formula: p =
exp(log-odds) / (1 + exp(log-odds)).
Applying this formula, we find that a 5-year increase in age from 80 to 85 years is associated with an
increase in the average risk from about 53.8% to 57.2%, which is about a 3.4% increase in risk due to a
5-year increase in age.
Bear in mind that this is an approximation based on the provided data, and the actual increase in risk
may vary depending on the specific age range and other factors. Also, as previously mentioned, there are
many other factors that can influence a patient’s risk of dying from pneumonia, not just age."
This answer from GPT-4 is factually correct but incomplete. We asked for the average increase in risk represented
by the graph, but GPT-4 opted to answer only about two local averages. This example demonstrates the common
need for LLMs to perform guided bottom-up analyses rather than unguided global queries, motivating our strategy to
interface with the LLM via modular function representations rather than non-separable black-box models.
LLMs Understand Interpretable Models
GAMs are separable into a set of univariate component functions (Figure 3A). Starting from the univariate component
graphs, we use chain-of-thought prompting [2] to summarize the effects of the different features in the model. For
example, in the pneumonia dataset, the model summarizes the role of ‘respiration rate’ as follows:
5This graph represents a continuous-valued feature. The keys areintervals that represent ranges where thefunction predictsamevalue.Feature Name: AgeFeature Type: continuousMeans: {"(18.0, 50.5)": -0.467, …,"(88.5, 106.0)": 0.503}Lower Bounds: {"(18.0, 50.5)": -0.727, …, "(88.5, 106.0)": 0.Upper Bounds: {"(18.0, 50.5)": -0.207, …, "(88.5, 106.0)": 0.r𝑃(𝑌|𝑋)=𝜎𝑓!+𝑓"𝑋"+𝑓#𝑋#+⋯+𝑓$𝑋$⋯A. GAMsestimate separable component graphs…
B. that can each be encoded in text…
C. and analyzed jointly by LLMs.⋯
This graph represents a continuous-valued feature. The keys areintervals that represent ranges where the function predicts the samevalue.Feature Name: AgeFeature Type: continuousMeans: {"(18.0, 50.5)": -0.467, …,"(88.5, 106.0)": 0.503}Lower Bounds: {"(18.0, 50.5)": -0.727, …, "(88.5, 106.0)": 0.392}Upper Bounds: {"(18.0, 50.5)": -0.207, …, "(88.5, 106.0)": 0.614}2
This graph represents a continuous-valued feature. The keys areintervals that represent ranges where the function predicts the samevalue.Feature Name: AgeFeature Type: continuousMeans: {"(18.0, 50.5)": -0.467, …,"(88.5, 106.0)": 0.503}Lower Bounds: {"(18.0, 50.5)": -0.727, …, "(88.5, 106.0)": 0.392}Upper Bounds: {"(18.0, 50.5)": -0.207, …, "(88.5, 106.0)": 0.614}1
GPT-4: "The model predicts the likelihood of death from pneumonia based on several patient features. The most influential global feature is age, with increasing age associated with a higher likelihood of death, particularly after the age of 73. Respiration rate and BUN level also have substantial influences on the model's predictions, with higher levels of both associated with an increased risk of death.Other significant features include the number of diseases a patient has, their systolic blood pressure, and their pO2 levels. More diseases, lower systolic blood pressure, and very low or very high pO2 levels are all associated with a higher likelihood of death.The model also shows that certain medical readings can increase or decrease the likelihood of death, depending on their levels. For example, both very low and very high glucose levels, […]In summary, the model provides an intricate and nuanced prediction of the likelihood of death from pneumonia based on a wide range of patient features. […]"Figure 3: LLMs can be used to reason about complicated outcomes if the outcomes are first broken down into an
additive model before passing to the LLM.
6Question: "Now provide a brief executive summary of the role of the feature in the model. Assume that
the reader already knows what the model is about, so you don’t need to describe the outcome or feature
details. Use at most 7 sentences. Avoid boilerplate sentences like ’other factors may also impact this
likelihood’ or ’further analysis would be needed’."
Respiration Rate (Feature Summary): "The model’s output indicates an increasing likelihood of death
as the respiration rate increases. Specifically, a respiration rate of 32 or less tends to decrease the likeli-
hood of patient death, while a rate above 32 increases this likelihood. However, the model exhibits some
uncertainty in its predictions, as shown by overlapping confidence intervals across different respiration
rate intervals. This suggests that while the respiration rate is influential, its precise impact may vary
between patients."
We then aggregate the different summaries and ask the LLM to provide an overall summary of the GAM. We can
also provide the model with additional information. Because GAMs also provide straightforward measures of feature
importance, we also provide the LLM with global feature importance scores.
Model Summary (GPT-4, Pneumonia): "The model predicts the likelihood of death from pneumonia
based on several patient features. The most influential global feature is age, with increasing age associ-
ated with a higher likelihood of death, particularly after the age of 73. Respiration rate and BUN level
also have substantial influences on the model’s predictions, with higher levels of both associated with an
increased risk of death.
Other significant features include the number of diseases a patient has, their systolic blood pressure, and
their pO2 levels. More diseases, lower systolic blood pressure, and very low or very high pO2 levels are
all associated with a higher likelihood of death.
The model also shows that certain medical readings can increase or decrease the likelihood of death,
depending on their levels. For example, both very low and very high glucose levels, pH levels, and
potassium levels can increase the risk of death, while moderate levels are associated with a lower risk.
Finally, the model shows a considerable amount of uncertainty in its predictions for certain features, such
as age, heart rate, and albumin levels. This is likely due to a lack of data points in certain ranges or the
complex, non-linear relationships these features may have with the outcome.
In summary, the model provides an intricate and nuanced prediction of the likelihood of death from
pneumonia based on a wide range of patient features. However, given the complexity of the relationships
and the uncertainty in some predictions, the model’s output should be interpreted with caution and used
in conjunction with clinical judgement."
LLMs Automatically Find Surprising Effects
Finally, LLMs can apply their embedded domain knowledge to provide grounded interpretations of these graphs and
models. We are particularly motivated to use LLMs to automatically detect surprising effects, as this is a common but
difficult task in data analysis that typically requires domain expertise and iteratative data exploration, model fitting,
and evaluation. However, LLMs can automate this process by generating a ranked list of surprises that may indicate
underlying model or data problems. To accomplish this, we include in our system prompt an instruction to pay
attention to abnormal and surprising effects:
System Prompt: "[...] Pay special attention to values or patterns that appear abnormal. It is very
important that you alert the user about these potentially surprising aspects of the graphs."
Once the LLM has been prompted to save notes about these surprises, we can subsequently ask the LLM to
summarize and rate the different surprises. Surprises can be identified in individual graphs:
Question: "Great, now summarize the the most important surprising aspects of this graph in at most 5
sentences."
7Component GraphGPT-4 Surprises"The graph of albumin levels versus predicted log-odds of death from pneumonia reveals several surprising aspects. Firstly, it includes negative albumin levels, which are physiologically impossible and might represent data errors. Secondly, the graph shows an initial increase in predicted death risk with increasing albumin levels up to about 2.1, contrary to typical clinical understanding that lower albumin levels usually signify poorer health status. Lastly, there's a sharp decrease in death risk for albumin levels above 2.6, suggesting potential interactions with other factors not captured in the model. These findings highlight the need for careful data validation and consideration of the multifaceted relationships between health measures.""The graph of sodium levels versus the risk of dying from pneumonia reveals three surprising aspects: 1) a sharp decrease in risk at very high sodium levels (159 and above), contrary to the expectation of higher risk with severe hypernatremia; 2) a peculiar peak in risk at sodium levels of 156 to 159, suggesting possible overfitting or unaccounted confounding factors; 3) a lower-than-expected risk at very low sodium levels (0 to 110), potentially due to aggressive treatment of severe hyponatremia. These unexpected patterns may indicate data quality issues, overfitting, or unaddressed confounders, warranting further investigation."
"The most surprising aspect of the heart rate graph is the unexpected drop in the predicted log-odds of death from pneumonia for patients with extremely high heart rates, specifically in the 170 to 200 beats per minute range. This is contrary to medical expectations, as severe tachycardia (high heart rate) is generally associated with a higher risk of death in severe illnesses like pneumonia. Possible explanations for this counterintuitive patterncould include data quality issues, presence of confounding factors, or limited data in this heart rate range. The wide confidence intervals for this range also indicate a higher level of uncertainty in the model's predictions. This unusual pattern requires further investigation to ensure accurate interpretation and application.""The graph for creatinine level shows a surprising decrease in the predicted log-odds of death from pneumonia as creatinine levels increase beyond approximately 5.0 and then again after about 10.4, which is counterintuitive given that higher creatinine levels usually indicate worse health outcomes. This could potentially be due to a skewed distribution of data with fewer patients having very high creatinine levels, causing the model to learn inaccurate patterns. Alternatively, it might reflect that patients with extremely high creatinine levels receive more aggressive or specialized care, improving their outcomes. The wide confidence intervals in these ranges also suggest a high level of uncertainty in the model's predictions. […]""The graph depicting the relationship between 'percentage_bands' and the predicted death from pneumonia shows a surprising nonlinear pattern. Most notably, the risk of death sharply increases when 'percentage_bands' is within the range of 42.0 to 62.0. After this peak, the risk decreases, but still remainshigher than at lower 'percentage_bands' values. These patterns suggest a possible threshold effect or could indicate issues with the data. Further investigation is needed to understand these unexpected patterns."A
B
DC
EFigure 4: Pneumonia effects and surprises. In each example, we show the estimated impact on patient mortality (left)
with 95% confidence intervals shaded and observed training data points as red ticks along the horizontal axis, and
(right) the surprises reported by GPT-4.
8GPT-4: "The graph for creatinine level shows a surprising decrease in the predicted log-odds of death
from pneumonia as creatinine levels increase beyond approximately 5.0 and then again after about 10.4,
which is counterintuitive given that higher creatinine levels usually indicate worse health outcomes. This
could potentially be due to a skewed distribution of data with fewer patients having very high creatinine
levels, causing the model to learn inaccurate patterns. Alternatively, it might reflect that patients with
extremely high creatinine levels receive more aggressive or specialized care, improving their outcomes.
The wide confidence intervals in these ranges also suggest a high level of uncertainty in the model’s
predictions. These surprising patterns warrant further investigation into the data and consultation with
clinical experts."
By asking the model to summarize the entire model as a ranked list of surprises, we generate an ordered list of
hypotheses to investigate for confounding or novel discoveries (Figure 4).
Finding Surprises and Abnormal Patterns (GPT-4, Pneumonia): "The Generalized Additive Model
(GAM) predicts the likelihood of death from pneumonia based on patient factors. However, it reveals
several surprising patterns.
The most surprising factors are Heart Rate, Systolic Blood Pressure, Albumin Level, Percentage Bands,
Creatinine Level, and Sodium Level.
The Heart Rate graph shows a decrease in predicted risk of death for patients with extremely high heart
rates, contrary to medical expectations.
The Systolic Blood Pressure graph shows a counterintuitive pattern: as blood pressure increases, the risk
of death decreases, which is surprising as high blood pressure usually indicates severe health risks.
The Albumin levels graph includes negative albumin levels, which are impossible, and shows an initial
increase in predicted death risk with increasing albumin levels up to about 2.1, contrary to typical under-
standing that lower albumin levels signify poorer health.
The ’Percentage Bands’ graph shows a surprising nonlinear pattern, with the risk of death sharply in-
creasing when ’percentage_bands’ is within the range of 42.0 to 62.0.
The Creatinine Level graph shows a decrease in predicted risk of death as creatinine levels increase
beyond approximately 5.0, which is counterintuitive as higher creatinine levels usually indicate worse
health outcomes.
The Sodium Levels graph reveals a sharp decrease in risk at very high sodium levels, a peak in risk at
levels of 156 to 159, and a lower-than-expected risk at very low sodium levels, all contrary to standard
medical expectations.
These patterns may indicate overfitting, unknown confounding factors, data quality issues, or unique
dataset characteristics. Further investigation is necessary for accurate clinical decision-making."
Directly interfacing with LLMs can lead to unstructured and confusing results. For example, the second surprise
listed for albumin (“an initial increase in predicted death risk with increasing albumin levels up to about 2.1”, Figure
4A) can be interpreted in multiple ways. Is the LLM referring to the change in mortality risk from 0.0 to 2.1g/dL
that is mostly attributable to the effect of missing albumin values at 0.0g/dL, the change in mortality risk from 1.0 to
2.1g/dL that is mostly attributable to the effect of the jump at 1.5 g/dL, or the gradual increase in mortality risk from
1.5 to 2.1 g/dL? This ambiguity motivates development of standardized graph-LLM interfaces and methods to verify
LLM interpretations prior to presenting them to the user.
Discussion
In this study, we have shown that because LLMs are remarkably good at understanding graphs, they are also able to
understand and explain glass-box Generalized Additive Models (GAMs) which decompose learning into an additive
sum of feature functions represented as graphs. Moreover, LLMs are able to use their extensive background knowledge
to detect and describe anomalies in the learned feature functions that defy expectations. While the experiments and
results reported here are preliminary, they encourage future exploration to automate data science with LLMs.
9Memorization, Data Pollution, and Hallucinations: What Happens if the Data or Models
Trained on the Data Are in the LLM’s Train Set?
One important concern in using LLMs to automate data science is whether the LLM itself was trained on the studied
data, papers written about interpreting models trained on that data, or similar datasets that may cause the LLM to
hallucinate insights. See [17] for a discussion of this issue on medical datasets. In our preliminary experiments
reported here, the pneumonia data we use has not been made public, so we are confident that the pneumonia data is
not in the LLM’s train set. However, there have been several papers that discuss interpretable models trained on the
pneumonia data [4] and the LLM may have been trained on these papers. As a result, LLMs hallucinate results when
prompts are too open-ended.
For example, GPT-3.5 hallucinates surprises about pneumonia patients without ever being given a model:
Question: “You are an expert statistician and data scientist. Your task is to interpret global explanations
produced by a generalized additive model (GAM). GAMs produce explanations in the form of graphs that
contain the effect of a specific input feature. Answer all questions to the best of your ability, taking into
consideration your knowledge about the real world. Pay special attention to values or patterns that appear
abnormal. It is very important that you alert the user about these potentially surprising aspects.
This model represents outcomes of hospitalized patients with pneumonia. The outcome is in-hospital
mortality. What surprises do you find? For each surprise, tell me the specific feature, the feature values
(bins) that have surprising effects, and rank the surprise on a scale from 0-5 where 0 means unsurprising
and 5 means surprising.”
GPT-3.5: “As an expert statistician and data scientist, I’ll interpret the global explanations produced by
the generalized additive model (GAM) for the outcomes of hospitalized patients with pneumonia. Let’s
discuss the surprises:
1. Surprise: Age has a surprising effect on in-hospital mortality. Specific Feature: Age Feature Values
(Bins): The age bins where the effect is surprising are 18-30 and 70-85. Ranking: 5/5 Explanation: The
GAM model shows that patients between the ages of 18 to 30 and 70 to 85 have a significantly higher or
lower in-hospital mortality rate compared to other age groups. This finding is unexpected as one might
assume that older patients would generally have a higher mortality rate. Further investigation is warranted
to understand the underlying factors contributing to this surprising effect.
2. Surprise: Length of hospital stay has a surprising effect on in-hospital mortality. Specific Feature:
Length of Hospital Stay Feature Values (Bins): The effect is surprising for patients with a length of stay
between 1 to 3 days. Ranking: 4/5 Explanation: The GAM model indicates that patients with a length of
stay between 1 to 3 days have a notably higher mortality rate compared to those with slightly shorter or
longer stays. This finding is unexpected as shorter hospital stays are often associated with better outcomes,
but here, it seems to be the opposite. Further investigation is needed to understand the reasons behind this
surprising trend.
3. Surprise: Blood pressure has a surprising effect on in-hospital mortality. Specific Feature: Blood
Pressure Feature Values (Bins): The effect is surprising for patients with extremely high or low blood
pressure values. Ranking: 5/5 Explanation: The GAM model reveals that patients with either extremely
high or extremely low blood pressure readings have a significantly higher risk of in-hospital mortality
compared to patients with more moderate blood pressure values. This unexpected finding contradicts the
common belief that extremely high blood pressure is the primary concern.2Further analysis is crucial to
comprehend the reasons for this surprising relationship.
4. Surprise: Seasonal effect on in-hospital mortality. Specific Feature: Season (Time of Year) Feature
Values (Bins): The effect is surprising during the winter season. Ranking: 3/5 Explanation: The GAM
model suggests that in-hospital mortality rates are higher during the winter season compared to other
seasons. While it is not entirely surprising to observe some seasonal variations in health outcomes, the
2In its hallucination, the LLM invented a belief – it is well known that extremely low blood pressure is linked to in-hospital mortality [18].
10magnitude of the effect during the winter is unexpected. This finding warrants further investigation to
identify potential factors contributing to increased mortality during this time.
5. Surprise: Smoking status has a surprising effect on in-hospital mortality. Specific Feature: Smoking
Status Feature Values (Bins): The effect is surprising for current smokers. Ranking: 4/5 Explanation:
The GAM model indicates that current smokers have a lower mortality rate compared to non-smokers or
former smokers. This finding is unexpected as smoking is generally associated with adverse health out-
comes, including higher mortality rates. Further analysis is needed to understand the potential underlying
factors contributing to this surprising relationship.
Please note that these surprises are based on the statistical analysis of the GAM model’s output and might
require further investigation and validation with domain experts and additional data to draw definitive con-
clusions. Interpretation of statistical models should always be done cautiously, considering the complexity
of real-world factors that may influence the outcomes.”
In this example, we never gave the LLM any model to interpret, but the LLM still readily volunteered hallucinated
“surprises” and gave plausible explanations. This behavior is undesirable. GPT-4 correctly provides hypothetical
insights in response to this malformed question:
GPT-4: “I’m sorry for the confusion, but I’m unable to view or interpret specific graphs or data directly
as I don’t have access to external databases or the capability to view images or graphs. As an AI text
model developed by OpenAI, I work purely on the basis of the text-based inputs and my training data up
to September 2021.
However, I can provide general insights on how to interpret generalized additive model (GAM) results.
In GAMs, a separate smooth function is estimated for each predictor. If you look at the plot for each
predictor, the y-axis shows the effect on the response after accounting for the average effects of all other
predictors. It’s important to examine these plots to see if there’s an intuitive or medically-explainable link
between the predictor and the response.
For example, let’s say you’re looking at the age of patients as a feature. If you observe that in-hospital
mortality increases sharply for very young and very old patients, but decreases for middle-aged patients,
this might be surprising (rank 4-5) given that pneumonia is generally more dangerous for older people
due to weakened immune systems and potential comorbidities.
However, in the case of variables like the white blood cell count, you might expect a U-shaped relationship
with mortality, with both very low and very high values being associated with increased risk. If you
instead observe a simple linear relationship, this might be surprising (rank 4-5), as it would contradict our
expectations based on medical knowledge.
Please provide more detailed information about the specific features and their effects for a more detailed
interpretation. Remember to also look for interactions between variables, as these can sometimes produce
surprising results.”
These responses demonstrate the dangers of data pollution when interfacing with LLMs. Despite the serious
concern for memorization and pollution from prior literature, we suspect one of the reasons why LLMs are so good at
interpreting graphs is because they have read a vast literature (larger than any one human being) that includes graphs
and descriptions of graphs, which has given them significant expertise in interpreting graphs. This skill makes LLMs
well-matched to additive models such as GAMs that learn models consisting of graphs.
Can LLMs Explain Black-Box Models?
While we suspect it is unlikely that the current generation of LLMs could be prompted in such a way as to directly
understand and explain complex black-box models such as deep neural nets, or large ensembles of boosted trees or
random forests, intermediate steps of model explanation could be used to enable LLMs to explain black-box models.
For example, SHAP [19] generates explanations that are additive and composed of graphs, and can be viewed as a
11very specific form of GAM trained via distillation on the original black-box model [20]. While SHAP explanations are
useful in data analysis, so LLM explanations of SHAP explanations will likely also be useful, it is important to keep
in mind that the explanations generated by black-box interpretation methods are approximate explanations of more
complex black-box models, and thus any LLM explanations will also be approximate. As a result, strange behaviors
could emerge, including instability or adversarial attacks [21]. In contrast, glass-box models such as EBMs provide
exactly additive learned functions, so there is no approximation needed to interpret the model.
Dealing with Finite Context Window Length
The textual description length of the entire GAM on the pneumonia dataset, after simplifying the graphs by removing
small artifacts and rounding numbers to significant digits, is 43,592 GPT-4 tokens. Even this relatively simple model
contains too many tokens to be directly encoded in GPT-4’s 32k context window. Black-box models might be much
more complex. On the other hand, univariate feature graphs are much more compact: the maximum description length
of a single graph in the pneumonia GAM is 2,345 GPT-4 tokens. This easily fits within GPT-4’s context window.
The challenge of breaking models into compact context windows is analogous to the challenge of human inter-
pretability of machine learning models: humans are not good at understanding multiple simultaneous effects. However,
when effects can be represented as separable and hierarchical components, even complicated effects can be reliably
interpreted by humans. Because additive models decompose learned functions into an additive sum of terms, both
humans and LLMs can effectively reason about GAMs one term at a time.
The Importance of Separable Components: Graph Scale and Vertical Offset affect LLM
Interpretation
LLMs are sensitive to small variations in how the graphs are presented, including variations that humans would ignore.
Sometimes this sensitivity is an advantage (e.g. the LLMs can identify jumps in the graph that might be ignored by
humans), but other times this sensitivity causes fragility in explanations. For example, in EBMs, by default each graph
is centered vertically so that the mean predicted score from that graph is zero on the train set. This vertical offset is
arbitrary: because GAM terms are additive, we can shift one graph up (or down) as long as we shift other graphs down
(or up) the same amount so that the additive offsets compensate for each other.
Typically, EBMs set the mean of each component to zero and collect the offsets as an intercept term that reflects the
baseline prediction of the model for the entire population. However, different choices of centering can be made (e.g.
some experts in some domains prefer that the minimum of each graph starts at zero so that the contribution of each
graph is always positive or zero). Quick experiments suggest that LLMs yield somewhat different results when asked
to explain graphs that are always positive vs. graphs that are both positive and negative — when explaining graphs that
have both positive and negative regions, the LLM often focuses on describing regions as adding or subtracting from
risk (i.e., positive or negative values), at the expense of focusing on the relative shape of the graph which is unchanged
by shifting the graph up or down.3
GPT-4 vs. GPT-3.5
For this project, we experimented both with GPT-4 and GPT-3.5. We found that GPT-4 is able to work with graphs at
a remarkable level of accuracy, whereas GPT-3.5 often struggles to perform the basic tasks of our setup. Consider, for
example, this response of GPT-3.5 when we ask the model about the average risk of a 82-year old individual, which
can be compared to the reported results.
GPT-3.5: "Based on the graph, the average risk of a patient aged 82 years old dying from pneumonia
would be around 0.264 (or -0.264 in log-odds). This value is obtained by taking the mean value for the
age interval that includes 82 years old, which is between 80.5 and 81.5 years old, and is equal to 0.238."
Of course, there is a potential that results with GPT-3.5 and other LLMs might be improved given sufficient prompt
engineering or fine-tuning. We leave this exploration for future work.
3Interestingly, we have observed this same effect in human experts when they are new to interpreting EBM plots that are mean-centered!
12Materials and methods
Datasets
Pneumonia The 1989 MedisGroups Comparative Hospital Database (MCHD) pneumonia dataset [16] contains in-
formation on inpatients from 78 hospitals in 23 states in the US between July 1987 and December 1988. The MCHD
contains over 250 pieces of clinical information that include patient demographic characteristics, history and physical
examination findings, and laboratory and radiological results, from which 46 variables were selected [16] with known
or postulated association with mortality in patients with community-acquired pneumonia. We used patient data that
were collected during the first 48 hr of hospitalization.
Methods
Model Estimation We use Explainable Boosting Machines (EBMs), a form of GAM [3] trained with boosted deci-
sion trees [22, 23] using the open-source package InterpretML [24]. GAMs are ideal to build glass-box models
of patient risk because: (1) GAMs can be precisely decomposed into risk curves of single variables for interpretation,
(2) the flexible component functions allow risk curves of any shape without any implicit p