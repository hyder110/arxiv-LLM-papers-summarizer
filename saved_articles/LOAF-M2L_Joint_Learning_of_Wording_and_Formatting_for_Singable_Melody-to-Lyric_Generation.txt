LOAF-M2L: JOINT LEARNING OF WORDING AND FORMATTING FOR
SINGABLE MELODY-TO-LYRIC GENERATION
Longshen Ou Xichu Ma Ye Wang
National University of singapore
{longshen, ma_xichu, wangye}@comp.nus.edu.sg
ABSTRACT
Despite previous efforts in melody-to-lyric generation
research, there is still a significant compatibility gap be-
tween generated lyrics and melodies, negatively impact-
ing the singability of the outputs. This paper bridges
the singability gap with a novel approach to generating
singable lyrics by jointly Learning wOrding And Format-
ting during Melody-to-Lyric training (LOAF-M2L). After
general-domain pretraining, our proposed model acquires
length awareness first from a large text-only lyric cor-
pus. Then, we introduce a new objective informed by mu-
sicological research on the relationship between melody
and lyrics during melody-to-lyric training, which enables
the model to learn the fine-grained format requirements
of the melody. Our model achieves 3.75% and 21.44%
absolute accuracy gains in the outputs’ number-of-line
and syllable-per-line requirements compared to naive fine-
tuning, without sacrificing text fluency. Furthermore, our
model demonstrates a 63.92% and 74.18% relative im-
provement of music-lyric compatibility and overall qual-
ity in the subjective evaluation, compared to the state-of-
the-art melody-to-lyric generation model, highlighting the
significance of formatting learning.
1. INTRODUCTION
As an emerging area of interest in the intelligent music re-
search community, automatic lyric generation has garnered
increasing attention from both academic and industrial sec-
tors. In particular, the melody-to-lyric generation (M2L)
© F. Author, S. Author, and T. Author. Licensed under a
Creative Commons Attribution 4.0 International License (CC BY 4.0).
Attribution: F. Author, S. Author, and T. Author, “LOAF-M2L: Joint
Learning of Wording and Formatting for Singable Melody-to-Lyric Gen-
eration”, in Proc. of the 24th Int. Society for Music Information Retrieval
Conf., Milan, Italy, 2023.task aims to produce lyrics that harmonize with the song,
rendering the generated outcome performable and thus fa-
cilitating the music creation process. The ability to swiftly
compose lyrics with the desired content that can be sung in
conjunction with the provided music is advantageous for
music and video creators [1].
However, obtaining singable lyrics from M2L systems
is challenging. Without carefully handling the relationship
between the melody and lyrics, lyrics that look good on
paper will likely be awkward when sung. Fig. 1 shows the
original and generated lyrics for a phrase in the song Free
as a Bird . We observe a significant quality gap between
the lyrics generated by SongMASS [2], AI-Lyricist [3],
and the original lyrics. For the SongMASS result, not only
are the sentences grammatically incorrect, leading to poor
text fluency, but the alignment between lyric syllables and
music notes is also unsatisfactory. The performer has to
extend the duration of the words in the red boxes to align
the lyrics with the music notes. Consequently, the rhythm
pattern will be changed, deviating from the music com-
poser’s intention. AI-Lyricist performs better in syllable
alignment; however, the least important words “oh” and
“it” in the orange box are highlighted with the aligned long
notes, making the performance somewhat unnatural.
In contrast, the singable lyric in the fourth row signif-
icantly outperforms the previous two results in the above-
mentioned aspects. It is not only more fluent than the
SongMASS output due to its grammatical correctness and
inter-sentence coherence, but it also excels in the compat-
ibility between lyrics and melody. It has the same number
of syllables as the number of musical notes, and their align-
ment results in a natural coupling. For instance, the second
syllable of the word “alright” is supposed to be stressed in
speech. Here it is aligned with the note with the longest
duration in its lines for emphasis, which creates a perfect
coupling and contributes to singability. These properties
make it a substantially more performable version of lyrics.
alseemtookreatomellynightWe'resorightgoThatItI'mtouchohrryyoursosotomeanallnagonalbedanceohmuchHeAndItfeelrightBaitmekeepwaysraldsbegonnaweloseplesomytherrycanbyyoudidlyreawepeollymadeshouldIt'sWhereLoneI'malSongMASS:AI-Lyricist:Singable:Original:
Figure 1 . Lyric generation quality comparison.arXiv:2307.02146v1  [cs.CL]  5 Jul 2023We design a novel strategy to generate singable lyrics
from a given melody to address these quality gaps and ob-
tain singable lyric outputs. The contributions of this paper
are as follows:
• We find that properly controlling the length of lyrics
(number of lines and number of syllables per line) is
a crucial factor in generating quality lyrics. A well-
trained length-aware model significantly alleviates
the issue caused by the misalignment between lyrics
and melodies. Although length awareness is chal-
lenging to learn given the limited paired melody-
lyric data, we introduce an additional text-only train-
ing phase that successfully enhances the model’s
length awareness ability.
• To enable the model to better capture the relation-
ship between melody and lyrics for a finer level of
compatibility, we identify essential aspects linking
music and lyrics from musicology research. Guided
by this knowledge, we design a new training objec-
tive for our M2L model to jointly learn the format re-
quirement of the given melody during training with
music-lyric paired data.
• Through both objective and subjective evaluations,
we demonstrate our model’s capability to generate
singable lyrics with high music-lyric compatibility.
Notably, we surpass previous M2L models by a large
margin regarding fluency, music-lyric compatibility,
and overall quality.
2. RELATED WORK
2.1 Relationship between melody and lyrics
Musical prosody necessitates that the number of syllables
aligns with those of the original lines in music [4]. If there
are more syllables in the lyrics, the singer must break the
note into smaller pieces to align with each syllable; con-
versely, if there are fewer syllables in the lyrics, the singer
must extend the duration of some syllables to align with
multiple notes. Either scenario disrupts the rhythm pattern
and is thus undesirable. Consequently, many studies have
focused on incorporating number-of-syllable requirements
into the lyric generation process [1, 3, 5–7].
As a stress-timed language, stress is more crucial than
syllabicity in English [8]. To achieve singability for a song,
lyric writers usually identify notes in the song highlighted
by downbeats and match them with stressed syllables in
the lyrics. Recognizing the importance of syllable stress,
some researchers have attempted to explicitly control the
position of stressed syllables in the output to achieve spe-
cific rhythmic patterns [9, 10].
The compatibility between melody and lyrics actually
involves more than just the two aspects above. Accord-
ing to the quantitative observations in [11], the linkage
between lyrics and melody encompasses the following as-
pects: (1) The level of syllable stress is strongly correlated
with the strength of metric position, melodic peaks, and
note duration. (2) Stopwords are strongly correlated withmetric position and melodic peaks. (3) V owel length tends
to be associated with note duration. These crucial observa-
tions imply a subtle yet essential coupling between melody
and lyrics.
Such relationships raise a potential issue: data spar-
sity. The relationship between notes in the melody and
words in the lyrics is not a one-to-one correspondence—
theoretically, an infinite number of melodies can be com-
patible with a given paragraph of lyrics, with the ground
truth melody in the song being just one of them. Given
the limited melody-lyric parallel data, it can be challeng-
ing for a translation-style model to capture these seemingly
loosely-coupled properties between melody and lyrics ac-
curately. This issue urges us to consider whether there is a
way to incorporate these inductive biases into the training
process as guidance.
2.2 Melody-to-Lyric Generation
Several pioneering studies have attempted to address
the melody-to-lyric (M2L) generation problem. [12] con-
structed a melody-conditioned lyric language model in
Japanese using melody-lyric aligned data; [5] tried to solve
both M2L and lyric-to-melody (L2M) generation prob-
lem with LSTM models; [13] developed a Seq-GAN-based
model with theme and melody as conditions; [3] added
length, music structure, and keyword constraints to a Seq-
GAN model; [2] employed a unified model to solve M2L
and L2M problems concurrently and show the effective-
ness of unsupervised masked pretraining; [14] introduced
a reconstruction loss in training the dual M2L and L2M
model; [15] incorporated additional music-related infor-
mation as input, such as beat and tempo.
Despite these efforts, several aspects still need improve-
ment. Firstly, the lack of singability has not been ade-
quately addressed. As will be shown in §5.2, even the
state-of-the-art models [2,3] failed to achieve the most ba-
sic compatibility requirement satisfactorily: the alignment
between melody notes and the syllables in lyrics. Besides,
none of these works tried to match syllable stress with the
melody, let alone other compatibility requirements.
Secondly, studies have yet to propose a persuasive ap-
proach to enabling the model to better capture the rela-
tionship between melody and lyrics during training, given
the sparse and limited data available. Although [14] sug-
gested forcing the model to memorize the melody-lyric
relationship by reconstruction loss, the generalizability of
this method is questionable, as even a human-written com-
patible melody for the ground truth lyrics might signifi-
cantly differ from the original melody.
2.3 Generate Lyrics with Other Inputs
Numerous works of lyric generation have different re-
search focuses. Some of these designs accept other input
constraints, such as predefined length requirements [1, 6,
16], stress patterns [10, 17], rhyme patterns [1, 6, 17–19],
keywords [20], melody emotion [21], style [18, 22], struc-
tural patterns [23], passage-level text input [24], music
accompaniment [25], and music audio [26]. Some otherresearch enables specialized outputs, such as generating
song-level output with structural tagging [27] or hiding in-
formation in the generated lyrics [28]. Notably, prompt-
based control has been proven to be effective in controlling
number-of-syllable, stress pattern, and end rhyme [1,6,10].
However, several issues remain to be solved. Firstly, the
inclusion of format constraints that contributes to music-
lyric compatibility is insufficiently comprehensive. In ad-
dition to number-of-syllable and stress patterns, music-
lyric compatibility is influenced by more elements, such
as the level of word importance and the length of vow-
els. Furthermore, the current stress pattern control requires
users to provide a predefined rhythmic pattern to indicate
which syllables should be stressed and which should not.
This design is sub-optimal: if users indicate too few or
too many stressed syllables, the generated sentence may
become unnatural because the stress pattern may differ
from the actual syllable distribution in English. Addition-
ally, the problem of recognizing stress requirements from
the melody remains unsolved, leaving users to handle this
task themselves, making the systems not friendly to users
with little background knowledge about music and rhyth-
mic patterns.
3. METHODOLOGY
We focus on improving the compatibility between lyrics
and melody to address the singability gap. We first achieve
the length control for paragraph-level generation using a
prompt-based method, followed by finer-level alignment
of syllable stress, word importance, and vowel length by
introducing a new training objective.
3.1 Length: the Basic Requirement for Compatibility
Ensuring the compatibility of lyrics and melody begins
with aligning their lengths. The term "length" here encom-
passes two aspects: the number of lyric lines in a paragraph
should be equal to the number of phrases in the melody,
and the number of syllables in each line should correspond
to the number of musical notes in each phrase.
3.1.1 Prompt-Based Control
We achieve length control for generation using a prompt-
based method. We construct special tokens as indicators of
the number of syllables in each line. For each paragraph in
the dataset, the corresponding prompt is a sequence of spe-
cial tokens. The length of the vector represents the number
of lines, and the type of prompts represents the number of
syllables in each line.
We choose a model pre-trained on general domain data
as the foundation model and fine-tune it for length aware-
ness. During training with paired data, we add these spe-
cial token vectors as additional input to guide the model to
generate outputs with the desired length.
3.1.2 Text-only Corpus
In our experiment, we found that the quantity of our paired
music-lyric data is insufficient to support the model inlearning length awareness. We attempt to leverage large-
scale text-only lyric data, as it is much easier to access than
paired data. We add another training phase, which solely
uses the text-only data for length awareness learning, be-
fore fine-tuning with the paired data. This training phase
aims to allow the model to generate lyrics based solely on
the length requirement without any other hints. This train-
ing phase has two advantages: first, it can better adapt the
output style to the lyric domain; second, it provides more
samples to help the model learn length control more effec-
tively.
3.2 Finer-Level Compatibility
3.2.1 Overall Principle
With the most basic requirement for compatibility ad-
dressed, we turn our attention to the finer aspects of the
relationship between lyrics and melody. Inspired by the
observations in [11], we establish the following two princi-
ples for our method design: (1) we aim to match important
notes with important syllables, where the importance of
notes is determined by metric position, note duration, and
melody peak, while the importance of syllables is defined
by whether they are stressed and the significance level of
the word they belong to; (2) we aim to match long notes
with long vowels. These principles guide the design of
both our method and compatibility metrics.
3.2.2 Joint Learning of Output Formatting
To achieve finer-level compatibility, we try incorporating
the inductive biases discussed in the previous section into
our model design. Since the position of the stressed sylla-
bles, important words, and vowels with different lengths
contributes to the music-lyric compatibility, we let the
model explicitly learn the lyrics’ format pattern required
by the input melody during M2L training. For each sylla-
ble in the input, we introduce three additional classification
tasks for the encoder: classifying whether the correspond-
ing syllable in the output should (1) be stressed, (2) belong
to an important word, and (3) contain a long vowel. The
resulting training objective becomes:
L=CE(y,ˆy) +CE(s,ˆs) +CE(i,ˆi) +CE(v,ˆv),(1)
where y,s,i, and vrepresent lyric, syllable stress, word
importance, and vowel length, respectively; letters with
and without hats represent predictions and ground truth,
respectively; and CE refers to cross-entropy loss. With
this strategy, we hypothesize that by explicitly learning the
format arrangement, the output embedding of the model’s
encoder will contain format information, and this will as-
sist the decoder in generating text in the desired format,
thereby allowing the model to capture the relationship be-
tween melody and lyrics more easily.
3.3 System Overview
The overall structure of our model is shown in Fig. 2. It is a
Transformer encoder-decoder model. The input consists ofEncoderDecoder
1, 2, 0, ...
2, 1, 0, ...
2, 1, 1, ...47.52s, 49.02s, 49.52s, ...
1.50s, 0.50s, 0.25s, ...
C3, G3, A2, ...
5.00s, 0.00s, 0.00s, ...len_2, len_10, len_7, ...
zI don't wanna be
your hero ...
Linearstr
Linearimp
LinearvowEmbon
Embdur
Embpitch
EmbrestLength prompt
Onset times
Durations
Pitches
Rests beforeSyllable stress
classification
Word importance
classification
Vowel length
classificationGenerated lyricsEmblenFigure 2 . Illustration of our proposed model structure. The ⊕refers to addition; the ⊗refers to concatenation.
Train Validation Test Total
Text-only#paragraphs 519,616 1,993 1,996 523,605
#lines 7,046,894 26,684 27,109 7,100,687
Paired#paragraphs 47,222 1,989 1,989 51,200
#lines 279,682 11,667 11,430 302,779
Table 1 . Dataset size of different splits.
two sources: length prompts, indicating the number of syl-
lables in each line of the paragraph, and note information,
including onset, duration, pitch, and rest before the note,
of all the notes inside the melody. There is an embed-
ding layer to handle each note element, respectively, and
the note element embeddings are added together to form
the note embedding vector. Afterward, length embedding
is concatenated with the note embedding to serve as the
input to the encoder.
Each of the additional classification tasks for the
encoder’s output embedding is a 3-class classifica-
tion. For syllable stress classification, the labels are
primary stress, secondary stress, and unstressed. The
labels for word importance classification are important
non-stopwords (non-stopwords with top 50% TF-IDF
weights), secondary-important non-stopwords, and stop-
words. For vowel length classification, the labels are short,
long, and diphthong.
Taken together, these innovations form our final
melody-to-lyric generation model, which generates the
singable lyrics in the 4th line in Fig. 1. More case stud-
ies are featured in §5.3.
4. EXPERIMENTS
4.1 Dataset
Many previous studies have adopted the dataset from [29],
initially designed for the L2M task. After some initial
exploration of this dataset, we found that the quality of
the lyrics-side data is questionable. It is common to find
misaligned lyrics and melodies or missing words in lyrics,
and hence, it is not a good choice for optimal output lyric
quality and alignment, and the author discourage future re-
searcher using this dataset.
We chose to adopt DALI v2 [30] as the paired dataset,
as it offers higher-quality lyric text and alignment. For the
text-only lyric corpus, we used a lyric dataset for genreclassification from Kaggle1, which is much larger than the
paired melody-lyric data. We performed text normaliza-
tion on the data, including removing non-English pieces,
converting all letters to lowercase, and removing special
symbols and blank lines. After splitting, the dataset statis-
tics are presented in Table 1.
4.2 Model Configuration
We chose BART-base2[31] as the foundation model. Dur-
ing our experiments, we set the batch size to the largest
possible value that could fit into an NVIDIA A5000 GPU
(24G), which was 48 for both text-only and paired data
training. We conducted a grid search for learning rate se-
lection, resulting in 2e-4 and 1e-4 for text-only and paired
data training, respectively. The learning rate was sched-
uled with linear decay in text-only training. AdamW [32]
is used as the optimizer. The warm-up steps were set to
2500 and 200 for text-only and paired data training, re-
spectively. We trained the model on text-only and paired
corpora for 15 and 10 epochs, respectively, and selected
the best checkpoint based on validation loss. Dropout and
label smoothing were not used during training.
4.3 Evaluation
4.4 Objective Metrics
To measure text quality for generated lyrics, we compare
the perplexity of each trained model. Since we do not con-
trol the content of generated lyrics, it is natural to see a
significant difference between the outputs and the original
lyrics; hence, BLEU is not used as a metric in our problem
setting.
A series of metrics are adopted to measure music-lyric
compatibility. To measure the syllable alignment between
the lyrics and the melody, we compute the accuracy of the
number of lines per paragraph and the number of syllables
per line between the outputs and the melody. For a more
fine-grained compatibility measure, we use the estimation
of the conditional probability as the metric:
Pr(B|A) =1
|C|X
p∈CP
l∈pP
n,s∈lCount (n∈A, s∈B)P
l∈pP
n,s∈lCount (n∈A),
(2)
1https://www.kaggle.com/datasets/mateibejan/multilingual-lyrics-
for-genre-classification
2https://huggingface.co/facebook/bart-baseNo. Model PPL #Line Line len Dur-str Peak-str Dur-imp Peak-imp Dur-vow
- Original lyrics - 100.00 100.00 82.45 66.45 72.29 52.73 59.99
1 Baseline 10.84 95.40 75.67 64.61 47.96 45.94 30.30 50.10
2 + Pretrain 10.98 14.28 13.15 10.44 8.88 8.26 5.08 7.20
3 Ours 7.93 99.15 97.11 77.78 61.36 54.33 40.04 55.96
4 - Multitask 7.96 99.65 97.42 75.15 59.91 54.44 41.49 53.75
5 - Note info 7.99 99.35 97.15 75.25 58.99 54.78 41.18 54.06
6 - Pretrain 11.62 95.10 70.67 57.86 42.33 38.97 26.74 43.28
Table 2 . The main result of objective evaluation. Baseline : BART pretarining + finetuning on paired data. Pretrain :
in-domain pretraining with text-only lyric corpus. The best results are bolded .
where landprefers to line and paragraph, and n, srep-
resent a pair of aligned note and syllable. We apply this
to the following five music-lyric property pairs as the val-
ues of ( A, B ): (1) long note (notes with duration longer
than the median in the paragraph) and stressed syllable, (2)
melody peak (notes with a higher pitch than both adjacent
notes) and stressed syllable, (3) long note and important
words (non-stop words), (4) melody peak and important
words, and (5) long note and long vowels. If the generated
sentence does not perfectly satisfy the length requirement,
we will let Count (A, B) = 0 ,∀n, s∈leven if AandB
occur together. It is meaningless to consider fine-grained
compatibility if the model cannot even achieve the basic
compatibility requirement.
4.5 Subjective Evaluation
The objective metrics might not comprehensively reflect
the quality of the lyrics. For example, surprising words
can sometimes positively impact text quality, but they may
not be generated by the language model with the lowest
perplexity. Regarding compatibility, lyrics and melody do
not strictly follow the rules mentioned in §3.2.1; instead,
they are only correlated in these aspects. The choice of
positions not adhering to this correspondence and the re-
sulting consequences cannot be adequately reflected in the
objective measures.
We randomly picked 10 paragraphs from 6 songs in the
dataset of [29] for the subjective evaluation. We recruited
10 students from a local university with lyric composition
or music performance backgrounds. We asked the partici-
pants to score the generated lyrics from three perspectives:
(1)fluency , which considers grammatical correctness and
semantic coherence; (2) music-lyric compatibility , which
assesses the degree to which the output and music match
each other and the resulting singability gain; and (3) over-
all quality , a score considering both the text quality and
compatibility with music, representing the ultimate goal of
singable lyric generation. Participants were asked to as-
sign a 5-point score to each generated paragraph. The final
score of a model was calculated by averaging the scores of
all participants and all paragraphs.5. RESULTS
5.1 Main Results
Table 2 shows the performance comparison on objective
metrics. First, we compare the perplexity, which reflects
the text fluency of these models. Our model achieves the
lowest perplexity among all ablation variants. Contrary to
the results in [2], adding alignment (length) constraints to
our model improves the perplexity performance. This is
because these constraints serve as additional hints during
training, enabling the model to learn the lyric text more
effectively.
As for the performance of length control, the baseline
model (No. 1) can reasonably ensure that the output con-
tains the same number of lines as the music (95.40%), but
there is still room for improvement in terms of number-of-
syllable per line (75.67%). Although adding length hints in
the paired-data training does not work well (No. 6), incor-
porating them during the text-only pretraining stage (No.
3, 4, 5) allows the model to achieve much better length
awareness. We also notice that length-unaware in-domain
pretraining for the baseline (No. 2) does not bring advan-
tages to the length constraints and results in worse perplex-
ity.
Finally, we observe the performance of fine-grained
compatibility measures. For the dur-str, peak-str, and dur-
vow metrics, our model (No. 3) outperforms the second-
best models (No. 4 or 5). In the dur-imp metric, our
model exhibits performance comparable to the best model
(No. 5). In the peak-imp metric, our model scores lower
than the best performance (No. 4); however, this metric is
actually not a good indicator of compatibility. Even the
ground truth original lyrics only achieve a 52.73% score
on this metric, suggesting that the relationship between
melody peaks and non-stopwords is insignificant in this
dataset. Therefore, it is acceptable for our model not to
excel in this metric. Overall, our model demonstrates the
best performance on those property pairs with a tight re-
lationship, although there is still room for improvement to
reach the compatibility level of human-written lyrics.
5.2 Human Evaluation
As shown in Table 3, our model (No. 4) demonstrates su-
periority in all aspects of the subjective evaluation, includ-
ing fluency, music-lyric compatibility, or overall quality,136


heyeahro
youthe	the	ingpain	pain	leveldiseSo	So	So	it	it	
mine/girlhere	here	ButAndis,	is,	
lovesy
taplaysgotsidebusgladis	is	someEaOfyou'reyou
Your	Your	dream	dream	oftimeclosebutone
ofday.	day.	kingworldmathetherchance,	chance,	stoptomeof
the	the	iswithyoufeelmy
lyooneotardgleandajunonKeepyou	you	faceThenGirlThetheknowbeI
therwake	wake	Thelyonoover	onItthergamele
sidecothetheaboutming
omyaaoneman
ingniteWideheadathat'suplikeI'mseeinvingOrleait	it	has	has	fromDanSiraYour	feel	feel	layYour	pabeyourIto	to	freak
inof
cinglifeGot
Ithisboutofa
pingnoyourtellyouna
makesiyou	you	llythetadon'twan
memind	mind	llytricked	tricked	
onthatkeesidebebejust	just	gun?	gun?	sonyou'reHancenceno'Cause	=	142Original:SongMASS:LOAF-M2L:AI-Lyricist:
(C)	1994	Tune	1000	Corporation		;EMI	Music	Publishing.	Int'l	Rights	Secured.Figure 3 . Lyric generation result comparison.
No. Model Fluency Compatibility Overall Quality
- Original lyrics 4.02 4.24 4.16
1 Baseline 2.96 2.23 2.37
2 AI-Lyricist 2.24 2.41 2.23
3 SongMASS 1.70 1.94 1.82
4 Ours 3.43 3.18 3.17
5 - Multitask 3.31 3.11 3.07
6 - Note info 3.42 3.07 3.05
Table 3 . Subjective evaluation results. The best results are
bolded .
even though this is an out-of-domain test for our model.
Conversely, SongMASS [2] (No. 3) performs surprisingly
poorly in all aspects, despite using the same training cor-
pus as the test data. The performance of AI-Lyricist [3]
(No. 2) is better than SongMASS but still falls short of our
baseline model.
Comparing our model with the ablation variants, we
find that removing the additional training objectives for the
encoder (No. 5) leads to a decrease in both fluency and
compatibility, resulting in a lower overall quality. Inter-
estingly, when we remove the note information from the
input and only allow the model to generate lyrics based on
the number-of-syllable and number-of-line requirements
(No. 6), the model achieves a better fluency score than the
end-to-end training (No. 5) and only a slightly lower score
in compatibility and overall quality. This result highlights
the importance of the length requirement as a crucial factor
in generating singable lyrics for a song.
5.3 Case Study
Fig. 3 displays the generated lyrics for one melody used in
the subjective evaluation.
We can notice a significant quality issue in Song-
MASS’s output: the generated results are nonsensical,
let alone exhibiting any inter-sentence coherence. AI-
Lyricist’s output is slightly better, with complete sentences,
but it still lacks coherence since each sentence is generatedindependently. In contrast, the lyrics of our LOAF-M2L
model are not only grammatically correct but also demon-
strate a high degree of inter-sentence semantic coherence.
We then examine music-lyric compatibility. Notably,
our model’s results perfectly match each note with a syl-
lable, laying a solid foundation for compatibility. On the
contrary, both SongMASS and AI-Lyricist’s results con-
tain many misalignments between syllables and musical
notes (as marked in the red boxes).
Our model also shows the capability to achieve fine-
grained compatibility. For instance, the first syllable of
the word “hero”, which should be stressed in speech, is
matched with a melody peak for emphasis, and the unim-
portant word "the" in the third sentence is matched with
a short note, etc. Conversely, SongMASS and AI-Lyricist
are not doing well in this aspect. In SongMASS’s output,
the second syllable of the word “gotta” in the penultimate
sentence is matched with a note of longer duration than the
note for the first syllable, making the pronunciation unnat-
ural when sung. Similarly, for AI-Lyricist, the unimportant
word “of” in the third sentence is matched with a quarter
note, resulting in awkward singing.
6. CONCLUSION
This paper addresses the problem of inadequate compati-
bility between generated lyrics and melodies. The music-
lyric compatibility was enhanced through prompt-based
length control and a new objective design based on a
quantitative analysis of the melody-lyrics relationship to
achieve better compatibility. Our study showed that these
two strategies improve compatibility and singability in the
lyric generation, based on both objective and subjective
evaluations. This work contributes to the growing field
of computational creativity and has potential applications
in various music-related tasks, such as songwriting assis-
tance, music education, and personalized songwriting.7. 