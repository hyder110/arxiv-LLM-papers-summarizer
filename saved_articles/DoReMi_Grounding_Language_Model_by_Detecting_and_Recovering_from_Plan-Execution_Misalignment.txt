DoReMi: Grounding Language Model by Detecting
and Recovering from Plan-Execution Misalignment
Yanjiang Guo1âˆ—, Yen-Jen Wang1âˆ—, Lihan Zha2âˆ—, Zheyuan Jiang1, Jianyu Chen13
1Institute of Interdisciplinary Information Sciences, Tsinghua University
2Weiyang College, Tsinghua University,3Shanghai Qi Zhi Institute
Abstract
Large language models encode a vast amount of semantic knowledge and pos-
sess remarkable understanding and reasoning capabilities. Previous research has
explored how to ground language models in robotic tasks to ensure that the se-
quences generated by the language model are both logically correct and practically
executable. However, low-level execution may deviate from the high-level plan
due to environmental perturbations or imperfect controller design. In this paper,
we propose DoReMi , a novel language model grounding framework that enables
immediate Detecti on and Recovery from Misalignments between plan and exe-
cution. Specifically, during low-level skill execution, we use a vision question
answering (VQA) model to regularly detect plan-execution misalignments. If
certain misalignment occurs, our method will call the language model to re-plan
in order to recover from misalignments. Experiments on various complex tasks
including robot arms and humanoid robots demonstrate that our method can lead
to higher task success rates and shorter task completion times. Videos of DoReMi
are available at https://sites.google.com/view/doremi-paper .
1 Introduction
Pretrained large language models (LLMs) encode vast amounts of semantic knowledge and exhibit
remarkable reasoning ability and understanding of the world. Previous works have incorporated
language models into robotic tasks to help embodied agents better understand and interact with
the world to complete challenging long-horizon tasks that require complex planning and reasoning
[1, 2, 3, 4].
To make the generated plan executable by embodied agents, we need to ground the language. One
line of the works leverages pretrained language models in an end-to-end manner that directly maps
language and image inputs into the robotâ€™s low-level action space [ 5,6,7,8,9]. These approaches
often require large amounts of robot action data for successful end-to-end training, which is expensive
to acquire [ 5,6]. Moreover, these action-output models often contain large transformer-based
architectures and cannot run at high frequencies. Therefore, they may not be suitable for tasks with
complex dynamics (e.g., legged robots) which require high-frequency rapid response. Recently,
many works adopt a hierarchical approach where high-level task planning is performed by language
models, and then some low-level controllers are adopted to generate the complex robot control
commands [1, 2, 4, 10]. Under this hierarchical framework, we can leverage powerful robot control
methods, such as reinforcement learning, to handle complex robot dynamic control problems with
high frequency.
However, these grounding methods often assume that every low-level skill can perfectly execute
the high-level plan generated by the language model. In practice, low-level execution may deviate
âˆ—Equal contribution, listed alphabetically
Preprint. Under review.arXiv:2307.00329v1  [cs.RO]  1 Jul 2023from the high-level plan due to environmental perturbations or imperfect controller design. These
misalignments between plan and execution may occur at any time during the task procedure. Previous
works consider incorporating execution feedback into language prompts once the previous plan step
is finished. If the step is unsuccessful, the process is repeated [ 10]. However, this delayed feedback
can be inefficient. For instance, as illustrated in Figure 1(b), when a human is carrying a box and
performing the low-level skill "Go to the gray table", if the box is accidentally dropped, it becomes
futile to continue with the current skill. The human will immediately abort the current skill and call
for the skill "Pick up the box". However, agents without immediate re-planning will continue going
forward, and will take more time to pick up the box dropped halfway after reaching the destination.
LLM ?????
LLMAbort and replan 1. Pick up box 2. Go to the gray table
3. Pick up box on the floor 4. Go to the gray tableLLM
LLMLLM
1. Pick up box 2. Go to the gray tableLLM
Timeline
?????I need to 
find the box.
? Constraint Detector?LLM
Can you move the 
yellow box from 
white table to the 
gray table?
I would:
1.Pick the yellow box
2.Go to the gray table
3.Place the yellow box on 
the gray table
4.DoneBasic skill set
Description
Prompts
â€¦
(a) High -level Task Planning (b) Low -level Skill ExecutionBox dropWithout Immediate Re -planning
Immediate Detection and RecoveryLLM
Figure 1: Illustration of our motivation. Low-level execution may deviate from the high-level plan.
DoReMi can immediately detect the misalignment between the plan and execution when the box
drops accidentally and quickly recovers. Agents without immediate re-planning suffer from such
misalignment.
In this paper, we propose a novel framework DoReMi which enables immediate Detecti on and
Recovery from plan-execution Misalignments. Once a misalignment between plan and execution
occurs, our method immediately calls the language model to re-plan, enabling timely recovery.
Specifically, we maintain and update a set of constraints that indicate the alignment between the
plan and execution. These constraints are provided through natural language description. During the
execution of low-level skills, a visual question answering (VQA) model [ 11,12] is employed as a
"constraint detector" to regularly check whether the agent violates any constraint in the constraint
set. If some constraints are found to be violated, which indicates that the plan and execution may
be misaligned, the language model is called to re-plan the high-level task. Furthermore, under mild
assumptions, we conduct a theoretical analysis to estimate how much time can be saved or how
much the success rate can be improved through immediate re-planning. Experiments in physical
simulations, including robot arm manipulation tasks and humanoid tasks, demonstrate that DoReMi
leads to a higher success rate and shorter task execution time.
Our contributions can be summarized as follows:
â€¢We show the importance of aligning high-level plan and low-level execution and propose the
DoReMi framework, which enables immediate detection and recovery from plan-execution
misalignments for complex long-horizon robotic problems.
â€¢We explicitly maintain a constraint set through natural language description and adopt the
VQA model as a general constraint detector to timely detect plan-execution misalignment
and re-plan.
â€¢Theoretical analyses and experiments on various complex robotic tasks verify the effective-
ness of DoReMi, which results in less task execution time and higher success rates.
2 Related Works
Language Grounding Prior research has attempted to employ language as task abstractions and
acquire control policies that are conditioned on language [ 13,14,15,16,17]. Furthermore, some
2studies have investigated the integration of language and vision inputs within embodied tasks to
directly predict the control commands [ 18,19,20]. Recent works, including [ 5,19,8,21,18,22],
have demonstrated significant progress in utilizing transformer-based policies to predict actions.
However, these end-to-end approaches heavily depend on the scale of expert demonstrations for
model training and often encounter challenges when generalizing to unseen scenarios.
Another line of work adopts a hierarchical framework and demonstrates that language models
can perform high-level task planning in a zero/few-shot manner [ 2,1] with appropriate grounding.
Specifically, SayCan [ 1] incorporates an autoregressive language model and affordance functions
to determine the most reasonable and executable skill from pretrained skill sets. These skill sets
are trained using reinforcement learning or imitation learning [ 23,24]. Huang et al. [ 2] leverages
semantic translation to identify the skill closest to the desired output. Additionally, vision information
can be integrated into prompts to assist the natural language model in the planning process [ 25,26].
The above methods typically assume successful execution of each step, resulting in an open-loop
system. In contrast, Inner-Monologue [ 10] takes into account various environmental feedback (e.g.
success detectors and scene descriptions [ 11,27]) upon the completion of each step. Our method
emphasizes the importance of immediate detection and recovery from plan-execution misalignment,
which can be highly beneficial in certain tasks.
Vision Language Model for Embodied Control. The visual language model (VLM) is trained on
image-text pairs, enabling it to simultaneously understand visual and textual inputs and address a
variety of downstream tasks, such as visual question answering (VQA) [ 12,11], image description
[28], and object detection [ 25]. VLM models align semantic information between visual and natural
language, thereby aiding in grounding language models and facilitating embodied control. Pretrained
models such as CLIP [ 29] have been integrated into diverse embodied tasks [ 30]. CLIPort [ 23],
which incorporates Transporter [ 24] with CLIP, effectively combines spatial precision and semantic
understanding. The PaLM-E model [ 31], equipped with pretrained vision transformer [ 32] and PaLM
model [ 33], can leverage multimodal inputs and generate textual plans directly. Inner-Monologue
[10] assumes perfect VLMs as success detectors and scene descriptors to obtain task feedback. To
ensure adherence to crucial constraints, we employ the VQA model [ 12] as a "constraint detector",
periodically verifying whether the agent satisfies specific constraints.
3 Problem Statement
Open -loop
planning
Close -loop 
planning
Immediate 
re-planningð…ðŸ,ð’ð…ðŸ,ð’•ðŸð…ðŸ,ð’ð…ðŸ,ð’•ðŸð…ðŸ‘,ð’ð…ðŸ‘,ð’•ðŸ‘ð…ð’,ð’ð…ð’,ð’•ð’
ðŸŽ ð’•ðŸð’•ðŸ+ð’•ðŸð’•ðŸ+ð’•ðŸ+ð’•ðŸ‘PlanAbstract language instruction ð’ŠTask Planning
Figure 2: Comparison of the planning time.Our objective is to enable the embodied agent to
accomplish long-horizon tasks specified as natu-
ral language instruction i. The agent has a basic
skill set Î , with each skill Ï€jâˆˆÎ correspond-
ing to a distinct function that can be described in
natural language lÏ€jPretrained large language
models, which have the ability to understand
and reason, are used as planners [ 1,2] to de-
compose complicated language instructions into
basic skill sequences: iâ†’(Ï€1, Ï€2, ..., Ï€ n)as
shown in Figure 2.
We denote low-level skill execution time as (t1, t2, ..., t n). Planning only at time t= 0 will lead
to completely open-loop planning without the chance of re-planning, making it susceptible to
environmental disturbances or intermediate failures. Previous works often consider language model
planning at the time when the previous skill is finished and the next one is required [ 1,10], which
indicates close-loop planning at time tâˆˆ {0, t1, t1+t2, ...,Î£n
k=1tk}.
However, planning only at these switching times can be inefficient, and immediate re-planning
and recovery during the skill execution process can be critical in certain situations (e.g., box
drop example in Figure .1). Therefore, instead of planning only at the switching time points
tâˆˆ {0, t1, t1+t2, ...,Î£n
k=1tk}, our goal is to develop a framework that enables immediate detection
and re-planning throughout the entire time period tâˆˆ[0,Î£n
k=1tk]. This allows for timely re-planning
in case of plan-execution misalignment, such as the box being dropped in the above scenario, leading
to quicker recovery. A comparison of the planning time can be found in Figure. 2. In the following
section, we will introduce our DoReMi framework in detail, which facilitates immediate detection and
recovery from misalignment throughout the entire low-level execution time period tâˆˆ[0,Î£n
k=1tk].
34 Method
In this section, we introduce our DoReMi framework which enables immediate Detecti on and
Recovery from Plan-Execution Misalignment. Our algorithm can be succinctly described in three
stages depicted in Figure 3:
1.Given a set of low-level skills, scene description, and high-level task instruction, language
models are utilized to generate executable high-level plans.
2.We explicitly maintain a constraint set through natural language which indicates plan-
execution misalignment.
3.During low-level skill execution, we employ the VQA model [ 12] as a general "constraint
detector" that periodically verifies the satisfaction of all constraints. If any constraints are
violated, the language model is called for immediate re-planning to help recovery.
LLM Basic Skill Set ðš·
Scene description
Instructions ð’Š
Prompts ð’‘
â€¦â€¦Constraint 
SetAdd constraint ð‘ªð’‹
?yes
No?yes
No?yes
No?yes
No?yes
Noâ€¦â€¦Pop constraint ð‘ªð’Œ
Feedback
Re-planning?Constraint Detector ð‘«ðœŸð’•ðœŸð’• ðœŸð’•1. Planning/re -planning       2. Constraint update        3. Immediate detection and feedback
â€¦â€¦
Figure 3: Our DoReMi framework mainly contains 3 components: (1) Task planning/re-planning
with grounded language models; (2) Explicit maintenance of a constraint set that updates according
to the planned step; (3) Utilization of VQA as general constraint detectors to identify misalignment
between the plan and execution."
4.1 Language Model for Task Planning
Previous work has demonstrated that language models can plan executable skill sequences through
appropriate grounding in zero/few-shot manners [ 26,2], as shown in Figure 1(a). Similar to previous
works [ 1,10], we add instruction i, scene description, and skill language description to prompts to
assist language models in high-level task planning. We try large language model GPT-4 [ 34] and
Vicuna-13B [ 35]. Besides forward planning, we also use language models for re-planning if our
constraint detector identifies a plan-execution misalignment. Under this scenario, we additionally
include the misalignment information in prompts and then call the language model for re-plan.
Through this mechanism, our agent can quickly recover from stages where the high-level plan and
low-level execution are misaligned.
4.2 Constraint Set
During low-level skill execution, there are some constraints that the agent must satisfy. Violation of
these constraints can indicate misalignments between the high-level plan and low-level execution.
For instance, after performing the skill "pick up box", the agent must satisfy the constraint "hold
box". If this constraint is violated, it may indicate an unsuccessful execution of the "pick up" skill
or a dropped box. These constraints cover a variety of types. To check the constraints in a general
manner without the need for specific designs or hard coding, we utilize a visual question answering
(VQA) model [11, 12] as a general "constraint detector".
However, solely relying on open-ended questions such as "Is there any exception happening?"
without specifying the form of the constraint can introduce ambiguity, making it challenging for
the VQA model to determine plan-execution alignment. To tackle this, we propose using natural
language to explicitly specify constraints for each skill. As these constraints are described in natural
language, they can be easily understood and modified by humans based on requirements. For instance,
some common robot skills can be categorized as shown in Table 1 [ 36], along with their respective
constraint update processes. Navigation skills can be represented in goto{recep} . Manipulation
motion primitives parameterized in two poses [ 37,38] can be represented in pick andplace . We
have also explored using language models to automatically generate constraints for skills. However,
4Low-level Skill Constraint Update VQA Question
goto {recep} +no obstacle in the front +Is there any obstacle in the front?
pick {obj}from {recep} +{agent} holding {obj} +Is the {agent} holding {obj} ?
place {obj}on{recep}+{obj} on{recep}
-{agent} holding {obj}+Is the {obj} on{recep} ?
-Is the {agent} holding {recep} ?
open {obj}+{obj} open
-{obj} close+Is the {obj} open?
-Is the {obj} close?
close {obj}-{obj} open
+{obj} close-Is the {obj} open?
+Is the {obj} close?
Table 1: Constraint set update rule based on the natural language description. {obj} and{recep}
correspond to objects and receptacles respectively. The symbol " +" indicates the addition of the
constraint into the set while " -" means popping out this constraint. Questions are in the general
structure: "Is the {constraint}?"
Algorithm 1 DoReMi (Immediate Detecti on and Recovery from Misalignment)
Given: A high level instruction i, a skill set Î , language description lÎ forÎ , language
model Las task planner with initial prompt p0, and VQA model as constraint detector
D.
1:Initialize the constraint set Câ† âˆ…, the skill sequence Ï€â† âˆ…, the number of steps nâ†1.
2:while lÏ€nâˆ’1Ì¸=done do
3:Ï€nâ†arg max Ï€âˆˆÎ L(lÏ€|i, pnâˆ’1, lÏ€nâˆ’1, ..lÏ€0)
4:pnâ†pnâˆ’1
5: Update Caccording to Ï€n.
6: while Ï€nis not finished do
7: Every âˆ†tsecond, query agent all the constraints cjâˆˆCusing the constraint detector D.
8: ifâˆƒD(cj) =false then
9: Add constraint violate information into prompt pnandbreak .
10: end if
11: end while
12: Update the prompt pnwith the status of Ï€n, andnâ†n+ 1.
13:end while
their outputs result in large ambiguity. Therefore, in this paper, we adopt precise natural language
descriptions as constraints, as shown in Table 1.
4.3 Constraint Detector and Re-planning
After the constraint set updating stage, the agent proceeds to execute the low-level skill selected by
the language model. Our algorithm employs a visual question answering (VQA) model, denoted as D,
which acts as a "constraint detector". It periodically checks whether the agent satisfies all constraints
in the set. The visual input is captured from first-person or third-person perspective cameras. For each
constraint cj, the question input is formulated in a general structure "Is the constraint cjsatisfied ?"
The exact phrasing of the questions may vary based on grammatical considerations. For instance, for
the constraint " agent holding box ", the question would be "Is the agent holding the box ?" Similarly,
for the constraint " green block on red block ", the question becomes "Is the green block on red block ?".
We use D(vt, cj)to denote the answer of the VQA model Dwhen checking cjwith vision input vt
at time t. If the constraint cjis satisfied at time t,D(vt, cj) =True ; otherwise, D(vt, cj) =False .
Every âˆ†tsecond, we use the VQA model to examine whether the constraint set is fully satisfied. If
so, the robot continues executing the current low-level skill; otherwise, the robot aborts the current
skill, and the re-planning process is triggered. In practice, the zero-shot VQA model may make errors
with a single image. Therefore, we employ an ensemble approach by considering multiple time steps
and views from different cameras.
To summarize, our method has three main components: (1) We use language models as task planners.
(2) We maintain a constraint set in natural language and use the VQA model to check constraint
satisfaction. (3) If any constraint is violated, we trigger timely re-planning with the language model.
The pseudo-code is provided in Algorithm 1.
54.4 Theoretical Analyses
We analyze the potential time savings and success rate improvements achievable through immediate
detection and recovery. We denote the execution time of low-level skill with random variable t
with mean E[t] =Âµand variance V ar(t) =Ïƒ2. Misalignment can occur at any time swithin
the execution time interval [0, t]where 0â‰¤sâ‰¤t. We define the discrete random variable M
as the number of misalignment occurrences under the following assumptions: (1) Plan-execution
misalignments occur independently. (2) Misalignments occur at a constant ratio Î»within a small time
interval: lim
tâ†’0P(M= 1) = Î»t. (3) No two misalignments occur simultaneously: lim
tâ†’0P(M=k) = 0
fork >1. Under these assumptions, the number of plan-execution misalignments follows a Poisson
distribution [39]:
P(M=k) =(Î»t)keâˆ’Î»t
k!k= 0,1,2,3... (1)
Our analysis considers two types of misalignments: (1) Soft misalignment : If this occurs at time s,
an agent without immediate re-planning must recover to the stage at time s, wasting time from stot
(e.g., a robot that drops an object must return to the drop location to pick it up). We assume DoReMi
can detect this misalignment within âˆ†tsecond and recover immediately. (2) Critical misalignment :
If this occurs at time s, a delayed re-planning invariably results in failure; only immediate re-planning
can address this misalignment (e.g., unexpected obstacles appear in front of the robot).
Theorem 1 The following equations describe the wasted time twunder soft misalignment and the
failure probability Pfunder critical misalignment without immediate detection and re-planning:
E(tw) =X
kP(M=k)E(tw|M=k) =Î»(Âµ2+Ïƒ2)
2âˆ’Î»Âµâˆ†t (2)
E(Pf) = 1âˆ’E(eâˆ’Î»t)â‰ˆÎ»Âµâˆ’Î»2(Âµ2+Ïƒ2)
2(3)
The detectorâ€™s reaction time, âˆ†t, is much smaller than the average execution time Âµ, soE(tw)is
greater than 0.Î»represents the misalignment occurrence ratio per second, which is very small, so
E(Pf)is also greater than 0. Under perfect detection and re-planning conditions, DoReMi can reduce
the failure probability to 0, which is significantly less than E(Pf). Detailed proof can be found in
Appendix A.
5 Experiments
In this section, we conduct experiments involving both robotic arm manipulation tasks and humanoid
robot tasks, as shown in Figure 4. These tasks incorporate various environmental disturbances and
imperfect controllers, such as random dropping by the robot arms, noise in end-effector placement
positions causing stacked blocks to collapse, and unexpected obstacles appearing in the robotâ€™s path.
We aim to answer the following questions: (1) Does the DoReMi framework enable immediate detec-
tion and recovery from plan-execution misalignment? (2) Does DoReMi lead to higher task success
rates and shorter task execution time under environmental disturbances or imperfect controllers?
Pick and Place Stack blocks in order Move box from A to B Go through obstacle forest
Figure 4: Robot manipulation and humanoid robot tasks in our experiments. We consider various
types of environmental disturbance and imperfect controllers.
65.1 Robot Arm Manipulation Tasks
Robot and Environment This environment is adapted from Ravens [38], a benchmark for vision-
based robotic manipulation focused on pick-and-place tasks. A UR5e robot with a suction gripper
works on a 0.5Ã—1m black tabletop. Two cameras are pointing toward the workspace: one in front
(facing the robot) and one in the left (facing the task area), providing 2simulated 640Ã—480RGB-D
images at every time step. We inject additional disturbances to the original environment and the robot
controller to validate our algorithmâ€™s effectiveness.
Task 1: Pick and Place. The agent is required to pick up an 8Ã—8Ã—6cm cuboid block and place it in
a fixture. The agent determines the pick position and place position (apick, aplace)based on the birdâ€™s-
eye view image, and then the pick-place primitive generates the corresponding trajectory. However,
we assume the block has a probability pto drop every second when sucked by the end-effector, so the
agent may need to try several times to finish the task.
Task 2: Stack blocks in order. The robot is required to stack the 4Ã—4Ã—4cm blocks in an order
given by language instructions. The agent decomposes the instruction into multiple "pick and place"
sequences and executes them sequentially. We assume the controllers are not perfect by introducing
uniform [0, n]cm noise to the place positions. There is also a probability pthat a block held by the
end-effector might randomly drop every second.
5.1.1 Main Result
We maintain a constraint set to indicate the alignment between the plan and execution as described
in 4.2 and use BLIP (VQA model) [ 12] to check constraint satisfaction using camera images. Our
low-level skills are trained pick-and-place primitives conditioned on single-step instructions similar
to the CLIPort [ 23] and Transporter Nets [ 38]. We compared DoReMi with 3 baselines: (1) SayCan :
large language model (LLM) decomposes instructions into steps and executes them sequentially.
However, this approach assumes the successful execution of each step without considering potential
failures. (2) CLIPort : a multi-task CLIPort policy conditioned on the single pick-place step. It
utilizes LLM to decompose instructions into steps, and our VQA is used as a success detector to
check whether the current step should be repeated until success. However, CLIPort cannot perform
re-planning during execution. (3) Inner-Monologue : similar to CLIPort, but can re-plan at the end
of each step. Results are shown in Table 2.
5.1.2 Analyses
In the single-step pick-and-place task, all methods perform well when the controllers are perfect.
However, with the introduction of random drop disturbances, SayCan may fail due to its lack of
success detectors and re-planning capabilities. CLIPort and Inner-Monologue, while able to repeat
steps until success, require more time as they cannot detect the drop immediately and only retry after
completing the previous trajectory. In contrast, DoReMi can immediately detect the drop and re-plan
the trajectory, resulting in lower execution times.
Stack-in-orders are multi-step tasks that require planning. SayCan consistently fails under various
disturbances, including block-drop or block-collapse, due to its inability to manage plan-execution
misalignments. CLIPort, armed with success detectors, can repeat the current step until success,
making it more resilient to disturbances like block drops. However, in situations where re-planning is
necessary, naively repeating the current step would lead to failure. For example, as depicted in Figure
5, when the robot attempts to stack the third blue block, all blocks collapse. In such cases, repeating
the current step would result in failure due to the incorrect stack order. Although Inner-Monologue
can re-plan at the end of each sub-task, our experiments revealed that Inner-Monologue often fails to
re-plan correctly due to the imperfectness of the VQA detector in recognizing failures. There is a
possibility that certain misalignments may remain undetected at the moment when the last step is
finished. For example, failure to detect a block collapse in progress can directly result in an error
during re-planning. In contrast, DoReMi achieves significantly better performance by continuously
detecting and recovering from failures, which are more robust under imperfect VQA detectors.
Furthermore, DoReMi can alleviate the high variance of single VQA detection through ensembling
multi-step detection results (see Appendix C.1 for full details), which is enabled by our continuously
detecting and re-planning mechanism.
7Tasks with disturbanceSuccess Rate(%) â†‘ Execution Time(s) â†“
SayCan CLIPortInner
MonologueDoReMi
(ours)SayCan CLIPortInner
MonologueDoReMi
(ours)
Pick and place
with random drop pp=0.0 100 (Â±0) 100 ( Â±0) 100 ( Â±0) 100 ( Â±0) 2.7 (Â±0.0) 2.7 ( Â±0.0) 2.7 ( Â±0.0) 2.7 ( Â±0.0)
p=0.1 96 (Â±4) 100 ( Â±0) 100 ( Â±0) 100 ( Â±0) 3.2 (Â±0.4) 2.9 ( Â±0.1) 2.9 ( Â±0.1) 2.8 (Â±0.1)
p=0.2 81 (Â±9) 100 ( Â±0) 100 ( Â±0) 100 ( Â±0) 4.7 (Â±1.0) 3.4 ( Â±0.2) 3.4 ( Â±0.2) 3.0 (Â±0.2)
p=0.3 63 (Â±9) 100 ( Â±0) 100 ( Â±0) 100 ( Â±0) 6.6 (Â±1.3) 4.0 ( Â±0.2) 4.0 ( Â±0.2) 3.3 (Â±0.2)
Stack in order
with noise nn=0.0 100 (Â±0) 100 ( Â±0) 100 ( Â±0) 100 ( Â±0) 7.2 (Â±0.0) 7.2 ( Â±0.0) 7.2 ( Â±0.0) 7.2 ( Â±0.0)
n=1.0 96 (Â±4) 96 ( Â±4) 96 ( Â±4) 100 (Â±0) 8.0 (Â±3.0) 8.0 ( Â±3.0) 8.0 ( Â±3.0) 7.5 (Â±0.5)
n=2.0 63 (Â±9) 85 ( Â±7) 88 ( Â±7) 96 (Â±4) 26.1 (Â±7.7) 16.4 ( Â±5.7) 15.2 ( Â±5.3) 10.2 (Â±1.7)
n=3.0 31 (Â±11) 75 ( Â±10) 79 ( Â±9) 83 (Â±8) 42.6 (Â±7.0) 23.2 ( Â±6.0) 22.7 ( Â±5.7) 16.6 (Â±3.2)
Stack in order
with noise n
random drop p=0.1n=0.0 71 (Â±9) 94 ( Â±7) 98 (Â±4) 98 ( Â±4) 22.4 (Â±6.8) 11.0 ( Â±3.6) 9.6 ( Â±2.7) 8.4 (Â±1.7)
n=1.0 71 (Â±9) 94 (Â±7) 94 ( Â±7) 94 ( Â±7) 21.4 (Â±7.1) 10.7 ( Â±3.9) 10.2 ( Â±3.8) 9.6 (Â±3.2)
n=2.0 54 (Â±12) 79 ( Â±9) 81 ( Â±9) 92 (Â±6) 29.8 (Â±7.5) 19.5 ( Â±6.7) 18.7 ( Â±6.5) 11.5 (Â±3.4)
n=3.0 21 (Â±9) 33 ( Â±10) 42 ( Â±10) 50 (Â±10) 48.0 (Â±6.4) 42.8 ( Â±7.0) 40.2 ( Â±7.0) 26.8 (Â±4.3)
Table 2: Success rates and task execution time under different degrees of disturbances. If the task fails,
the execution time is set to timeout, which is 10s for the pick-place task and 60s for the stack-in-order
task. The results show the mean and standard deviation over 4 different seeds each with 12 episodes.
TimelinePick blue block and place on green block
Pick green block and place on red block
CLIPort with
Success Detector
(e)                           (f)                
DoReMi
(a)                        (b)                         (c)                         (d)
Figure 5: A comparison example. The robot arm tries to finish the step "Stack the blue block on the
green block" but collapses (bcd). DoReMi detects this misalignment and replans to pick and place
the green block first (e). The baseline continues to repeat the previous step (ef) and results in failure.
5.2 Humanoid Robot Tasks
Robot Description. The humanoid robot utilized in our experiments possesses 5degrees of freedom
per leg and 3degrees of freedom per arm, totaling 16degrees of freedom. We equip the robot with a
first-view camera on its base to provide visual information. Detailed robot information can be found
in Appendix B.
Task 1: Move Box from Place A to Place B. This long-horizon and challenging task involves both
navigation and manipulation, commanding the robot to transport a box from one location to another.
A proper solution might involve (1) Go to place A. (2) Pick up box (3) Go to place B. (4) Put down
box. We introduced additional perturbations to this task by assuming that the robot has a probability
pof dropping the box every second during transport.
Task 2: Go through Obstacle Forest. The robot is commanded to move forward, despite the risk of
running into unexpected obstacles in its path. The robot needs to continuously detect whether there
are obstacles in the front and change direction to avoid collision.
5.2.1 Train Low-level Skills
Controlling complex humanoid robots with a single policy is challenging. Thus, we train low-level
skills at the category level. Following the framework in [ 40], we utilize reinforcement learning to
train locomotion policy and use model-based methods to obtain manipulation policy. Specifically, we
train3categories of policy: (1) A locomotion neural network policy conditioned on commanded
linear and angular velocity, allowing the robot to execute low-level skills such as "Go forward fast",
"Go forward at speed v", "Stand still", "Turn right/left", "Go to target place A", etc. (2) A stand/squat
neural network policy conditioned on the commanded height, enabling skills like "Stand", "Squat",
"Raise height with length l", etc. (3) A hand manipulation policy based on a linear interpolation
controller, using the Deepmimic [ 41] algorithm with multiple motion capture data (Mocap data) to
obtain policies that can produce natural behaviors. Detailed architecture and training process can be
found in Appendix B.
8Tasks with disturbanceSuccess Rate (%) â†‘ Excution Time (s) â†“
SayCanInner
MonologueDoReMi
(ours)SayCanInner
MonologueDoReMi
(ours)
Move box from A to B
with random drop pp=0.0 98(Â±3) 98( Â±3) 97( Â±4) 23.8(Â±0.7) 23.8( Â±0.7) 24.4( Â±1.4)
p=0.02 61(Â±4) 88(Â±6) 92( Â±5) 53.0(Â±7.2) 44.3( Â±7.7) 36.9(Â±6.4)
p=0.04 43(Â±12) 82(Â±8) 88( Â±8) 67.4(Â±3.3) 57.2( Â±6.0) 43.2(Â±6.7)
Go through
obstacle forest
with density dd=0.0 100(Â±0) 100( Â±0) 100( Â±0) 27.3(Â±0.1) 27.3( Â±0.1) 27.3( Â±0.1)
d=0.3 68(Â±6) 68( Â±6) 92(Â±6) 52.4(Â±5.0) 52.4( Â±5.0) 38.5(Â±5.0)
d=0.6 40(Â±8) 40( Â±8) 90(Â±4) 64.9(Â±5.1) 64.9( Â±5.1) 41.5(Â±3.0)
Table 3: Success rates and task execution time under different degrees of disturbances. If tasks failed,
execution time is set to timeout which is equal to 90s. The results show the mean and standard
deviation over 5 different seeds each with 12 episodes.
TimeGo to the gray table
Pick the yellow box
Place the yellow box
Inner
MonologueDoReMi
(a)                 (b)                         (c)                  (d)                  (e)                  (f)           (g)                  (h)                  ( i)
Box Dropped
Figure 6: A comparison of DoReMi and baseline. Plan-execution misalignment occurs at the time (b).
DoReMi immediately detects box drop and replans to pick up the box at the time (b). The baseline
with delayed feedback keeps going forward and costs more time to pick the box at the time (c)-(g).
5.2.2 Main Result and Analyses
We follow the pipeline mentioned in the section 3. During low-level skill execution, the BLIP
[12] (VQA model) consistently queries the first-view images to verify constraint satisfaction. If a
constraint is violated or the current step completes, the language model is invoked for re-planning.
The full prompt used can be found in Appendix D. We compare our method with (1) SayCan [1]
which assumes every step is executed successfully, and (2) Inner-Monologue [10] which plans at the
end of each step and uses the same VQA model as a success detector and scene descriptor.
Analyses As shown in Table 3, when the task disturbance is set to 0, indicating the perfect execution
of each step, all methods exhibit high success rates as long as the language model plans correctly.
However, when plan-execution misalignments occur, such as a box dropped on the floor or unexpected
obstacles appearing in the path, both SayCan and Inner-Monologue struggle. SayCan fails to handle
these misalignments, while Inner-Monologueâ€™s delayed re-planning leads to longer task completion
time and decreased success rate. Figure 6 provides a visual comparison of the Move-box task. In this
example, if the box drops (b) while the agent is executing the "Go to the gray table" skill, the agent
without immediate detection continues to complete the current skill (b)-(d). Upon reaching the grey
table (d), the robot re-plans but encounters difficulties in retrieving the halfway-dropped box (e)-(g).
On the other hand, DoReMi immediately detects the drop and picks up the box, resulting in shorter
task completion time.
6 Discussion
Limitation Our experiments indicate that the zero-shot transfer VQA model is not a perfect con-
straint detector. We need to employ an ensembling approach to improve detection accuracy. Our
framework can benefit from a more advanced vision-language model in the future. Furthermore,
our method maintains a constraint set through specified natural language to indicate plan-execution
misalignments. There is potential for language models to automatically generate these constraints in
future implementations.
Conclusion When employing language models for task planning in a hierarchical approach, the
low-level execution might deviate from the high-level plan. We emphasised the importance of
continuously aligning the plan with execution and proposed the DoReMi framework, which grounds
language through immediate detection and recovery from plan-execution misalignments. Theoretical
analyses and a variety of challenging tasks in disturbed environments demonstrated the effectiveness
of DoReMi.
9