Summary:
The paper introduces KDSTM, a neural semi-supervised topic modeling method using knowledge distillation. KDSTM outperforms existing supervised topic modeling methods in classification accuracy, robustness, and efficiency, and achieves similar performance compared to weakly supervised text classification methods. It requires few labeled documents, no pretraining, and is designed for resource-constrained settings. The method incorporates knowledge distillation and optimal transport into the neural topic modeling framework.

Bullet Points:
1. KDSTM is a neural semi-supervised topic modeling method using knowledge distillation.
2. It outperforms existing supervised topic modeling methods in classification accuracy, robustness, and efficiency.
3. KDSTM requires few labeled documents as input, making it practical in low resource settings.
4. The method does not rely on transfer learning or pretrained language models.
5. KDSTM is efficient and suitable for training and running inference on resource-constrained devices.
6. It incorporates knowledge distillation and optimal transport into the neural topic modeling framework.
7. The method achieves improved performance across several classification metrics.
8. KDSTM is faster to train and fine-tune compared to existing methods.
9. The method does not require pretrained embeddings.
10. KDSTM is designed for application in low resource scenarios.

Keywords:
- KDSTM
- Neural semi-supervised topic modeling
- Knowledge distillation
- Optimal transport
- Classification accuracy
- Robustness
- Efficiency
- Few labeled documents
- Resource-constrained settings
- Pretrained embeddings