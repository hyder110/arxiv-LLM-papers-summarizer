Advancing Beyond Identification:
Multi-bit Watermark for Language Models
KiYoon Yoo1Wonhyuk Ahn2Nojun Kwak1*
1Seoul National University2Webtoon AI
{961230,nojunk}@snu.ac.kr whahnize@gmail.com
Abstract
This study aims to proactively tackle misuse
of large language models beyond identifica-
tion of machine-generated text. While existing
methods focus on detection, some malicious
misuses demand tracing the adversary user
for counteracting them. To address this, we
propose "Multi-bit Watermark through Color-
listing" (COLOR), embedding traceable multi-
bit information during language model genera-
tion. Leveraging the benefits of zero-bit water-
marking (Kirchenbauer et al., 2023a), COLOR
enables extraction without model access, on-
the-fly embedding, and maintains text qual-
ity, while allowing zero-bit detection all at the
same time. Preliminary experiments demon-
strates successful embedding of 32-bit mes-
sages with 91.9% accuracy in moderate-length
texts (∼500 tokens). This work advances strate-
gies to counter language model misuse effec-
tively.
1 Introduction
How can we take a step further from merely identi-
fying machine-generated text to proactively tack-
ling misuse of large language models? The emer-
gence of human-like language models has necessi-
tated the development of various methods to detect
machine-generated texts through techniques such
as zero-shot detection, supervised training, water-
marking, and more (Mitchell et al., 2023; Wang
et al., 2023; Kirchenbauer et al., 2023a; Krishna
et al., 2023). These endeavors focus on the cru-
cial task of identifying machine-generated content,
which serves as a pivotal step in mitigating the
potential harm caused by such text. An illustra-
tive instance where detection can play a significant
role is in curbing students’ academic misconduct,
particularly in cheating on assignments within edu-
cational institutions. Properly identifying machine
text can mitigate the negative impact on students’
learning experiences by creating an environment
that guarantees a fair assessment of assignments.While the potential harm is real, such misconduct
does not call for more drastic measures such as
legal actions against the adversary user.
However, when it comes to more pernicious mis-
uses of large language models, such as the dis-
semination of misinformation and propaganda on
social media platforms, the stakes are consider-
ably higher, potentially leading to the erosion of
social trust (Valenzuela et al., 2022). Notable in-
stances that exploited automated bots in the past in-
clude manipulating an election campaign (Badawy
et al., 2018), spreading disinformation about the
Russian invasion of Ukraine (Pierri et al., 2023),
and promoting products through fake reviews (An-
nie, 2023).
In these scenarios, merely identifying the
machine-generated text may not suffice; instead,
the ability to trace back to the adversary user re-
sponsible for generating the content becomes piv-
otal in counteracting such misuse. By doing so, the
API provider can take a precursory measure to ban
the user from its system. More importantly, this
allows media and social platforms, along with API
providers, to collaborate with law enforcement au-
thorities and take more decisive actions. All in all,
watermarking this information (or part thereof) can
hold the adversary user accountable for potential
harms facilitated through language model APIs.
Technically, this requires embedding multi-bit
information as opposed to zero-bit watermark-
ing (Kirchenbauer et al., 2023a; Munyer and Zhong,
2023) or simply identifying machine text (Mitchell
et al., 2023; Krishna et al., 2023). In this study, we
demonstrate preliminary evidence that this can be
achieved on top of the recently proposed zero-bit
watermarking method (Kirchenbauer et al., 2023a)
in an extremely simple way without sacrificing the
text quality. Zero-bit watermarking works by ran-
domly favoring a "greenlist" of tokens at each to-
ken generation. The selection of a greenlist from
the vocabulary set is determined by a random seedarXiv:2308.00221v1  [cs.CL]  1 Aug 2023generated by a pseudo-random function.
Our proposed method called Multi-bit water-
mark through Color-listing (COLOR) partitions
the vocabulary into multiple colored-lists instead
of a single green list, effectively encoding multiple
states for every token. Then, we allocate each to-
ken pseudo-randomly onto a bit position using a
pseudo-random function similar to the zero-bit wa-
termarking scheme. To the best of our knowledge,
our work is the first work embedding multi-bit in-
formation during language model generation.
Since our method works on top of zero-bit wa-
termarking, it leverages most of the advantages: (1)
Multi-bit message can be extracted without access
to the model parameters or the API, allowing other
parties to extract the adversary ID if given access
to the extraction algorithm. (2) Multi-bit embed-
ding can be done on-the-fly without pretraining or
finetuning the model. (3) Finally, multi-bit water-
marking has the same text quality as its zero-bit
counterpart. In addition, COLOR can also distin-
guish between machine and human text just like
zero-bit watermarking.
Our preliminary experiments demonstrate that
a 16-bit message can be embedded effectively in
short text lengths ( ∼100) with over 90% bit accu-
racy. At moderate text lengths ( ∼500), a 32-bit
message can extracted with an accuracy of 91.9%,
underscoring its feasibility in real-world applica-
tions.
2 Related Works
Watermarking has been studied in various types of
multimedia such as image (Potdar et al., 2005),
video (Asikuzzaman and Pickering, 2017), au-
dio (Hua et al., 2016), and natural language (Top-
kara et al., 2005). Following previous works (Zhu
et al., 2018; Luo et al., 2020), we use the term wa-
termarking to denote embedding information into
natural language in a manner that is robust against
possible attacks given an original text – in our case,
this is the output generated by a language model
given the prompt. This differs from steganography
that focuses more on the undetectability of a secret
message that is embedded in a multimedia (Ched-
dad et al., 2010; Tao et al., 2014), which may be a
completely arbitrary generated text for the purpose
of carrying a secret message (Fang et al., 2017).
Recently, methods relying on neural networks
have shown progress in natural language water-
marking, outperforming traditional methods thatrelies on rule-based watermarks (Topkara et al.,
2006b,a; Atallah et al., 2001). Abdelnabi and Fritz
(2021) propose an end-to-end framework where
a decoder network predicts the encoded message.
Yang et al. (2022) improve upon the quality of the
watermarked text by using an algorithmic approach.
Yoo et al. (2023) focus on the robustness and ca-
pacity, outperforming previous works on the two
aspects.
Meanwhile, directly watermarking language
models in a zero-bit manner during token genera-
tion has emerged as a promising approach for dis-
tinguishing language model outputs from human
text (Kirchenbauer et al., 2023a). While the afore-
mentioned works can also trivially extend their
multi-bit capability to zero-bit (through embedding
non-random messages such as all 1’s), their robust-
ness has not been verified against stronger forms
of attacks such as paraphrasing or copy-pasting hu-
man texts onto machine text. Conversely, the zero-
bit approach has shown to be promising against
such attacks as it can reinforce the watermark ev-
ery token (Kirchenbauer et al., 2023b). Several
works have improved upon Kirchenbauer et al.
(2023a), e.g., in the quality aspect during code
generation (Lee et al., 2023), undetectability of
the watermark (Christ et al., 2023), and its robust-
ness (Munyer and Zhong, 2023). We focus on
extending the prior work for a more proactive coun-
teraction towards malicious use of language models
by embedding anyinformation while maintaining
key advantages of zero-bit watermarking.
3 Method
We briefly review zero-bit watermarking intro-
duced in Kirchenbauer et al. (2023a) and elaborate
on extending this method to multi-bit watermark-
ing.
3.1 Zero-bit Watermarking
A watermark is embedded by biasing the language
model to output a certain subset of tokens. Given
an autoregressive language model that predicts the
next token with vocabulary V, a subset of tokens
is randomly selected from the vocabulary at each
token step tand forms a green list Gt. The logit
scores lt∈R|V|are modified towards selecting the
green-listed tokens in favor of the other tokens by
adding a bias term δto the logits in Gt.
Instead of fixing the greenlist using rule-based
heuristics, it is selected (pseudo-)randomly at eachFigure 1: Left: An overview of our method COLOR. The number inside a token (e.g. 1) denotes the sampled
position, while the color is determined by the message at that position. Right: Zero-bit watermarking without
position sampling and a single-colored list.
time step to minimize a noticeable shift in text dis-
tributions. At each time step, a seed sis outputted
dependent on the previous htokens using a pseudo-
random function f:Nh→N. Finally, the seed s
is used to select Gtfrom V.
3.2 Extending to Multi-bit Watermarking
The objective of multi-bit watermarking is to em-
bed and extract a ˜b-bit message M∈ {0,1}˜binto
the text. Our proposed method Multi-bit water-
marking via Color-listing (COLOR) is shown in
Figure 1 whereby the binary message is first con-
verted to r-radix message, then encoded into the
language model output through color-listing.
First, we color the vocabulary set with "multiple
colors" instead of "green-listing" a single subset.
While the number of colors rcan be arbitrarily
determined considering the trade-off between to-
ken diversity and watermark strength, we follow
zero-bit watermarking and determine it using the
greenlist proportion γ, i.e. r=⌊1
γ⌋. Thus, this
allows encoding rstates into each token as op-
posed to encoding zero-bit (whether the token is
selected from the greenlist or not). To encode the
original binary message, we convert the original
message into radix rattaining Mr∈[0, r−1]b
where b=˜b
⌊log2r⌋. In Figure 1 Left, we illustrate
when r= 4andb= 8, the 8-bit message is chun-
ked into four 2-bit messages and converted into
radix 4, resulting in a 4-digit message ( b=4). At
each token generation, the message content deter-
mines which color-list to add δto. If the message
content is "3", the tokens from the last partition are
favored.
While we now can map each token to rstates,
it still lacks the capacity to embed b-digit mes-
sage. To achieve this, we select the position (digit)p∈[0, b−1]prior to selecting the colorlist. Sam-
pling b−1and 0 signifies the most significant and
least significant digit, respectively. Similar to zero-
bit watermarking, we rely on the hash tokens to
seed the random number generator. By considering
each token as an opportunity for watermarking, we
effectively allocate these opportunities across mul-
tiple digits, as opposed to utilizing all tokens for a
single digit, as done in zero-bit watermarking. Re-
lying on the hash scheme for selecting the position
makes it robust against realistic use cases where
snippets of machine text and human-written text are
mixed together. This allows enjoying the relative
robustness of the hashing scheme towards simple
attacks that alter the total length (e.g. paraphrasing)
or mixing the watermarked text with human text.
Combined together, the two schemes increase the
bit capacity to b× ⌊log2r⌋.
Our method COLOR is extremely easy to imple-
ment over the zero-bit watermarking scheme with
minor modifications as shown below (highlighted
part indicates steps specific to ours). Given t−1
prefix tokens X1:t−1, bit message M, its converted
radix rmessage Mr, and pseudo-random function
f, thetthtoken is generated by
1.Compute hash of tokens s=f(Xt−h:t−1).
Usesto seed a random number generator.
2.p←sample ([0, r−1])
3.m←Mr[p]
4. Permute vocabulary Vtusing sas seed.
5.PartitionVt= [G0
t,···,Gr−1
t]discardingre-
main ders ifany.
6. Add δto token logits in Gm
t.
Note that zero-bit watermarking can be seen as a
special case of embedding a single bit message of
radix 2 ( b= 1,r= 2, and M= 0) as shown in
Figure 1 Right, or equivalently b=r= 1.Message extraction Given a watermarked mes-
sage, we extract the message by counting the num-
ber of tokens in all the color-listed tokens. After
sampling the digit, we loop through m∈[0, r−1]
and check whether the next token is present in Vt[m].
After computing this on the entire text segment, we
predict each digit by taking the color-list with the
most tokens. The radix rmessage is then converted
back to a binary message. A more detailed algo-
rithm is shown in Algorithm 1.
3.3 Discussions
Detecting Human Text Our multi-bit watermark-
ing scheme can be used to distinguish human text
just like zero-bit watermarking, which uses the
number of green-listed tokens for distinguishing
between human and machine texts. Similarly, we
take the number of color-listed tokens as our detec-
tion metrics (See Line 16 in Alg. 1). We show the
results in Section 4.2. We leave analyzing the sta-
tistical properties of our metric and its robustness
to attacks to future works.
Text Quality When we use the greenlist proportion
γto determine the number of colorlists, the propor-
tion of tokens favored by watermarking is identical
to zero-bit watermarking. Additionally, we allocate
each token to a digit, which was originally used
for reinforcing the strength of the single digit for
the zero-bit watermark, without any further mea-
sure. Essentially, this impacts the text distribution
only to the extent that zero-bit watermarking does,
implying that multi-bit information can be embed-
ded without any sacrifice in text quality. While
the effect of watermarking on the generated text
quality compared to no watermarking has not been
extensively studied yet, Kirchenbauer et al. 2023b
(Appendix A.2 and A.9) show promising results in
automatic and human evaluations when sufficiently
large models are used.
Naive Multi-bit Watermarking One can also
think of naively extending the zero-bit watermark-
ing scheme to encode multi-bit information. First,
one can encode one-bit information by choosing
whether to favor the green-list or not: a message
of "0" can denote having tokens from the green-
list and a message of "1" can denote having to-
kens from the non-green-list. Next, this can be
extended to multi-bit by uniformly partitioning the
tokens into multiple bit positions. For instance,
when generating 1000 tokens and embedding 16
bits, each segment of 31 ( ∼1000
32) tokens signifyAlgorithm 1: Watermark Extraction
Input: Watermarked text X1:T
Output: Predicted message ˆM, number of
colorlisted tokens c
/* Initialize counter for every position */
1forpin[0, b′]do
2 form in [0, r−1]do
3 COUNT p[m] = 0
/* Count whether token is in colored lists
*/
4fortin[h+ 1, T]do
5 s=f(Xt−h:t−1)
6 p←sample ([0, r−1])
7 form in [0, r−1]do
8 Permute Vtusing mas seed
9 ifXt∈ Gm
tthen
10 COUNT p[m]+= 1
/* Predict message */
11ˆMr=“ "
12c= 0
13forpin[0, b′]do
14 ˆm←argmax (COUNT p)
15 ˆMr+=str(ˆm)
16 c+=max(COUNT p)
17Get bit message ˆMby converting ˆMr
18return ˆM,c
each bit position. This naive extension, however,
is problematic as not all adversaries will constrain
the language model outputs to a pre-defined token
length in advance. Moreover, a more realistic use
cases will include mixing snippets of machine text
and human-written text. Simply assigning token
segments to bit position by their order is extremely
fragile against such modifications of text.
4 Experiments
4.1 Experimental Settings
Following Kirchenbauer et al. (2023a), we use OPT-
1.3b (Zhang et al., 2022) on the newslike subset
of the C4 dataset and continue generating until
reaching a specified amount of token length T. For
watermarking and generation, we follow the con-
figurations used in Kirchenbauer et al. (2023a,b)
unless otherwise denoted: bias δ= 2.0and the
greenlist ratio γ= 0.25, which have shown a good
trade-off between the detection performance and
generation quality. Since γ= 0.25, the numberof colors ris 4. We use the selfhash scheme with
4 previous tokens for hashing ( h= 4). For gen-
eration, we use sampling with a temperature of
0.7. For attacks, we consider the copy-paste attack
where an adversary use the output of the language
model in combination with external texts. This sim-
ulates a realistic use case of language models and
has shown to be hinders the detectability compared
to other types of attack (Kirchenbauer et al., 2023b).
We embed a random b-bit message onto 100-500
samples and report the mean metrics across sam-
ples. We use bit accuracy to measure how much
of the embedded bits can be extracted without er-
ror. For zero-bit watermark detection, we use Area
under ROC (AUROC).
4.2 Results
We first show how the watermark bias affects
the strength of the watermark by varying δ∈
{0.5,2,5,10}. As expected, the bias term enforces
the watermark as it increases the probability of
color-listed tokens being selected. However, this
comes at the cost of text quality as mentioned by
Kirchenbauer et al. 2023a. We see that at δ=5 level,
the effect of the bias term is already saturated and
does not lead to higher bit accuracy when δis fur-
ther increased. Upon qualitative inspection, we
observe that this is due to degraded outputs that are
highly repetitive, which leads to the same hash h
hence, the same position p. This causes no token
to be allocated to certain positions.
δ 0.5 2 5 10
Bit Acc. .621±.10 .919 ±.07 .972 ±.05 .969 ±.08
Table 1: Bit accuracy on embedding 32 bit-width mes-
sage on T=500 across various magnitudes of bias δ.
For the following experiments, we fix δ=2.0 and
vary bit width, token lengths, and attack strength.
When varying one variable, we fix the others. The
results are in Table 2. When varying the bit-width
at a fixed token length of T= 500 , we see as more
bit widths are embedded, accurately extracting the
message becomes more difficult. This is because
the number of tokens assigned to a specific position
linearly decreases as more bit width needs to be
embedded. Nevertheless, our method can achieve
over 90% accuracy when embedding 32 bits. Note,
however, the ability to distinguish human and ma-
chine text is not degraded by the bit-width as shown
by the nearly 1.0 AUROC across all bit-widths.Next, we vary Twhile fixing the bit-width to
16. The reason for the bit accuracy not being 100%
even at significantly long lengths is that some texts
lead to degraded outputs with highly repetitive out-
puts as mentioned earlier in the high bias exper-
iment. We hypothesize that this is likely due to
the limitation of the language model used in our
experiment.
Last, bit accuracy is measured in the presence
of copy-paste attacks. We copy a segment of hu-
man text on three random locations, synthesizing
a text with certain ratios of human text. Our ex-
periments demonstrate that multi-bit watermark
is significantly affected by attacks, highlighting a
large room for improvement in robustness. In ad-
dition, the difference in the accuracy for b=16 and
b=32 indicates embedding more bit-width makes
the watermark more fragile.
Bit Width @ T=500
Bit 8 16 24 32
BPW 0.016 0.032 0.48 0.064
Bit Acc. .989±.06 .969 ±.07 .939 ±.11 .919 ±.07
AUROC 1.0 .997 .999 .999
Token Length @ b=16
T 100 200 600 1000
BPW 0.16 0.080 0.027 0.016
Bit Acc. .900±.10 .946 ±.08 .973 ±.06 .980 ±.05
AUROC .986 .998 .998 1.0
Attack Strength @ T=500
Attacked
Ratio0.2 0.4 0.6 0.8
Acc@b=16 .890±.12 .823 ±.14 .749 ±.14 .644 ±.13
Acc@b=32 .804±.10 .750 ±.11 .677 ±.11 .601 ±.10
Table 2: Multi-bit watermarking performance and water-
mark detection performance across various bit widths,
token lengths, and attack strength.
5 Conclusion
Our preliminary results show the feasibility of em-
bedding any information into the outputs of lan-
guage models. This opens up a new possibility of
counteracting high-stake misuse of large language
model via API without downgrading ordinary users’
experience. While promising, the current results
are limited to a small (by recent standards) model,
which is prone to generating degenerate outputs
without a careful decoding scheme. In addition, the
robustness of the watermark both in bit accuracy
and watermark detection under attacks has not been
extensively verified.