Summary:
This paper explores the subtokenization options for large language model pretraining on source code. The authors investigate the effect of different subtokenizations on the performance and length-efficiency of the models. They propose a subtokenization method that reduces average length by 17% without sacrificing downstream performance. They also find that UnigramLM subtokenization performs slightly better than BPE. Additionally, they examine the impact of vocabulary size and transferability between programming languages. 

Bullet Points:
1. This paper investigates subtokenization options for large language model pretraining on source code.
2. The authors propose a subtokenization method that reduces average length by 17% without sacrificing downstream performance.
3. UnigramLM subtokenization performs slightly better than BPE.
4. Vocabulary size reduction may lead to a slight performance improvement but with sequences elongation.
5. The baseline subtokenizer is universal and can be used for other programming languages with small length increase.
6. The authors provide recommendations for choosing optimal subtokenization options for source code.
7. This work has the potential to improve the performance of pretrained language models on source code tasks.
8. The research may have a negative environmental impact due to the computational resources required.
9. There are no direct negative social impacts anticipated from this work.
10. Further research could explore the impact of subtokenization on both code and natural language processing.

Keywords:
- Subtokenization
- Large language model
- Pretraining
- Source code
- Length-efficiency
- Performance
- Vocabulary size
- Transferability
- UnigramLM
- BPE