Evaluating ChatGPT text-mining of clinical records for obesity
monitoring
Ivo S Fins1, Heather Davies1, Sean Farrell2, Jose R Torres3, Gina Pinchbeck1, Alan D Radford1
and Peter-John Noble1
1Small Animal Veterinary Surveillance Network, Institute of Infection, Veterinary and Ecological Sciences, University
of Liverpool, Liverpool, UK
2Department of Computer Science, Durham University, Durham, UK
3Institute for Animal Health and Food Safety, University of Las Palmas de Gran Canaria, Canary Archipelago, Las
Palmas, Spain
ABSTRACT
Background: Veterinary clinical narratives remain a largely
untapped resource for addressing complex diseases. Here we
compare the ability of a large language model (ChatGPT) and
a previously developed regular expression (RegexT) to iden-
tify overweight body condition scores (BCS) in veterinary nar-
ratives.
Methods: BCS values were extracted from 4,415
anonymised clinical narratives using either RegexT or by ap-
pending the narrative to a prompt sent to ChatGPT coercing
the model to return the BCS information. Data were manually
reviewed for comparison.
Results: The precision of RegexT was higher (100%, 95%
CI 94.81–100%) than the ChatGPT (89.3%; 95% CI82.75-
93.64%). However, the recall of ChatGPT (100%. 95% CI
96.18-100%) was considerably higher than that of RegexT
(72.6%, 95% CI 63.92-79.94%).
Limitations :Subtle prompt engineering is needed to im-
prove ChatGPT output.
Conclusions: Large language models create diverse oppor-
tunities and, whilst complex, present an intuitive interface to
information but require careful implementation to avoid un-
predictable errors.
Keywords: obesity, veterinary health informatics, text-
mining, Artificial Intelligence, ChatGPT
INTRODUCTION
Obesity is a common and significant medical condition in
companion animals (1,2). The Small Animal VeterinarySurveillance Network (SA VSNET) collects anonymised elec-
tronic health records (EHRs) from veterinary practices in real-
time (3). These remain an unexploited resource for investigat-
ing canine health, with relevant clinical information often sub-
merged in unstructured free text. Automated systems to sur-
face this information are therefore essential where reading and
manual annotation is infeasible. Regular expressions, tools de-
signed to detect fixed word-patterns, have often be used in this
setting. Such methods try to try to identify implicit negation
(“not vomiting”) and contextual information that indicates a
feature is not present (“come back if there are any signs of
vomiting”) and require complex rules to accommodate var-
ied/unpredictable language (4).
Recently, large language models (LLM) including genera-
tive pre-trained transformers (such as GPT3.5 which under-
pins ChatGPT) have become available. These complex neural
networks, with hundreds of billions of parameters (5), trained
using vast datasets to generate responses to preceding text (4-
6) can generate human-like responses to complex prompts.
These provide an exciting opportunity for automated data ex-
traction (7-9) and studies assessing their veterinary application
are urgently required.
Here we compare the performance of a validated rule-base
system using regular expressions (RegexT, 10) to that of a
prompt-based approach using ChatGPT to identify body con-
dition score (BCS) of the patient if recorded at the time of the
consultation in a sample of publicly available clinical narra-
tives.
MATERIALS AND METHODS
The Small Animal Veterinary Surveillance Network (SA VS-
NET) collects EHR from a sentinel network of UK veterinary
Ivo S Fins., et al. 1arXiv:2308.01666v1  [cs.IR]  3 Aug 2023practices. Each EHR contains a clinical narrative written by
the attending practitioner. This study used an anonymised ran-
dom sample of 4,415 EHRs available in the public domain
(11). These narratives were read by domain experts to identify
overweight BCSs recorded either on a five-point scale ( ≥3.5
out of 5 are considered overweight) or nine-point scale ( ≥6
out of 9 are considered overweight). The collection and use
of EHRs by SA VSNET is approved by the Research Ethics
Committee at University of Liverpool (RETH001081).
A regular expression (RegexT) was designed to detect over-
weight BCSs considering the variety of notation used by prac-
titioners to record both the denominator and numerator in free
text (Figure 1).
In parallel, we refined a ChatGPT prompt describing four
basic rules to coerce output of any BCS along with a prediction
regarding overweight status. Data extraction was performed
using Python version 3.7.10 (12). GPT3.5 Turbo (13) was ac-
cessed through a Python API processing multiple EHRs, each
a separate row, appended to the prompt in Figure 2.
The accuracy of these RegexT and ChatGPTs systems were
assessed based on the returned BCS using precision (equiv-
alent to positive predictive value), and recall (equivalent to
sensitivity). The 95% confidence intervals (95% CI) were
calculated by modified Wald method using GraphPad (Quick-
Calcs)® (14).
RESULTS
Extracting overweight BCS measurements with ChatGPT
Reading 4415 records, identified 117 (2.65%) reporting an
overweight BCS.
ChatGPT identified all 117 overweight BCSs (recall 100%;
95%CI 96.18-100%). The output variably fitted the struc-
ture requested by the prompt requiring some manual interpre-
tation. Only 89 were additionally described as being over-
weight. Twelve GPT outputs appended text either peripheral
to the task or generally not relevant to weight such as “Last
WORM and FLEA treatments?” and “Possible dietary indis-
cretion”; some of these additional texts were however rele-
vant (“Coming down in weight. 600g since last September.
O thinks because he is getting less treats.”).
ChatGPT falsely identified BCSs in 14 records; these re-
flected extraction of other similarly formatted clinical in-
formation such as lameness scoring (e.g., EHR-text “6/10
lameness”; ChatGPT-output “BCS 6/9. BCS overweight”.)
and body weight information (e.g., EHR-text “wt 6.65kg”;
ChatGPT-output: “BCS 6.65/9”). The precision of ChatGPT
was therefore 117/131 (89.3%; 95%CI 82.75-93.64%).
ChatGPT classified an additional 61 consultations as be-ing overweight in the absence of a recorded overweight BCS.
Forty of these were also described as overweight by the attend-
ing veterinarian (eg “she is overweight”, “normal on clinical
exam apart from overweight”). Of the remaining 21 records
ChatGPT recorded a normal or low BCS as overweight, or
an erroneous false positive high BCS. For example, ChatGPT
coded “BCS 5.75/9 would benefit from further weight loss” as
overweight despite BCS not crossing the threshold of 6.
ChatGPT failed to return an appropriate answer for 29
records, instead outputting texts such as “Hello! How can I as-
sist you today?”. This was often associated with short narra-
tives (27 less than 23 characters in length) such as “pay now”
and “all ok sign off”, none containing BCS information.
In comparison, from the 4415 records, RegexT successfully
identified 85 of the 117 (recall 72.6%; 95% CI 63.92-79.94%)
narratives containing an overweight BCS and no false posi-
tives (precision 100%; 95% CI 94.81–100%). The 32 over-
weight BCS missed were associated with format variants not
captured by the regex syntax (eg “BCS: 6-7 out of 9” and
“BCS: 6/9”). Clearly the regex did not identify any of the
40 narratives that lacked a BCS, but that were described as
overweight by the vet and identified by ChatGPT.
Full list of narratives, regex and ChatGPT outputs is avail-
able in supplementary material.
DISCUSSION
To leverage the true value of clinical free text in large health
datasets to understand complex diseases like obesity will re-
quire careful application of increasingly complex text mining
solutions. Regular expressions have been used to identify a
wide range of disease phenotypes based on coded patterns of
text. More recently, LLMs are offering novel solutions in a
wide range of situations. Here we assess the strengths and
weaknesses of a LLM (GPT 3.5 Turbo) for a named entity
recognition task identifying overweight BCSs in clinical vet-
erinary EHRs, comparing the results to a regular expression
and to manual reading.
Using an intuitive prompt, ChatGPT was coerced to iden-
tify both all overweight BCSs and overweight animals without
a reported BCS. In contrast, the regex method missed BCSs
with novel unpredicted formatting. In this setting of obesity,
ChatGPT efficiently identified most overweight animals, with
or without a BCS and could be further used to aid the reengi-
neering of systems based on regular expressions. However, oc-
casional false positives were identified by ChatGPT, often as-
sociated with other scores like lameness; this behaviour might
be avoidable through more subtle prompt engineering (e.g.,
by prompting: “exclude lameness scores normally recorded
2 Ivo S Fins., et al.out of ten and heart murmur scores, normally recorded out
of six”). When high accuracy is the goal, many case studies
may still require a final manual classification. Any follow-on
manual reading task is made simpler again by prompt engi-
neering, attempting to constrain ChatGPT to tabular output.
However, in our study, this frequently failed, making the man-
ual classifying step somewhat more complex than envisaged.
Future studies may focus on engineering these prompts to re-
duce false positives, and to tighten the output structure (15).
ChatGPT could create overtly false assertions (sometimes
described as hallucinations), sometimes comprising under-
standable unhelpful output. In other settings these can be far
more fanciful (16, 17).
CONCLUSIONS
Despite the complexity of both clinical narratives and the un-
derlying technology of ChatGPT, we were able to efficiently
extract overweight BCSs with higher recall than a rule-based
system. Challenges remain around the ethics of submitting
health texts to an online server; here we used a public domain,
anonymised dataset. Issues of cost prohibiting screening big-
ger datasets may be overcome by installing in-house increas-
ingly available free models. The two systems (regex and lan-
guage model) offer some complementarity. Future improved
prompt engineering will enhance precision and outputted text
format.
DATA AVAILABILITY STATEMENT
The data that supports the findings of this study are available
in the supplementary material of this article.
AUTHOR CONTRIBUTIONS
Hypothesis generation and experimental design: Ivo S. Fins,
Sean Farrell, Peter-John Noble. Data analysis and validation:
Ivo S. Fins, Heather Davies, Sean Farrell, Gina Pinchbeck,
Jose R. Torres, Alan Radford, Peter-John Noble. Writing and
revising the manuscript: Ivo S. Fins, Alan Radford, Heather
Davies, Sean Farrell, Peter-John Noble.
ACKNOWLEDGEMENTS
Ivo Fins is currently funded by Dogs Trust. We are also grate-
ful for prior support and major funding from BSA V A and BB-
SRC. Heather Davies is funded by the University of Liverpool
and the Veterinary Medicines Directorate (VMD). Sean Farrell
is funded as part of a BBSRC-DTP. J Torres is funded by Pet-
Plan Charitable Trust. We wish to thank data providers bothin veterinary practice (VetSolutions, Teleos, CVS, and other
practitioners) and in veterinary diagnostic laboratories, with-
out whose support and participation this research would not be
possible.
FUNDING AND COMPETING INTERESTS
STATEMENT
This project was part funded by Dogs Trust (SA VSNET-Agile
grant). All authors declare that they have no conflicts of inter-
est.
