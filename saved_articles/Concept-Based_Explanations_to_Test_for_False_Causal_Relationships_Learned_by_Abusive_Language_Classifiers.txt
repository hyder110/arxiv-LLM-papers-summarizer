Concept-Based Explanations to Test for False Causal Relationships
Learned by Abusive Language Classifiers
Isar Nejadgholi, Svetlana Kiritchenko, Kathleen C. Fraser, and Esma Balkır
National Research Council Canada
Ottawa, Canada
{Isar.Nejadgholi,Svetlana.Kiritchenko,Kathleen.Fraser,Esma.Balkir}@nrc-cnrc.gc.ca
Abstract
Classifiers tend to learn a false causal relation-
ship between an over-represented concept and
a label, which can result in over-reliance on the
concept and compromised classification accu-
racy. It is imperative to have methods in place
that can compare different models and identify
over-reliances on specific concepts. We con-
sider three well-known abusive language classi-
fiers trained on large English datasets and focus
on the concept of negative emotions , which is
an important signal but should not be learned as
a sufficient feature for the label of abuse. Mo-
tivated by the definition of global sufficiency ,
we first examine the unwanted dependencies
learned by the classifiers by assessing their ac-
curacy on a challenge set across all decision
thresholds. Further, recognizing that a chal-
lenge set might not always be available, we
introduce concept-based explanation metrics
to assess the influence of the concept on the
labels. These explanations allow us to com-
pare classifiers regarding the degree of false
global sufficiency they have learned between a
concept and a label.
Content Warning: This paper presents exam-
ples that may be offensive or upsetting.
1 Introduction
In various natural language classification tasks,
particularly in abusive language detection, certain
concepts are known to be strong signals for the
label of interest. These concepts are often over-
represented in the respective class of the training
set, making them susceptible to being learned as
potential causes for the label. Consequently, the
classifier over-relies on these concepts and ignores
the broader context, leading to reduced generaliz-
ability (Yin and Zubiaga, 2021). Hence, to ensure
models are robust and reliable, it is crucial to de-
velop methods that can detect these over-reliances
in various natural language classification tasks.
In the context of abusive language detection, we
consider the concept of negative emotions . The
Figure 1: Probability of offensiveness generated by the
TweetEval Classifier (Barbieri et al., 2020). The clas-
sifier has learned a false global sufficiency between
negative emotions and the label of offense. It over-relies
on this concept and ignores the broader context.
presence of an expression associated with negative
emotion is an important signal for detecting abusive
language and has been used in feature-based sys-
tems before (Chiril et al., 2022; Fortuna and Nunes,
2018). Crucially, in some examples, negative emo-
tion words might be the cause for the abusive label,
i.e., the sentence might not be abusive if the neg-
ative emotion word is replaced with other words
(e.g., I know these people. They are disgusting ).
However, at the global level, the relationship be-
tween negative emotion and abusive language is
a strong correlation, not causation, as it is neither
globally necessary nor globally sufficient for the
label of abuse.1Negative emotions are not globally
necessary for the label of abuse because there are
abusive sentences that do not contain any negative
emotion words (e.g., offensive jokes, stereotyping
and microaggressions). Also, words evoking nega-
tive emotions are not globally sufficient for a sen-
tence to be abusive when interpreted in a broader
context (e.g., We should admit that in our society,
they are oppressed .). But, an end-to-end model
might learn that negative emotion in a sentence is
globally sufficient for that sentence to be abusive.
Such a classifier will struggle in classifying non-
abusive sentences that contain negative emotion
1Phenomenon P is globally sufficient for the Phenomenon
Q, if whenever P happens, Q happens too. P is globally neces-
sary for Q if whenever Q happens, P happens, too (Zaeem and
Komeili, 2021).arXiv:2307.01900v1  [cs.CL]  4 Jul 2023words leading to a lack of generalizability. An ex-
ample of such a case is shown in Figure 1. Specifi-
cally, classifiers’ over-reliance on the negative emo-
tion signal can inadvertently discriminate against
marginalized groups since their communications
(e.g., discussing their experiences of discrimination
and marginalization) can contain negative emotion
words and, therefore can be wrongly considered
abusive.
We explore a scenario where a user, aware of the
importance of negative emotions for their use case,
wants to evaluate and compare a set of trained mod-
els. Their goal is to identify and eliminate those
models that are more prone to generating inaccu-
rate results due to an overemphasis on negative
emotions as primary indicators. For that, we use
concept-based explanations to test if a model has
learned a false global causal relationship between
a user-identified concept and a label where the true
relationship is a correlation. Note that global causal
relationships explain the model’s output across an
entire dataset, as opposed to local causal explana-
tions, which concern the dependency of an individ-
ual prediction on a specific input feature.
Concept-based explanations are a class of ex-
plainability methods that provide global explana-
tions at the level of human-understandable concepts
(Yeh et al., 2022). While local explanations help
the users understand the model’s reasoning for an
individual decision with respect to the input fea-
tures, global explanations are critical in comparing
the processes learned by models and selecting the
one that best suits the needs of a use case (Balkir
et al., 2022; Burkart and Huber, 2021). Global
explanations might be obtained at the level of in-
put features through aggregating local explanations
(Lundberg et al., 2020). Alternatively, global-by-
design methods (e.g., probing classifiers (Conneau
et al., 2018)) can be used to gain insights at higher
levels of abstractions, such as linguistic properties
or human-defined concepts.2
Similar to most feature importance explainabil-
ity methods (e.g, Ribeiro et al. (2016); Lundberg
and Lee (2017a)), concept-based explanations are
originally designed to measure the importance of a
concept. The intuitive meaning of importance usu-
ally refers to correlation, and it can be interpreted
differently based on two notions of causality: neces-
sity and sufficiency (Galhotra et al., 2021). Local
2Here, we use the term “feature” to refer to the latent
representations of a semantic concept learned by a classifier.explainability methods usually focus on features
that are of high local necessity or high local suf-
ficiency for the label (Watson et al., 2021; Balkır
et al., 2022; Joshi et al., 2022), thus considered
important by human users. However, at the global
level, all features must be interpreted in a larger
context for accurate decision-making. We aim to
determine if concept-based explanations can be uti-
lized to evaluate whether a trained binary classifier
for abusive language detection has learned a false
global sufficiency relationship between the label
and the concept of negative emotion. Our code and
data are available at https://github.com/Isa
rNejad/Global-Sufficiency/tree/main . Our
main contributions are:
•We formalize the issue of over-reliance on a con-
cept as falsely learned global sufficiency. For
the task of an abusive language classifier, we
consider concepts related to negative emotion as
being important but not globally sufficient for the
label of abuse. We discuss how learning these
concepts as globally sufficient results in compro-
mised classification accuracies.
•Based on our formalization of false global suf-
ficiency, as a baseline method, we measure the
over-reliance of models on a human-defined con-
cept using an unseen challenge set that contains
the concept in both classes. Recognizing that
various classifiers may have a distinct range of
optimal decision thresholds, we assess the over-
reliance on a concept across all possible decision
thresholds and show that one of the classifiers
over-relies on emotion-related concepts signifi-
cantly more than the other two classifiers.
•Taking the challenge set approach as a baseline
for comparison, we propose novel concept-based
explanation metrics, demonstrating that similar
conclusions about the degree of false global suf-
ficiency can be drawn using these metrics. Build-
ing on previous work, we modify the TCA V pro-
cedure to measure not only the feature’s impor-
tance but also the extent of its impact on the
label. We conclude that a concept-based method
is preferable as it eliminates the need for manual
data curation.
2 Concept-Based Explanations
Concept-based explanations evaluate the model’s
decision-making mechanism at the level of ahuman-defined concept expected to be important
for the task (Koh et al., 2020). Specifically, we use
the Testing Concept Activation Vectors (TCA V)
method to measure the influence of a human-
defined concept on the model’s predictions (Kim
et al., 2018). The idea of TCA V is based on the
observation that human-understandable concepts
can be encoded as meaningful and insightful infor-
mation in the linear vector space of trained neural
networks (Mikolov et al., 2013). A Concept Acti-
vation Vector (CA V), which represents the concept
in the embedding space, is a vector normal to a
hyperplane that separates concept and non-concept
examples. Such a hyperplane is obtained by train-
ing a linear binary classifier to separate the repre-
sentations of concept and non-concept examples in
the embedding space.
Although TCA V can be applied to all neural net-
work classifiers, for simplicity we limit our experi-
ments to binary RoBERTa-based abusive language
classifiers. We choose the RoBerta-based models
for their superior performance in processing social
media data compared to other base language mod-
els (Liu et al., 2019). The concept, C, is defined
byNCconcept examples. Also, NRrandom exam-
ples are used to define non-concept examples. The
RoBERTa representations for all these examples
are calculated using femb, which maps an input text
to its [CLS] token representation. Then, Pnumber
of CA Vs, υp
C, are generated, each through training
a linear classifier that separates a sub-sample (with
sizeNc) of concept examples from a sub-sample of
random examples (with size Nr) in the RoBERTa
embedding space. The conceptual sensitivity of a
label to the CA V , υp
C, at input xcan be computed
as the directional derivative SC,p(x):
SC,p(x) = lim
ϵ→0h(femb(x)+ϵυp
C)−h(femb(x))
ϵ
=▽h(femb(x)).υp
C(1)
where hmaps the RoBERTa representation to the
logit value of the class of interest.
In this work, we use two metrics to specify the
influence of the concept on the model’s prediction.
First, we calculate TCAV dir, the fraction of in-
puts in a set of input examples X, for which the
directional derivative SC,p(x)is positive, i.e.:
TCAVC,p
dir=|x∈X:SC,p(x)>0|
|X|(2)
TCAV dirindicates the fraction of input exam-
ples for which the prediction scores of the modelincrease if the input representation is infinitesimally
moved towards the concept representation. This
metric has been widely used to identify if the label
has learned the concept as an important signal for
the label (Yeh et al., 2020).
Besides the widely used metric of TCAV dir(re-
ferred to as TCAV score in previous work), we in-
troduce a new metric, TCAV mag, which considers
the size of the directional derivatives, and measures
the magnitude of the influence of the concept on
the label for the positive directional derivatives:
TCAVC,p
mag=P
x∈X,SC,p(x)>0SC,p(x)
|X|(3)
We demonstrate in our results that TCAV mag
can be an indicator of the over-reliance of the la-
bel on the concept. When calculated for all CA Vs,
Equations 2 and 3 generate two distributions of
scores with size Pfor the concept C. Using a
t-test, these distributions are compared with the dis-
tributions of TCAV dirandTCAV magcalculated
for random examples to check for statistical signifi-
cance (Kim et al., 2018).
3 False Global Sufficiency
Phenomenon P is considered globally sufficient for
phenomenon Q ( P⇒Q) if, whenever P occurs, Q
also occurs (Zaeem and Komeili, 2021). In other
words, global sufficiency refers to the extent to
which a concept can explain the model’s output
across all instances in a held-out dataset, as op-
posed to the more studied topic of local sufficiency,
which concerns the stability of an individual pre-
diction for a given feature in perturbed contexts
(Balkır et al., 2022).
In a real-world setting, it is very unlikely that
any single concept is truly sufficient for the label at
a global level. In a binary classifier, a concept Cis
falsely learned as sufficient for the positive label if
all inputs containing Care classified as positive by
the classifier, regardless of context. This undesired
dependency of the label on the concept suggests
that the model has failed to learn how the concept
interacts with context to influence the label. While
this issue is closely related to spurious correlation,
we use the term false global sufficiency because
spurious correlation typically implies that the fea-
ture is irrelevant to the label, and a correlation is
learned due to a confounding factor. In contrast,
we consider the cases where the feature is relevant
and important but not globally sufficient.Figure 2: Illustration of the potential distribution of probabilities generated by a trained binary classifier for a
challenge set that represents an important concept, along with accuracy versus threshold curves.
To make this clearer, consider the case of abu-
sive language detection and the concept of negative
emotions; if the mere presence of negative emo-
tions in a sentence always guarantees the predic-
tion of the positive label (abuse), then the model
has learned a false sufficiency relation between the
concept and the label. It over-relies on this feature
and ignores the context.
To quantify falsely learned global sufficiency,
we consider two scenarios: 1) where a balanced
challenge set is available, which contains Cin all
of its examples (both classes), and 2) where no
challenge set is available. For the first scenario
we use the traditional approach of assessing accu-
racy of the classifier on a held-out test set. This
approach provides a baseline in our evaluations.
For the second scenario, we propose concept-based
explanation metrics and compare them with the
baselines obtained with the challenge sets.
3.1 Quantifying the Falsely Learned Global
Sufficiency with a Challenge Set
Based on our definition of global sufficiency, one
way to assess a model’s over-reliance on a concept
is to evaluate its performance on a held-out chal-
lenge set, F, containing both positive and negative
examples of the concept of interest (Yin and Zu-
biaga, 2021). For simplicity, we assume that this
challenge set consists of equal numbers of positive
and negative examples. If a model learns a high
global sufficiency between the concept Cand the
label of abuse, all examples in both positive and
negative classes of a challenge set Fwill be labeled
as abusive. However, if the model interprets the
concept in context, only the positive examples of F
will receive the abusive label. This indicates that incases where the decision threshold of the classifier
clearly separates the probability distributions of the
two classes, the model has learned a low global
sufficiency between the concept and the label.
However, when comparing different classifiers,
it is important to note that a reliable classifier
should perform well (high precision and high re-
call) over a broad range of decision thresholds.
This is because different applications may require
different thresholds depending on the desired trade-
off between precision and recall. For example, a
classifier used to moderate social media content
may need to prioritize precision over recall, which
could mean using a high threshold to avoid false
positives. On the other hand, a classifier used to de-
tect all instances of abusive language may need to
prioritize recall over precision, which would mean
using a lower threshold to catch as many instances
of abuse as possible, even if it means tolerating
more false positives. Therefore, a classifier that is
reliable over a wide range of decision thresholds
can be more effective in different use cases, making
it more practical and adaptable.
Figure 2 demonstrates two hypothetical cases for
the distribution of probabilities that the classifiers
might generate for the challenge set F. A classi-
fier that learned low global sufficiency between C
and the positive label generates easily separable
distributions of probabilities for the positive and
negative examples of F. In other words, for a large
range of decision thresholds, the two classes of F
are separable, and high accuracy is achieved. Con-
versely, the classifier that has learned high global
sufficiency between Cand the positive label as-
signs a similar distribution of probabilities to both
negative and positive examples. The two classesofFare hardly separable, and for a wide range of
thresholds, the accuracy is low. Note that in order
for this classifier to be accurate, it requires a careful
adjustment of the decision threshold with a labeled
dataset. However, this process can be very costly.
Based on this discussion, we argue that
AUC _Challenge , the area under the curve of ac-
curacy vs threshold, is a quantitative indicator of
the separability of two classes of Ffor all de-
cision thresholds. According to our definition
above, global sufficiency is negatively correlated
with the separability of these classes. Therefore,
False _Suff , described in Equation 4, is a quanti-
tative metric that can be used to compare the degree
of sufficiency learned by the classifiers based on F:
False _Suff = 1−AUC _Challenge (4)
3.2 Quantifying the Falsely Learned Global
Sufficiency with Concept-Based
Explanations
The practical application of the method detailed
in Section 3.1 can be limited due to the necessity
of creating a custom challenge set. In this sec-
tion, we use concept-based explanation to measure
the falsely learned global sufficiency in a scenario
where a challenge set is not available, but a lexicon
representing the concept of interest exists. Follow-
ing the approach of Nejadgholi et al. (2022a), we
employ short templates and the concept lexicon to
generate unlabeled concept examples. Then, we
utilize the method described in Section 2 to com-
pute two metrics: TCAV dirandTCAV mag. If the
TCAV dirvalue for the concept significantly devi-
ates from that of random concepts, it indicates that
the classifier has learned an association between the
label and the concept. A significant difference in
TCAV magcompared to random concepts suggests
a strong influence of the concept on the label, poten-
tially causing the classifier to disregard the context
when the concept is present. While the absolute
values of these metrics might not be definitive, we
show that they can be used to compare various clas-
sifiers in terms of the degree of global sufficiency
they have learned for a concept.
4Sufficiency of the Concept of Describing
Protected Groups with Negative Emotion
In this section, we evaluate the metrics introduced
in Section 3 in explaining the extent of the falsely
learned sufficiency between a human-defined con-
cept and the positive label of the classifiers. We
Figure 3: Probability distributions generated by the clas-
sifiers for a challenge set (F2 and F21 of HateCheck).
specifically consider the concept of describing a
protected group with negative emotion words and
refer to it as DesNegEm for brevity. We chose this
concept because it is tightly related to hate speech
and is expected to be important for more general
definitions of harmful language, such as toxic, abu-
sive or offensive. Still, it is not a sufficient concept
for these labels and has to be interpreted in the
broader context (as shown by examples in Table 1).
We consider three RoBERTa-based binary clas-
sifiers, publically available and trained with large
English datasets. The models are trained for gen-
eral definitions of abusive language, toxicity or
offensive language. We refer to these classifiers by
their training datasets: Jigsaw, Civil Comments (or
Civil for brevity) and TweetEval. These models are
described in detail in Appendix A
Quantifying Sufficiency with a Challenge Set:
To calculate the metric described in Section 3.1,
we first use the HateCheck (Röttger et al., 2021)
test cases to build a challenge set for the concept
ofDesNegEm . For that, we use the F2 and F21
functionalities of HateCheck, i.e., the hateful and
non-hateful examples that include this concept (Ta-
ble 1). Figure 3 shows the distribution of proba-
bilities that the three classifiers generate for this
challenge set. We observe that, for a large range
of decision thresholds, all three classifiers label the
majority of the examples of both classes of the chal-
lenge set with a positive label. In other words, all
three classifiers have learned a high sufficiency be-
tween DesNegEm and the label of abuse. However,
the extent of the learned sufficiency is different
among the classifiers. The TweetEval classifier
makes the least differentiation between the two
classes and generates similar distributions of prob-
abilities for negative and positive examples with
theDesNegEm concept. Because of this overlap
between probability distributions of positive and
negative classes, the accuracy of this classifier is
low over all ranges of thresholds, as shown in Fig-Functionality Example Test Case Gold Label n
F2: Description using very negative
attributes“[IDENTITY] are disgusting.”
“All [IDENTITY] like them are vile.”hateful 140
F21: Denouncement of hate that make
direct reference to it“You have to stop calling [IDENTITY] dis-
gusting.”non-hate 140
Table 1: Used test cases from HateCheck (Röttger et al., 2021).
Figure 4: Accuracy vs threshold curve for the challenge
set (F2 and F21 of HateCheck).
ure 4. The false sufficiency learned by the Jigsaw
and the Civil Comments classifiers is less extreme,
and Jigsaw makes the most differentiation between
the two classes.
This observation can be quantified with the
False _Suff metric (Equation 4) using the area
under the curves in Figure 4. We obtain
False _Suff of0.41,0.40, and 0.50for Civil,
Jigsaw and TweetEval, respectively. This metric
shows a higher falsely learned sufficiency score
for TweetEval than the two other classifiers, as ex-
pected. Based on these observations, we expect
TCA V metrics to show lower scores for Civil and
Jigsaw than the TweetEval classifier.
Global Sufficiency with Concept-Based Expla-
nations: Here, we use the results obtained with the
challenge set as a baseline to evaluate the TCA V-
based metrics. Concept examples are generated
using the template ‘ <protected_group> are <emo-
tion_word>. ’, where <protected_group> is one of
the protected groups women ,trans people ,gay peo-
ple,black people ,disabled people ,Muslims andim-
migrants as identified by Röttger et al. (2021). For
<emotion_word> , we use the disgust andanger
categories of the NRC Emotion Intensity Lexicon
(NRC-EIL) (Mohammad, 2018). We use the NLTK
package3to filter out words other than adjectives,
past tense verbs and past participles, and also re-
move the words with emotion intensity lower than
3https://www.nltk.org/0.5. After these steps, we are left with 368 concept
words. We calculate the TCAV dirandTCAV mag
scores for the concept of DesNegEm and compare
those to the metrics calculated for random concepts
with t-test for statistical significance. For random
concepts, the concept examples are random tweets
collected with stop words. In our implementation
of the TCA V procedure, NR= 1000 ,Nc= 50 ,
Nr= 200 andNC= 386 (number of filtered lexi-
con words). For input examples, X, we use 2000
tweets collected with stop words.
As presented in Table 2, for the Civil classifier,
TCAV diris not significantly different from the
random concept, indicating that the concept infor-
mation might not always be encoded as a coherent
concept in the embedding space of this classifier.
However, TCAV magis significantly higher than
random, indicating that when the information is
encoded well, the presence of this concept has a
significant influence on the label of abuse. The
other two classifiers have learned a strong associa-
tion between the concept and the label, i.e., when
the concept is added to a neutral context, the likeli-
hood of the positive label increases. However, only
in the case of the TweetEval classifier, TCAV mag
is significantly different from the random concepts,
indicating a strong influence of the concept on the
label, which might override the context. Therefore,
for TweetEval the distribution of generated prob-
abilities is mostly determined by the concept, not
the context (similar distributions are obtained for
the positive and negative examples of the challenge
set). The other two classifiers consider the con-
text to some extent and generate relatively different
distributions of probabilities for the two classes.
Discussion: For all classifiers, the presence of the
concept describing a protected group with negative
emotion words is a strong signal for the label of
abuse. All classifiers struggle in considering the
broader contexts in sentences such as ‘ It is not ac-
ceptable to say <protected_group> are disgusting. ’
Among the three classifiers, TweetEval has learned
a higher degree of sufficiency, leading to its worse
performance on a challenge set containing this con-TCAV dir TCAV mag
classifier DesNegEm random DesNegEm random
Civil 0.67(0.05) 0.5(0.4) 0.05(0.03) 0.00(0.01)
Jigsaw 1(0) 0.7(0.4) 0.04(0.01) 0.05(0.05)
TweetEval 1(0) 0.7(0.4) 0.15(0.03) 0.01(0.01)
Table 2: Mean and standard deviation of the TCA V score for explaining the sufficiency of Describing Protected
Groups with Negative Emotion (DesNegEm) for the three classifiers. All scores statistically significantly different
from random concepts are in boldface.
cept. The TCA V metrics can be used to compare
the classifiers regarding the false sufficiency rela-
tionships they have learned. These metrics provide
similar insights to what is learned from assessing
global sufficiency with a challenge set.
5 Global Sufficiency of Fine-Grained
Negative Emotions Concepts
In the previous section, we considered the con-
cept of describing protected groups with negative
emotions , which is tightly related to hate speech,
and thus prone to be mistakenly learned as suffi-
cient for the label of abuse. In this section, we
test our proposed method for a less obvious case
by disentangling the concept of emotions and hate
speech. We focus on the concept of describing a
(non-protected) group of people with negative emo-
tions , which differs from the previous section in 1)
removing the protected groups and replacing them
with unprotected groups and 2) breaking down the
emotion concept to more fine-grained levels.
For fine-grained emotion concepts, we first de-
velop a compact challenge set, examples of which
are presented in Table 3. Since we consider non-
protected groups in this challenge set, the examples
are labeled as abusive/non-abusive as opposed to
hateful/non-hateful in HateCheck (shown in Ta-
ble 1). We assess the sufficiency of these concepts
with the challenge set first and then compare the
results to those of the proposed concept-based ex-
planation metrics. Our goal is to investigate if
the findings for the broad concept of describing
protected groups with negative emotions can also
be replicated at a more nuanced level of emotional
granularity. We analyze the models for fine-grained
categories of negative emotions, identified by Mo-
hammad (2018), namely disgust ,anger ,sadness ,
andfear. Similar pre-processing steps to what was
described in Section 4 were performed to filter the
lexicon in each category of emotions.
For the challenge set, we write five abusive
and five non-abusive example templates for eachemotion. Then we generate 40 abusive and 40
non-abusive examples by replacing <group> with
one of the terms Canadians, Chinese people, doc-
tors, teachers, school children, football players, my
neighbours, and men to represent non-protected
groups.4Full list of examples of this challenge set
is available in our GitHub repository mentioned in
Section 1. Equivalently, for the TCA V procedure
for concept templates, we use ‘ They are <emo-
tion_word> ’, instead of ‘ <protected_group> are
<emotion word>’ , which we used in Section 4.
5.1 Results
We first compare the three classifiers in handling
negative emotions by investigating the results they
produce for the challenge set. The False Suff
scores in Table 5 show that TweetEval has learned
the highest sufficiency between these concepts and
the label of abuse and therefore achieves the low-
est separability between the positive and negative
classes of the challenge set. To further clarify this
we show the accuracy vs threshold curve for the dis-
gust category of the challenge set in Figure 5. We
observe that TweetEval only reaches high accura-
cies for a small range of thresholds, i.e, it generates
a similar distribution of probabilities for the posi-
tive and negative classes that contain the emotion of
disgust . On the other hand, Jigsaw has learned the
least global sufficiency and reaches high accuracy
over a wide range of thresholds.
Then we turn to the TCA V scores shown in Ta-
ble 4. First, TCAV dirshows that the Civil Com-
ments classifier is not significantly sensitive to neg-
ative emotions, i.e., the feature of negative emo-
tions is not fully learned as a coherent feature by
this classifier. TweetEval, on the other hand, shows
significant TCAV dirandTCAV magscores, indi-
cating that this classifier is not only sensitive to
these concepts but the influence of the concept on
4Though nationality may be considered a protected char-
acteristic in some contexts, we include “Canadian” and “Chi-
nese” here since nationality was not included in HateCheck
and therefore not covered in the previous section.Emotion Example Gold Label
disgust “<group> are known for being trash .”
“It’s reasonable that <group> are feeling vindictive .”abusive
non-abusive
anger “All <group> are abhorrent .”
“<group> were infuriated by the injustice. ”abusive
non-abusive
sadness “<group> are known for being pathetic .”
“<group> were crushed by the injustice.”abusive
non-abusive
fear “<group> are just so scary .”
“<group> are threatened by these policies.”abusive
non-abusive
Table 3: Challenge set created for emotion categories. In each sentence, we use a word from the corresponding
category of the NRC Emotion Intensity Lexicon (NRC-EIL) (Mohammad, 2018), shown in bolded font.
TCAV dir TCAV mag
classifier disgust anger sadness fear disgust anger sadness fear
Civil 0.48(0.37) 0.26(0.27) 0.31(0.31) 0.19(0.27) 0.05(0.06) 0.02(0.03) 0.02(0.03) 0.01(0.03)
Jigsaw 0.98(0.09) 0.93(0.2) 0.91(0.2) 0.95(0.18) 0.08(0.03) 0.05(0.03) 0.03(0.02) 0.05(0.03)
TweetEval 1(0) 1(0) 1(0) 1(0) 0.20(0.04) 0.17(0.04) 0.11(0.03) 0.13(0.03)
Table 4: Mean and standard deviation of concept-based metrics for four negative emotion concepts. Scores that are
significantly different from random concepts are in boldface.
Figure 5: Accuracy vs threshold for the disgust category
of the challenge set.
the label is also significantly high. Jigsaw is the
classifier that has learned the dependency between
negative emotions and the label of abuse and there-
fore is sensitive to it (as indicated by TCAV dir),
but the magnitude of the influence of concept on
the label is not significantly high, and the concept
is interpreted in the larger context. Interestingly,
the magnitude of the influence of disgust andanger
is higher than fear andsadness for all classifiers,
stating a higher association of disgust andanger
with abusive language. These results are in line
with conclusions drawn from assessing global suf-
ficiency with a challenge set.
6 Related Works
Most of the explainability works in NLP focus on
feature importance methods to measure the impor-
tance of an input feature for the prediction at thelocal level (Bahdanau et al., 2015; Sundararajan
et al., 2017; Ribeiro et al., 2016; Lundberg and
Lee, 2017b). However, recent works highlight that
models should be assessed beyond feature impor-
tance criteria and that the reasoning behind the
model’s decisions should be investigated through
explainability methods. Some examples of such ex-
plainability methods include counterfactual reason-
ing (Wu et al., 2021; Kaushik et al., 2021; Ribeiro
et al., 2020; Ross et al., 2020) or necessity and suf-
ficiency metrics (Balkır et al., 2022; Joshi et al.,
2022). Also, there is a need to compare various
classifiers at the global level. Although local expla-
nations can be aggregated to generate global expla-
nations, they are usually obtained through costly
interventions and are not practical to be applied
on a large scale. For global explanations, a pop-
ular approach is to train probing classifiers (Con-
neau et al., 2018). However, probes only identify
whether a classifier has learned a feature but stay
silent about whether the feature is used in predic-
tions (Belinkov, 2022; Tenney et al., 2019; Rogers
et al., 2020). Amnesic probing is an extension of
probing classifiers that identifies whether remov-
ing a feature influences the model’s predictions,
which relates to the notion of the global necessity
of a human-understandable concept for a predic-
tion (Ravfogel et al., 2020; Elazar et al., 2021).
Our work, on the other hand, focuses on the global
sufficiency of concepts. While probing classifiers
are applied to linguistic properties such as POSFalse _Suff
classifier disgust anger sadness fear
Civil 0.13 0.35 0.19 0.25
Jigsaw 0.08 0.28 0.14 0.22
TweetEval 0.36 0.36 0.35 0.35
Table 5: The global sufficiency of emotion categories
learned by classifiers with respect to the challenge set
described in Table 3.
tagging, which are necessary for accurate language
processing, we focus on human-defined semantic
concepts that are known to be important for the
label and test if they have been falsely learned as a
sufficient cause for the label.
Concept-based explanations have been intro-
duced in computer vision and are mostly used to
explain image classification models (Graziani et al.,
2018; Ghorbani et al., 2019; Yeh et al., 2020). In
NLP, concept-based explanations were used to mea-
sure the sensitivity of an abusive language classifier
to the emerging concept of COVID-related anti-
Asian hate speech (Nejadgholi et al., 2022b), to
assess the fairness of abusive language classifiers
in using the concept of sentiment (Nejadgholi et al.,
2022a), and to explain a text classifier with refer-
ence to the concepts identified through topic mod-
elling (Yeh et al., 2020). To the best of our knowl-
edge, our work is the first that uses concept-based
explanations to assess the sufficiency of human-
defined concepts in text classification.
7 Conclusion
Concept-based explanations can assess the influ-
ence of a concept on a model’s predictions. We
used two metrics based on the TCA V method: the
TCA V direction score identifies whether the classi-
fier has learned an association between a concept
and a label, and the TCA V magnitude score mea-
sures the extent of the influence of the concept
on the label. We showed that the best-performing
abusive language classifiers learned that negative
emotion is associated with abuse (positive direc-
tion) but did not over-rely on this concept (low
magnitude); that is, they did not overestimate the
global sufficiency of that concept.
Our method can potentially be used for other
NLP classification tasks. This approach is suitable
for tasks where certain concepts are closely related
to the label, but not enough to make a definitive
determination. For example, in sentiment analysis,
the price of products may have a strong connec-
tion to negative sentiment, but is insufficient todetermine it. Further research should explore how
concept-based explanations can help identify cases
where certain concepts are relied upon too heavily
in abusive language detection or other NLP classi-
fication tasks.
8 Limitations
Our work has limitations. First, we use the TCA V
framework, which assumes that concepts are en-
coded in the linear space of semantic representa-
tions. However, recent works show that in some
cases, linear discriminants are not enough to de-
fine the semantic representations of concepts in the
embedding spaces (Koh et al., 2020). Future work
should consider nonlinear discriminants to accu-
rately represent concepts in the hidden layers of
NLP neural networks.
In this study, we used simple challenge sets to
obtain a baseline for assessing the effectiveness
of concept-based explanations in measuring false
global sufficiency. Future work should focus on cu-
rating challenge sets by annotating user-generated
data for the label and the concepts, in order to
achieve a stronger baseline.
Our work is limited to pre-defined concepts and
requires human input to define the concepts with
examples. However, defining concepts in TCA V is
less restrictive than pre-defining features in other
explainability methods, in that concepts are abstract
ideas that can be defined without requiring in-depth
knowledge of the model’s inner workings or the
specific features it is using. This allows for a more
flexible approach where users can test the model
regarding their concept of interest.
Our method can only be applied to concepts that
are known to be important for the classifier and
are prone to being over-represented in training sets.
It’s important to check this condition independently
before using our metrics. In cases where this con-
dition does not hold true, the metrics we use in our
work may be interpreted differently and may not be
reliable indicators of global sufficiency. Also, we
only considered two variations of emotion-related
concepts. Other variations such as expression of
negative emotions by the writer of the post should
be investigated in future work.
Further, our metrics are limited to cases where
different classifiers are being compared since the
most important information is in the relative value
of the metrics. Our metrics should not be used as
absolute scores for testing a classifier.Testing a classifier for false causal relationships
is most valuable for detecting the potential flaws
of the models. If our metrics do not reveal a false
relationship between the concept and the label, that
should not be interpreted as an indicator of a flaw-
less model.
Ethical Statement
As with most AI technology, this approach can
be used adversely to exploit the system’s vulnera-
bilities and produce toxic texts that would be un-
detectable by the studied classifier. Specifically,
for methods that require access to the model’s in-
ner layers, care should be taken so that only trusted
parties could gain such access. The obtained knowl-
edge should only be used for model transparency
purposes, and the security concerns should be ade-
quately addressed.
Regarding environmental concerns, contempo-
rary NLP systems based on pre-trained large lan-
guage models, such as RoBERTa, require signifi-
cant computational resources to train and fine-tune.
Larger training datasets, used for fine-tuning, usu-
ally result in better classification performance but
also an even higher computational cost. To lower
the cost of this study and its negative impact on
the environment, we chose to use existing, publicly
available classification models.
