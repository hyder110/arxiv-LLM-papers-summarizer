Summary:
This paper explores techniques for estimating the necessary training sample size to achieve a targeted model performance in natural language understanding tasks. The authors propose an ensemble of non-linear functions to model the learning curve and predict model performance using only a small portion of the data. The approach shows promising results with a small margin of mean absolute error (around 0.9%) when predicting model performance. The study is conducted on four language understanding benchmarks and considers different types of classification tasks. The authors also investigate the effect of sample size, types of non-linear functions, and data weighting on the accuracy of the learning curve model.

Bullet Points:
1. Estimating the necessary training sample size is crucial for reducing annotation budgets in natural language understanding tasks.
2. The authors propose an ensemble of non-linear functions to predict model performance based on a small amount of training samples.
3. The approach shows a small margin of mean absolute error (around 0.9%) when predicting model performance.
4. The study is conducted on four language understanding benchmarks, including binary and multi-class classification tasks.
5. The proposed method is not confined to specific classification types or sample sizes.
6. Having more samples does not necessarily improve the accuracy of the learning curve model.
7. The choice of non-linear function affects the accuracy of the learning curve model, with the ensemble function performing best in most cases.
8. Weighting data points at a later phase of the learning curve improves the accuracy of the model.
9. The study does not explore sampling techniques other than random sampling.
10. Future work should investigate the influence of model architecture and explore more complex natural language understanding tasks.

Keywords:
- Sample size determination
- Natural language understanding
- Learning curve modeling
- Model performance prediction
- Ensemble of non-linear functions
- Classification tasks
- Data weighting
- Random sampling
- Model architecture
- Complex tasks