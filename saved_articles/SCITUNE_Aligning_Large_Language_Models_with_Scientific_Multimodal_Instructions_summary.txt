Summary:
The paper introduces SciTune, a tuning framework that improves the ability of large language models (LLMs) to follow scientific multimodal instructions.
They use a human-generated scientific instruction tuning dataset to train a multimodal model called LLaMA-SciTune, which connects a vision encoder and LLM for science-focused visual and language understanding.
Compared to models finetuned with machine-generated data only, LLaMA-SciTune outperforms human performance on average and in many sub-categories on the ScienceQA benchmark.

Bullet points:
1. SciTune is a tuning framework that enhances the ability of large language models to understand scientific multimodal instructions.
2. The framework uses a human-generated scientific instruction tuning dataset for training.
3. LLaMA-SciTune is a multimodal model that connects a vision encoder and a large language model for science-focused understanding.
4. LLaMA-SciTune surpasses human performance on average and in many sub-categories on the ScienceQA benchmark.
5. Instruction finetuning is a popular paradigm in aligning large language models (LLMs) with human intent.
6. The idea of instruction finetuning for aligning LLMs with existing foundation models in scientific disciplines is less explored.
7. The community has shown interest in instruction tuning for enhancing the capabilities of LLMs.
8. LLaMA-SciTune demonstrates improved performance compared to models finetuned with machine-generated data only.
9. The framework improves the LLMs' ability to accurately balance desired outcomes, context, and human intent.
10. The paper presents a methodological approach to improving language understanding in the context of scientific multimodal instructions.

Keywords:
1. SciTune
2. large language models
3. scientific multimodal instructions
4. instruction tuning
5. LLaMA-SciTune
6. vision encoder
7. science-focused understanding
8. ScienceQA benchmark
9. human-generated data
10. machine-generated data