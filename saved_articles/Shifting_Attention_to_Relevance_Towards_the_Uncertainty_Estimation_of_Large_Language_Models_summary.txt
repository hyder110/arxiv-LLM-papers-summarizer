Summary:
The paper focuses on characterizing the uncertainty of model outputs generated by Large Language Models (LLMs).
The authors identify generative inequalities in LLMs, where certain tokens and sentences are more relevant and representative than others.
They propose a method called Shifting Attention to Relevance (SAR) to address these inequalities and improve uncertainty estimation.
Experiments conducted on popular LLMs demonstrate the superior performance of SAR.
The paper also highlights the vulnerability of LLMs to reliability issues such as hallucination and factual errors.
Uncertainty estimation is critical for Human-AI interaction applications, especially in scenarios where human behavior is affected by LLM outputs.
The paper explores token-level generative inequalities and the correlation between relevance and uncertainty proportions.
The relevance of tokens and sentences is measured by comparing semantic changes before and after removing them.
SAR is evaluated on various free-form question-answering tasks and shows significant improvements over previous works.
The contributions of the paper include disclosing the impact of generative inequalities on uncertainty estimation and mitigating these biases through SAR.
Keywords:
- Large Language Models (LLMs)
- uncertainty estimation
- generative inequalities
- Shifting Attention to Relevance (SAR)
- reliability issues
- token-level generative inequalities
- relevance
- semantic changes
- free-form question-answering tasks
- semantic equivalence sentences