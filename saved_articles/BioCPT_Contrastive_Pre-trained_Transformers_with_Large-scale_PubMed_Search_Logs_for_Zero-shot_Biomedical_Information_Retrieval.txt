    
    BioCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval  Qiao Jin, Won Kim, Qingyu Chen, Donald C. Comeau, Lana Yeganova, John Wilbur, Zhiyong Lu National Center for Biotechnology Information (NCBI), National Library of Medicine (NLM), National Institutes of Health (NIH)  Abstract Information retrieval (IR) is essential in biomedical knowledge acquisition and clinical decision support. While recent progress has shown that language model encoders perform better semantic retrieval, training such models requires abundant query-article annotations that are difficult to obtain in biomedicine. As a result, most biomedical IR systems only conduct lexical matching. In response, we introduce BioCPT, a first-of-its-kind Contrastively Pre-trained Transformer model for zero-shot biomedical IR. To train BioCPT, we collected an unprecedented scale of 255 million user click logs from PubMed. With such data, we use contrastive learning to train a pair of closely-integrated retriever and re-ranker. Experimental results show that BioCPT sets new state-of-the-art performance on five biomedical IR tasks, outperforming various baselines including much larger models such as GPT-3-sized cpt-text-XL. In addition, BioCPT also generates better biomedical article and sentence representations for semantic evaluations. As such, BioCPT can be readily applied to various real-world biomedical IR tasks. BioCPT API and code are publicly available at https://github.com/ncbi/BioCPT.  Introduction Information retrieval (IR) is an important step in biomedical knowledge discovery and clinical decision support1,2. With the objective of finding relevant information to a given query, IR covers various information-seeking tasks in biomedicine such as literature search3, question answering4, and recommending related articles and sentences5,6. These applications are essential for biologists and clinicians to find useful information from vast biomedical databases that have been rapidly growing in size7. However, most IR systems in biomedicine, such as the widely-used literature search engine PubMed, are keyword-based and can only find articles that contain the exact mentions of the query terms. Such a matching mechanism will unfortunately miss articles that are semantically relevant but have no lexical overlap with the input query. Moreover, lexical matching does not consider the contextualized meanings of words. For example, “lead” most likely     
    denotes the heavy metal or a wire in an implanted device in the query “lead heart damage”, but keyword-based search engines will match articles with “lead to heart damage” that do not serve the original information needs. Understanding the semantics of queries and documents, i.e., the meaning of “lead” in “lead heart damage”, is a valuable property of IR systems.  Recent progress in IR and deep learning research has shown that dense retrievers, which encode and match queries and documents in low-dimensional semantic space, can perform better retrieval than traditional lexical (sparse) retrievers such as BM258-11. Dense retrievers are typically based on pre-trained transformers12, and are further fine-tuned with task-specific data. However, dense retrieval models trained on general-purpose datasets cannot generalize well to domain-specific IR tasks13. To facilitate the development and evaluation of biomedical IR methods, several community-wide challenges have been introduced, such as TREC and BioASQ14-17. However, since their relevant documents require expert annotation, these datasets are limited in scale and diversity with only hundreds to thousands of queries, thus restricting the creation of generalizable models. In fact, the majority of existing approaches are task-specific18-20 and their generalizability to other tasks in the field is modest at best. As a result, there is a pressing need for language models that can perform well across various biomedical IR tasks.  In response, we propose Biomedical Contrastive Pre-trained Transformers (BioCPT), a first-of-its-kind transformer-based model that is trained with large-scale query-document pairs from PubMed search logs using contrastive learning. Moreover, BioCPT innovatively integrates a dense retriever, which consists of a query encoder and a document encoder for retrieving initial candidate documents, and a cross-encoder for precisely re-ranking the retrieved documents based on more fine-grained relevance predictions. While a recent study suggested that the training of re-ranker should be dependent on the retriever21, previous dense retrievers and re-rankers are separately developed as the latter is typically trained on BM25 retrieval results13,22, leading to discrepancy between the two modules and suboptimal performance. To the best of our knowledge, we are the first biomedical IR study to train the two models together in one framework.   For training BioCPT, we collected data from PubMed search logs on an unprecedented scale with over 255 million query-document pairs from a period of three years.  To demonstrate that BioCPT is robust and effective at various biomedical IR tasks without requiring task-specific fine-tuning, we evaluate BioCPT with a comprehensive set of nine tasks in three categories, all in zero-shot settings: (a) document retrieval where five     
    biomedical IR tasks in the BEIR benchmark13 are used. (b) Document semantic representation where two benchmarks are used. (c) Finding similar sentences on two datasets.   Our experimental results show that BioCPT outperforms various strong baselines in (a), including sparse retrievers, transformer-based dense retrievers, and cross-encoder re-rankers. Notably, BioCPT (330 million parameters) is also generally better than much larger models, such as Google’s GTR-XXL (4.8 billion parameters, 14 times larger)23 and OpenAI’s cpt-text-XL (175 billion parameters, 530 times larger)24. For (b), we show that the BioCPT article encoder sets new state-of-the-art performance on the RELISH similar article dataset 25 and is comparable to state-of-the-art methods on the SciDocs benchmark26. For (c), we evaluate on the BIOSESS27 and MedSTS28 similar article datasets, and results show that the BioCPT query encoder performs the best or second best among compared methods.   BioCPT is the first transformer-based model in biomedical IR that integrates both retrieval and re-ranking by contrastive training with unprecedentedly large-scale PubMed search logs. BioCPT generalizes well to a set of biomedical IR tasks without requiring any task-specific training data. As a result, BioCPT can be directly applied to various biomedical applications such as searching relevant information, retrieving similar sentences, recommending related articles, and providing retrieval augmentation for large language models. To facilitate its use, we make the source code of BioCPT publicly available at https://github.com/ncbi/BioCPT. In addition, we will make the pre-trained models available through Web APIs upon publication, providing interested users access to cutting-edge biomedical text encoding capabilities.  Results In this section, we first describe the collected query-article pair dataset from PubMed search logs for training the BioCPT model, which is detailed in the Methods section. We then present the evaluation results of BioCPT for three types of IR capabilities as shown in Figure 1: (1) The training objective and the main usage scenario for BioCPT is to retrieve relevant articles for a given biomedical query or question. We use the BEIR benchmark13 to evaluate such an ability; (2) we use the RELISH dataset25 and the SciDocs benchmark26 to evaluate article semantic representations generated by the BioCPT article encoder; (3) we use the BIOSSES27 and the MedSTS28 datasets to evaluate sentence semantic representations generated by the BioCPT query encoder. All evaluations are conducted under zero-shot settings.       
     Figure 1. A high-level overview of this work. BioCPT is contrastively trained with PubMed search logs. We evaluate BioCPT on various biomedical information retrieval tasks under zero-shot settings, including article retrieval, sentence, and article similarity.  Collecting the largest-scale query-article pairs from PubMed search logs for BioCPT training In this work, we utilize billions of user interactions with PubMed to collect such query-article relevance signals29. Following the click-through model commonly used in IR research30, we assume that user-clicked articles are relevant to the input query, and automatically collected an unprecedent scale of 255M relevant query-article pairs from three years’ worth of PubMed search logs. Because most of the queries sent to PubMed are keywords, the clicked articles often contain exact matches of the whole input queries31. We also created a subset of about 18M query-article pairs where the exact queries are not directly mentioned in the clicked articles (e.g., “what are the impacts of diabetes on CNS”) to better model the semantic matching.   BioCPT overall architecture BioCPT includes a first-stage retriever and a second-stage re-ranker, shown in Figure 1. The BioCPT retriever is trained with 255M unfiltered query-article pairs, while the BioCPT re-ranker is trained with 18M non-keyword query-article pairs. The training details of BioCPT are described in the Methods section. The BioCPT retriever, including a query encoder (QEnc in Figure 1) and a document encoder (DEnc in Figure 1), can efficiently retrieve thousands of relevant candidates from millions of articles in the document collection. The BioCPT re-ranker is a cross-encoder (CrossEnc in Figure 1) that can further sort the retrieved articles by fine-grained relevance and generate the final article ranking.  
    
    BioCPT achieves state-of-the-art performance on biomedical IR tasks Benchmarking-IR (BEIR)13 is a standardized evaluation benchmark for zero-shot information retrieval systems. BEIR contains 18 datasets covering various domains such as biomedicine and news. In this study, we evaluate BioCPT with all five biomedical tasks in the BEIR benchmark: TREC-COVID17, NFCorpus32, BioASQ14, SciFact33, and SciDocs26. TREC-COVID17 contains questions about the COVID-19 pandemic and uses the CORD-19 corpus34 as the document collection for retrieval. NFCorpus32 collects natural language queries and relevant articles from the NutritionFacts.org site. BioASQ14 is a community challenge for biomedical question answering, where the task used in BEIR is to retrieve relevant articles from PubMed for a given question. SciFact33 is a scientific claim verification dataset, which contains a retrieval subtask and a veracity prediction subtask. BEIR uses the retrieval subtask of SciFact, where the objective is to find relevant articles that can be used to verify a given claim.  SciDocs26 is a benchmark for evaluating scientific article representation models. BEIR uses its citation prediction subtask, where the goal is to retrieve relevant citations for a given article.   We compared BioCPT with a wide range of other IR models, including its initialization model PubMedBERT35, BM2536 and BM25 with the MiniLM re-ranker37, sparse retrievers such as DeepCT38, SPARTA39, and docT5query40, dense retrievers such as DPR8, ANCE41, TAS-B42, GenQ13, Contriever43, large language model generated embeddings such as Google’s Generalizable T5-based dense Retrievers (GTR)23 and OpenAI’s cpt-text 44. We use the official evaluation library for BEIR1 and report the normalized discounted cumulative gain at rank 10 (NDCG@10) for different models.  Table 1 shows the evaluation results of BioCPT on the BEIR benchmark in comparison to the compared approaches. The evaluation is conducted in a zero-shot setting, where no task-specific training data is used for fine-tuning BioCPT. First, BioCPT improves its initialization model PubMedBERT by huge margins – where the latter basically fails on all the retrieval tasks. BioCPT also surpasses all compared sparse (DeepCT, SPARTA, docT5query), dense (DPR, ANCE, TAS-B, GenQ, Contriever), and late-interaction (ColBERT) retrievers on all of the compared tasks.         1 https://github.com/beir-cellar/beir     
     Method Size TREC-COVID NFCorpus BioASQ SciFact SciDocs Avg. Sparse retrievers BM2536 - 0.656 0.325 0.465 0.665 0.158 0.454 BM25 + MiniLM37 66M 0.757 0.350 0.523 0.688 0.166 0.497 DeepCT38 110M 0.406 0.283 0.407 0.630 0.124 0.370 SPARTA39 110M 0.538 0.301 0.351 0.582 0.126 0.380 docT5query40 220M 0.713 0.328 0.431 0.675 0.162 0.462 Dense retrievers DPR8 110M 0.332 0.189 0.127 0.318 0.077 0.209 ANCE41 110M 0.654 0.237 0.306 0.507 0.122 0.365 TAS-B42 66M 0.481 0.319 0.383 0.643 0.149 0.395 GenQ13 220M 0.619 0.319 0.398 0.644 0.143 0.425 Contriever43 110M 0.596 0.328 - 0.677 0.165 - Contriever + MiniLM43 176M 0.701 0.344 - 0.692 0.171 - ColBERT9  110M 0.677 0.305 0.474 0.671 0.145 0.454 Large language model retrievers Google GTR-Base23 110M 0.539 0.308 0.271 0.600 0.149 0.373 Google GTR-Large23 335M 0.557 0.329 0.320 0.639 0.158 0.401 Google GTR-XL23 1.24B 0.584 0.343 0.317 0.635 0.159 0.408 Google GTR-XXL23 4.80B 0.501 0.342 0.324 0.662 0.161 0.398 OpenAI cpt-text-S44 300M 0.679 0.332 - 0.672 - - OpenAI cpt-text-M44 1.20B 0.585 0.367 - 0.704 - - OpenAI cpt-text-L44 6.00B 0.562 0.380 - 0.744 - - OpenAI cpt-text-XL44 175B 0.649 0.407 - 0.754 - - BioCPT BioCPT 330M 0.709 0.355 0.553 0.761 0.172 0.510 BioCPT (retriever only) 220M 0.697 0.340 0.332 0.724 0.123 0.443 BioCPT w/o pre-training (PubMedBERT35) 110M 0.059 0.015 - 0.010 0.004 - Table 1. Zero-shot performance of BioCPT on biomedical subtasks of the BEIR benchmark. Bolded numbers, underlined, and italicized numbers denote the highest, 2nd highest, and 3rd highest, respectively.      
     As shown in the BEIR paper13, BM25 is a strong baseline that is generalizable to biomedical IR tasks. When combined with a cross-encoder re-ranker, BM25 performs the best in 15 out of 18 tasks in the original benchmark. Notably, BioCPT is still better than BM25 + cross-encoder in 4/5 of the biomedical tasks in BEIR, showing its effectiveness at retrieving relevant articles for biomedical queries. BM25 with re-ranker is only better than BioCPT on the TREC-COVID dataset, which might be due to the fact that most of the top-10 articles from BM25 are annotated while many top-10 articles from dense retrievers are not annotated and considered as irrelevant13.  We further compare BioCPT with more recent large dual retriever models, represented by Google’s GTR and OpenAI’s cpt-text, both of which have model sizes ranging from millions to billions of parameters. BioCPT is able to outperform all sizes of the GTR model. While the GPT-345 sized (175B) cpt-text-XL is better than BioCPT on NFCorpus, BioCPT outperforms cpt-text-XL on TREC-COVID and SciFact despite being about 500 times smaller. This indicates that small models trained on domain-specific datasets can still have better in-domain zero-shot performance than much larger general domain retrievers.  BioCPT generates better biomedical article representations We evaluate the BioCPT article encoder on the RELISH article similarity task25. RELISH is a large-scale expert-annotated dataset that contains 196k article-article relevance annotations for 3,278 query articles. Following the dataset split and evaluation settings of46, we use article embeddings generated by the BioCPT article encoder to calculate the article-pair similarity and evaluate ranking quality by mean average precision (MAP) and NDCG at 5, 10, and 15. For comparison, we also list the model performance reported in46, including a random baseline, term-based retrievers such as BM2536 and PubMed Related Articles (PMRA)5, embedding-based retrievers such as fastText47, BioWordVec48, InferSent49, Sent2vec50, BioSentVec51, document embedding models such as LDA52 and doc2vec53, and BERT-based retrievers such as BioBERT54, PubMedBERT35, SPECTER26, and SciNCL55.  Table 2 shows the evaluation results on the RELISH dataset. While not explicitly trained with article similarity data, the BioCPT article encoder (DEnc) outperforms all other models, including SPECTER and SciNCL that are specifically trained with article-article citation information. Compared to its base PubMedBERT model, the BioCPT article encoder improves by over 10% relative performance.  We also evaluate the BioCPT article encoder on the SciDocs26 benchmark, which contains all kinds of scientific articles from biomedicine to engineering. The BioCPT article     
    encoder achieves SOTA performance on the MeSH prediction subtask and is comparable to SOTA methods on the overall score (Table S1). Experimental details are shown in Appendix A in the Supplementary materials.  Method MAP@5 MAP@10 MAP@15 NDCG@5 NDCG@10 NDCG@15 Avg. Random 79.33 77.22 75.41 80.70 77.67 76.40 77.79 Sparse retrievers BM2536 88.91 86.72 84.54 89.48 87.39 86.21 87.21 PMRA5 90.30 87.57 85.75 90.95 88.40 87.45 88.40 Non-BERT embedding-based models fastText47 85.75 82.81 81.79 86.79 83.79 83.12 84.01 BioWordVec48 89.84 86.51 84.67 89.90 86.67 85.53 87.19 InferSent49 85.21 82.16 80.41 86.56 83.31 82.35 83.33 WikiSentVec50 87.92 85.23 83.40 88.65 85.74 84.81 85.96 BioSentVec51 90.76 88.10 86.16 90.05 87.76 86.89 88.29 LDA52 85.44 82.66 80.36 86.51 82.91 81.31 83.20 Doc2Vec53 86.23 84.74 83.39 86.55 84.70 84.09 84.95 BERT-based models BioBERT54 88.14 85.81 83.90 88.97 86.29 85.10 86.37 PubMedBERT35 83.69 81.07 79.53 85.47 82.39 81.41 82.26 SPECTER26 92.27 90.00 88.36 91.47 89.12 88.42 89.94 SciNCL55 94.72 92.74 91.14 93.67 91.91 90.94 92.52 BioCPT DEnc 95.58 93.99 92.39 94.78 93.12 92.43 93.72 Table 2. Evaluation results of the BioCPT article encoder on the RELISH dataset. Bolded numbers, underlined, and italicized numbers denote the highest, 2nd highest, and 3rd highest, respectively. All numbers are percentages.  BioCPT generates better biomedical sentence representations We evaluate the BioCPT query encoder on two datasets for sentence similarities: BIOSSES in the biomedical domain27 and MedSTS in the clinical domain28. The evaluation is conducted under the unsupervised (zero-shot) setting, where we directly apply the BioCPT query encoder model to test set instances without any model retraining or fine tuning. We follow the evaluation settings in51 and report Pearson’s correlation coefficients between the model predictions and the ground truth scores. For comparison, we include SOTA methods such as BioWordVec56, Universal Sentence Encoder (USE)57,     
    BioSentVec trained with different corpora51, and BERT-based models such as PubMedBERT35, Clinical BERT58, SPECTER26, SciNCL55.  Model BIOSSES MedSTS Non-BERT embedding-based models BioWordVec56 0.694 0.747 USE57 0.345 0.714 BioSentVec (PubMed)51 0.817 0.750 BioSentVec (MIMIC-III)51 0.350 0.759 BioSentVec (PubMed + MIMIC-III)51 0.795 0.767 BERT-based models PubMedBERT35 0.528 0.521 Clinical BERT 58 0.556 0.525 SPECTER26 0.694 0.702 SciNCL55 0.847 0.706 BioCPT QEnc 0.893 0.765 Table 3. Evaluation results (Pearson’s correlation coefficients) of the BioCPT query encoder on the BIOSSES and MedSTS datasets. Bolded numbers, underlined, and italicized numbers denote the highest, 2nd highest, and 3rd highest, respectively. All numbers are percentages.  Table 3 shows the evaluation results. On BIOSSES, BioCPT performs the best among all compared models, surpassing the second SciNCL by 5% relative performance (0.893 vs. 0.847). On the MedSTS dataset, BioCPT ranks the 2nd and the performance is comparable to the highest-ranking model BioSentVec (0.765 vs. 0.767), which uses an external clinical corpus MIMIC-III59 for its model training. Overall, our results show that the BioCPT query encoder can effectively encode biomedical and clinical sentences that reflect their semantic similarities, despite not being explicitly trained on sentence similarity data or clinical texts.  Discussions Web search logs have been used to build large-scale datasets and enable the development of deep learning methods for IR. In the general domain, the MS MARCO dataset22 contains about one million anonymized questions sampled from Bing’s search query logs, which are annotated with relevant passages and documents. However, IR models trained on general-purpose datasets cannot generalize well to biomedical domain-specific tasks13. In the biomedical domain, the TripClick dataset60 contains about     
    700 thousand queries submitted to the Trip Database, and collects query-document relevance signals from about 1.3 million user clicks. Our dataset is not only about 200 times larger, but also contains a subset of 18 million pairs to better train the semantic matching ability for the BioCPT re-ranker.   It should be noted that the BioCPT model is only trained with query-article relevance data derived from PubMed user logs. While not being explicitly trained with query-query similarity and article-article similarity data, the BioCPT query encoder and article encoder still achieve the SOTA performance on sentence similarity and article similarity tasks. This shows that the contrastive objective can train not only a dense retriever, but can also train the individual query and document encoders to perform tasks related to information-seeking behaviors. As such, BioCPT has broad implications in a variety of real-world scenarios: query-to-article retrieval, article-to-article retrieval, and sentence-to-sentence retrieval.  For query-to-article retrieval, one potential application of the proposed BioCPT model is to enhance existing ranking algorithms for biomedical literature search. For this, we have performed three empirical case studies with semantic queries such as “lead heart injury”, and compared the retrieved articles of BioCPT with other keyword-based search engines such as PubMed and Google Scholar (described in Appendix B). Our case studies show that articles retrieved from BioCPT are more semantically relevant to the input queries. Therefore, we plan to investigate the possibility of including the BioCPT relevance score into PubMed, both at the retrieval stage for rescuing the zero-hit queries29 and at the Best Match re-ranking stage61 as a semantic matching feature. For article-to-article retrieval, BioCPT can be used to improve similar article recommendation algorithms, a popular and desired feature in literature search5. BioCPT can also be used for sentence-to-sentence retrieval tasks, including sentence-level literature search6 and frequently asked question (FAQ) retrieval for entailment-based question answering62.  In addition, BioCPT can provide retrieval-augmentation for the recent large language models (LLMs) such as GPT-3 and PaLM45,63. Although they have shown significant performance improvements over many NLP tasks, they can still generate plausible-sounding but incorrect content (“hallucinations”) because there is no intrinsic mechanism for LLMs to “consult” with any source of truth64,65. One possible solution for the hallucination issue in biomedicine is to first retrieve relevant documents with BioCPT for the given input, and then prompt LLMs to summarize the retrieved content66,67.   This study has several limitations. First, we only use in-batch negatives for training the BioCPT retriever. Previous studies have suggested that using harder negative     
    instances8,41 can improve the retrieval performance, especially at lower ranks. Using exposed but unclicked articles in the PubMed logs as hard negatives remains an interesting future direction. Second, we only consider the text modality, while biomedical data has various other modalities such as images and sequences. It is also worth exploring multi-modal retriever training with user log data from various biomedical databases.   To summarize, we use unprecedented large-scale PubMed user logs to contrastively train BioCPT, the first integral retriever-reranker model for biomedical information retrieval. Systematic zero-shot evaluations show that BioCPT achieves the highest performance for five different biomedical information retrieval tasks, including query-to-article retrieval, semantic article representation, and semantic sentence representation. We anticipate that BioCPT will have a broad range of applications and significantly enhance access to biomedical information, making it a valuable and widely used tool for researchers and practitioners alike.  Methods Query-article relevance data collection from PubMed search logs To determine the relevance of articles in PubMed, we utilized anonymous user queries and analyzed subsequent clicks on the returned articles. We made the assumption that user actions serve as an approximation for relevance. However, please note that due to NIH/NLM/NCBI security and privacy policy (https://www.nlm.nih.gov/privacy.html), we are unable to share the specific log data. Nonetheless, we provide a comprehensive description of how the data was generated and present its overall statistics in the detailed section below.   We collected query-article relevance data in PubMed search logs over a period of three years (from 2020-01-01 to 2022-12-31) for our BioCPT model training. Supplementary Figure S1 shows a flowchart of the data collection process. The raw logs contain 167,831,160 unique queries and 23,736,605 unique PubMed articles. We first filtered the navigational queries like author and journal title searches with Field Sensor68. We also removed the articles that do not have a title or an abstract in our PubMed database, which are mostly editorial articles or articles in foreign languages. After filtering, there are 87,097,440 informational queries and 17,670,622 articles.  On average, each query is associated with 2.93 clicked articles. Based on the user click information, we generated 255,308,082 relevant query-article pairs to train the BioCPT retriever.       
    To better model the semantic matching when training the BioCPT reranker, we further filtered out 79,323,164 keyword queries (91%) from the informational query set, which are defined as either (1) having only one word or (2) all of the clicked articles containing exact mentions of the entire input query as a long phrase. In the end, there are 7,774,276 non-keyword queries and 5,275,352 articles, from which we generated 18,395,691 relevant query-article pairs as the training data for the BioCPT re-ranker.   BioCPT In this section, we first introduce the training of the BioCPT retriever and re-ranker (shown in Figure 2). Then, we describe how to apply the trained BioCPT to downstream literature retrieval tasks (shown in Figure 2). Finally, we will describe the specific configurations of BioCPT.  
 Figure 2. Overview of the BioCPT training process. (A) Training the BioCPT query encoder and document encoder with query-document pairs and in-batch negatives; (B) Training the BioCPT cross-encoder with non-keyword-query-article pairs and local negatives from the BioCPT retriever. Models in dashed and solid lines denote un-trained and pre-trained, respectively. MIPS: maximum inner product search.  BioCPT retriever training The BioCPT retriever contains QEnc and DEnc, both of which are 12-layer Transformer (Trm) encoders. The encoder weights are initialized with PubMedBERT35, a biomedical domain-specific version of BERT. Following other dense retrievers8,41, the BioCPT 
    
    retriever models the relevance between a query 𝑞 and a document 𝑑 by the dot product of their representations. Specifically, we first feed 𝑞 and 𝑑 to QEnc and DEnc, respectively. We use the last [CLS] hidden states as their representations 𝐸(𝑞) and 𝐸(𝑑): 𝐸(𝑞)=QEnc(𝑞)=Trm([CLS]	𝑞	[SEP])["#$]∈ℝ& 𝐸(𝑑)=DEnc(𝑑)=Trm([CLS]	𝑑'(')*	[SEP]	𝑑+,-'.+/'	[SEP])["#$]∈ℝ& where [CLS] and [SEP] are the special tokens used in BERT. ℎ is the hidden dimension of the BioCPT model, which is also the dimension of the generated vector representations. 𝑑'(')* and 	𝑑+,-'.+/' denote the title and abstract of the given document. Then, the relevance between 𝑞 and 𝑑 is calculated as: 𝑅𝑒𝑙(𝑞,𝑑)=𝐸(𝑞)0𝐸(𝑑)∈ℝ  For training the BioCPT retriever, each instance has a query 𝑞, a clicked document 𝑑, and the number of clicks 𝑐 recorded in our logs. During training, each mini-batch contains a list of 𝐵 instances, which we denote as [𝑞1,𝑑1,𝑐1]123|5| and |𝐵| is the batch size. For the training objective, we use a contrastive loss with in-batch negatives8,44.  Specifically, we first generate all 𝐸(𝑞1) and 𝐸(𝑑1) from QEnc and DEnc, respectively. 𝑑1 is a relevant document for 𝑞1, and we assume that the (|𝐵|−1) other documents [𝑑6	|	𝑗≠𝑖] in the mini-batch are irrelevant documents for 𝑞1. Similarly, we also consider the (|𝐵|−1) other queries [𝑞6	|	𝑗≠𝑖] in the mini-batch as irrelevant queries for 𝑑1. For the training instance indexed by 𝑖, we calculate its query-to-document loss ℒ1789and document-to-query loss  ℒ1987 by: ℒ1789=−log	(expM𝐸(𝑞1)0𝐸(𝑑1)N∑expP𝐸(𝑞1)0𝐸M𝑑6NQ|5|623) and ℒ1987=−log	(expM𝐸(𝑞1)0𝐸(𝑑1)N∑expR𝐸M𝑞6N0𝐸(𝑑1)S|5|623) where ℒ1789 and ℒ1987 are basically negative log likelihood losses for the in-batch query-to-document and document-to-query matching distributions, respectively. The distributions are modeled by the softmax function. We further weight the loss of instances by their clicks, where query-document pairs with more clicks will have higher weights. We then calculate the batch-level ℒ5789 and ℒ5987 by: ℒ5789=T𝑤1ℒ1789|5|123 and     
    ℒ5987=T𝑤1ℒ1987|5|123 where 𝑤1=log8(𝑐1+1)∑log8(𝑐6+1)|5|623  The final loss ℒ5of the mini-batch is a weighted sum of the two losses: ℒ5=𝛼ℒ5789+(1−𝛼)ℒ5987 where 𝛼 is a tunable hyper-parameter for loss combination. During training, ℒ5 is optimized by gradient-based methods.  BioCPT re-ranker training The BioCPT re-ranker is a cross-encoder, denoted as CrossEnc. Similar to QEnc and DEnc, CrossEnc is a 12-layer Transformer (Trm) encoder initialized with PubMedBERT. Unlike the BioCPT retriever, the BioCPT re-ranker predicts the relevance between a query 𝑞 and a document 𝑑 by passing them into a single CrossEnc. Specifically,  𝑅𝑒𝑙(𝑞,𝑑)=CrossEnc(𝑞,𝑑)=𝑊0	Trm([CLS]	𝑞	[SEP]	𝑑	[SEP])["#$]+𝑏∈ℝ where 𝑊∈ℝ& and 𝑏∈ℝ are trainable parameters.  For training the BioCPT re-ranker, each instance has a query 𝑞1, a clicked document 𝑑1:, and a list of 𝑀 non-relevant (i.e., not clicked) documents [𝑑16;	\	𝑗=1,2,3,…,𝑀}. Following21, we use local negatives to train the BioCPT re-ranker instead of in-batch negatives. Specifically, the non-relevant documents are randomly sampled from rank 𝑒 to rank 𝑓 in the top retrieved documents by the trained BioCPT retriever through a maximum inner product search (MIPS). The loss ℒ1 for the instance is a negative log-likelihood loss: ℒ1=−log	(expMCrossEnc(𝑞1,𝑑1:)NexpPCrossEnc(𝑞1,𝑑1:)Q+∑expPCrossEncM𝑞1,𝑑16;NQ<623)  Similarly, we take a weighted sum of the instance-level loss in a mini-batch, where the weights are computed with the click count. ℒ5=T𝑤1ℒ1|5|123 and 𝑤1=log8(𝑐1+1)∑log8(𝑐6+1)|5|623      
    During training, ℒ5 is optimized by gradient-based methods.  BioCPT inference As shown in Figure 3, when applying the pre-trained BioCPT to downstream tasks in a zero-short fashion, we need to first encode the task corpus. Specifically, we use DEnc to process each article 𝑑1 in the task corpus, getting their representations 𝐸(𝑑1)∈ℝ&. We save the representations of the entire corpus, denoted as 𝒟=[𝐸(𝑑3),𝐸(𝑑8),…,𝐸(𝑑=)]∈ℝ=×&. 𝑁 is the size of articles in the corpus. This step only needs to be done once in the offline setting.  
 Figure 3. Architecture for inference with pre-trained BioCPT. A test query 𝑞 is fed to the BioCPT query encoder, and its representation will be matched against the corpus representations using MIPS. The corpus representations are calculated and saved off-line. Top 𝐾 retrieved articles will be re-ranked by the BioCPT cross-encoder. MIPS: maximum inner product search.   Then, each input query 𝑞 is fed into QEnc, which generates its representative 𝐸(𝑞). We conduct a MIPS between 𝐸(𝑞) and 𝒟 to find the top-𝐾most similar articles to the input query: 𝑑37,𝑑87,…,𝑑?7=MIPS(𝐸(𝑞),𝒟)  We apply CrossEnc to score the relevance between 𝑞 and each relevant article candidate in 𝑑37,𝑑87,…,𝑑?7 retrieved from the previous step: 𝑅𝑒𝑙M𝑞,𝑑17N=CrossEnc(𝑞,𝑑17) 
    
     Finally, we sort the retrieved articles by 𝑅𝑒𝑙M𝑞,𝑑17N from the highest to the lowest and return the sorted results.  BioCPT configurations We implemented the BioCPT using PyTorch69 and the Hugging Face transformers library70. The hidden dimension for BioCPT ℎ=768 as in the BERT-base configuration. We use the Adam optimizer71 without weight decay to train both the retriever and the re-ranker, where we set the learning rate 2e-5 and epsilon 1e-8. For the BioCPT retriever, 𝐵=32 and 𝛼=0.8, and we also apply gradient accumulation of 8 steps. We train the retriever for 100k steps with 10k warm-up steps. For the BioCPT re-ranker: 𝑀=31, 𝑒=50, 𝑓=200. We train the re-ranker for 10k steps with 1,000 warm-up steps. We apply cosine learning rate schedule after the warm-up steps. During inference, 𝑁 and 𝐾 vary for specific tasks. We implemented MIPS with the FlatIP index of the Faiss library72.  Compared methods Sparse retrievers Sparse retrievers, as known as lexical retrievers, match the queries and documents with overlapped terms. BM2536, a widely used lexical retriever, represents queries and documents as bag-of-words, and scores the relevance based on term frequency and inverse document frequency. DeepCT38 uses contextualized term weights predicted by BERT. SPARTA39 pre-computes contextualized matching weights for each possible term and the document, resulting in a sparse vector that has the size dimension as the BERT vocabulary. DocT5query40  performs document expansion with T5-generated73 queries for the document. The PubMed Related Articles algorithm5 uses a probabilistic topic-based model to compute the content similarity between two given articles.  Dense retrievers Dense retrievers first encode both the queries and documents into low-dimensional dense vectors, and then perform nearest neighbor search to find the relevant documents for a given query. Depending on the encoder architecture, we broadly classify them into non-BERT embedding models and BERT-based dense retrievers.   Non-BERT embedding models include BioWordVec48 which is a biomedical version of word2vec74, FastText 47, a linear model on the N-gram features of the input texts, Sent2vec50 and its biomedical version BioSentVec51, the LDA topic model52, doc2vec53, InferSent49, and Universal Sentence Encoder (USE)57. BERT-based dense retrievers: DPR8 is a bi-encoder retriever trained by in-batch negatives and BM25 hard negatives.     
    ANCE41 improves the DPR training by using hard negatives from an approximate nearest neighbor index of the corpus. TAS-B42 is a bi-encoder retriever distilled from a cross-encoder and ColBERT9 with balanced topic aware sampling. GenQ13 is domain-adaptation method that trains a dense retriever with synthetic query-document pairs generated by T573. Contriever43 is a contrastively pre-trained dense retriever in the general domain with carefully engineered positive and negative query-document pairs. ColBERT9 is a late-interaction retriever that computes and matches the contextualized representations of each token in the query and document. In addition, we compare BioCPT with several off-the-shelf BERT models that are biomedical domain-specific, including BioCPT’s initialization model PubMedBERT35, BioBERT54, SPECTER26, and SciNCL55.  Large language model retrievers We also compare BioCPT with two large language model retrievers: Google’s GTR23 and OpenAI’s cpt-text24. Unlike most dense retrievers that are based on the BERT-base model of 110M parameters, GTR and cpt-text use much larger language model encoders. Specifically, GTR is based on T573 and its largest variant has 4.8B parameters, while cpt-text is based on GPT-345 and its largest variant has 175B parameter. Both GTR and cpt-text are pre-trained by large-scale Web corpora with in-batch negatives, and are further fine-tuned with supervised datasets such as MS MARCO22. In comparison, BioCPT is only trained by the user click data from PubMed logs without using any supervised datasets.  Ethics This research was conducted in accordance with the Web Policy2 of the National Library of Medicine (NLM). PubMed user search logs are collected to “improve organization, coverage, system performance or other problem areas”. In our case, we use the user logs to develop the BioCPT model that is aimed to improve the search performance of PubMed. No personally identifiable information is used in this work.  Competing Interests The authors declare that there are no competing interests.  Author contribution Q.J. and Z.L. conceived and designed the study. Q.J. implemented and evaluated the model. W.K., L.Y., and D.C. collected and processed the user log data. Q.C. collected the  2 https://www.nlm.nih.gov/web_policies.html#privacy_security     
    evaluation datasets. Q.C., J.W., and Z.L. contributed to improving the model design. Z.L. provided overall supervision. All authors were involved in manuscript writing.  Data availability Due to the NLM policy, we are not able to publicly release the PubMed user logs. To facilitate its use, we make the source code of BioCPT publicly available at https://github.com/ncbi/BioCPT. In addition, we make the pre-trained models available through Web APIs, providing interested users access to cutting-edge biomedical text encoding capabilities.  Acknowledgment This research is supported by the NIH Intramural Research Program, National Library of Medicine.      
    