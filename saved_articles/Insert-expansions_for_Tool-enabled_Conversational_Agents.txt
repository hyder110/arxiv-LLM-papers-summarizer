INSERT -EXPANSIONS FOR TOOL-ENABLED CONVERSATIONAL
AGENTS
Andreas Göldi , Roman Rietsche
University of St.Gallen
andreas.goeldi@unisg.ch
ABSTRACT
This paper delves into an advanced implementation of Chain-of-Thought-Prompting in Large Lan-
guage Models, focusing on the use of tools (or "plug-ins") within the explicit reasoning paths
generated by this prompting method. We find that tool-enabled conversational agents often become
sidetracked, as additional context from tools like search engines or calculators diverts from original
user intents. To address this, we explore a concept wherein the user becomes the tool, providing
necessary details and refining their requests. Through Conversation Analysis, we characterize this
interaction as insert-expansion — an intermediary conversation designed to facilitate the preferred
response. We explore possibilities arising from this ’user-as-a-tool’ approach in two empirical studies
using direct comparison, and find benefits in the recommendation domain.
Keywords Large Language Model ·Augmented Language Model ·Conversational Agent ·Chatbot ·Conversation
Analysis
1 Introduction
Human language is a means both of communication and thought [ 1]. In recent years, it has become much more
feasible to process written language computationally. This has been achieved by mimicry of the human brain. In
so-called deep learning [ 2], multiple layers of artificial neurons are used to approximate functions [ 3], for example,
text to labels. Advanced deep learning architecture [ 4], a focus on generating text [ 5], and scaling up models [ 6] has
led to human-like performance on natural language tasks. Aligning language outputs with human expectations [ 7]
and chat capabilities make the current state-of-the-art competitive with human-like performance in tasks outside of
traditional natural language processing, e.g., in test-taking [ 8]. These capabilities have been appreciated by both users
communicating with models in their daily lives and researchers trying to apply and advance them [ 9]. Since human
expectations on speech have become prominent, the paramount mimicry has shifted from biological to social exemplars.
Recent developments indicate that yet another source of inspiration is about to be included, namely explicit human
cognition and tool-use.
These developments have been kick-started by prompting models to think through their assigned tasks step-by-step
[10]. Reducing complex into simpler tasks has been advocated at least since Cartesius [ 11]. By chaining simple tasks
together, i.e., sequentially inputting outputs from previous steps to models, language models can solve more complex
problems more reliably [ 12]. In this way, it is possible to call a language model multiple times until a final answer is
returned1. Humans can think using writing [ 13], and language models imitate reasoning by generating written text,
step-by-step. Since this paradigm allows intermediate steps, the idea has arisen to insert calls to tools, such as search
engines, calculators, or python functions [ 14]. Thereby, the language model is simulating the behavior of a computer
user, and can therefore incorporate information accessible via these tools into its final answer. In this way, tool-enabled
language models depart from simple function approximators, and become so-called augmented language models [ 15],
with the core human capabilities of reasoning and tool-use [16] as the exemplar to be imitated.
With social exemplars as a main source of inspiration, chat models have been trained to mimic human speech patterns
[8]. Now that thought is imitated, less effort needs to be allocated to approximate these surface patterns, since many of
1This is how langchain agents operate: https://python.langchain.com/docs/modules/agents/arXiv:2307.01644v1  [cs.HC]  4 Jul 2023Göldi and Rietsche
them are a result of human reasoning and planning during dialog [ 17]. This means that natural speech patterns may
result as a side-effect of more closely imitated reasoning paths.
The augmented language models that emulate reasoning are still meant to provide answers, and even impressive
reasoning paths will not lead to user satisfaction if they remain hidden, unresponsive, and long-winded. If an answer
cannot be given after one or few tool uses, augmented language models will produce intermediate observations that are
ever-more divergent from the initial query, since the next step is always prompted by, i.e., conditioned on, the output of
the previous steps. In this way, they tend to become side-tracked, and the primary aim, providing a satisfactory answer
to the user, may be lost.
However, humans sometimes think about what they say. They raise and fulfil or deny each others’ expectations; and
instead of silently thinking about a final answer, humans will oftentimes probe the interlocutor, by, e.g., checking their
understanding, scoping the final answer, or enhancing its appeal. This interactive nature has been extensively studied
and formally described using conversation analysis [ 17]. If an appropriate response cannot be given immediately,
human speakers tend to insert a new pair of utterances into the conversation, which is supposed to bridge the remaining
gap. For example, if someone wants to sell you a souvenir, you will insert a question ascertaining its price before
deciding on your final answer. This pattern often relies on explicit reasoning carried out in-between dialog utterances.
Augmented language models already talk to themselves and to tools. There have also been recent developments
which insert intermediate steps to directly ask users to provide context or check formatting for tool inputs2. This is a
potentially powerful mechanism if used in regular chatbot interaction, since for one, it may help to avoid side-tracking
in tool-enabled conversational agents because using dialog, common ground can be more easily established between
interlocutors, even if one of them is a chatbot [ 18]. Furthermore, it replicates exactly the discussed feature of human
talk-in-interaction, namely that of probing interlocutors to support fulfilling or reshaping the expectations they raised in
their initial main utterance.
In this paper, we will therefore discuss how insert expansions may be used in tool-enabled conversational agents, and
how their impact may be studied. For this, we present a paradigm of direct comparison, as well as data from one pilot
and two empirical studies based on it.
2 Background
Before delving into the study materials and data, it pays to assimilate a better understanding of the nature of insert
expansions, and to give a short overview of the nascent field of augmented language models.
2.1 Sequence Organisation of Dialog
Insert-expansions are one of several distinct building blocks of natural dialog or talk-in-interaction that can be grouped
not based on topicality but on what is being done with the utterances belonging to the different blocks [ 17]. Talk-in-
interaction, such as with a conversational agent, is successful if to every raised utterance, one of four responses is given:
The nominally preferred response, such as agreement to an invitation; the dispreferred response, often a rejection; a
temporizing response like "I may be there"; or a blocking response, such as "I have plans already" or a counter in the
sense of "What about you?". This base pair of utterances is generally known as "adjacency pair". In natural dialog,
such adjacency pairs are often extended by other pairs that are inserted before, between, or after the base pair, which
determines the main action of a sequence. Pre-extensions either generically draw attention or gauge interest in the
action performed with the specific intended base pair. If drawing attention fails or interest is low, the intended second
base pair part may never be uttered. Multiple exchanges can occur before the first part of a base pair. A special kind
of pre-extension is the pre-pre-extension, e.g. "Can I ask you something?" - "Yes?", which always precedes other
pre-extensions, such as "You now know something about dialogues, don’t you?" - "I guess I do". Normally, while
pre-extensions are used by the initiator, insert-expansions are used by the receiver, except in multi-turn inserts. They are
meant to get the conversation from the raised expectation to its fulfillment. So-called post-first insert expansions serve
to recover from misunderstandings and involve acknowledgement of the repair or a restatement of the first part of the
base pair. So here, we can check understanding, clarify intent, or gather information. Pre-second inserts, on the other
hand, ask for information required to choose between the four options for a second base pair part, for example the time
and day for an invitation. This may include scoping the response but also enhancing the appeal (or at least managing
expectations). After the second pair part has been uttered, a subdialog may continue with a follow-up. Very often, this
happens minimally with so-called sequence closing thirds, such as "Great". Others are more wordy, for example if
receivers tack on qualifications like "Just as friends, right?". Especially if the preferred response is not given, initiators
2https://python.langchain.com/docs/modules/agents/tools/how_to/human_approval , https://python.
langchain.com/docs/modules/agents/tools/integrations/human_tools
2Insert-expansions for Conversational Agents
may choose to challenge the response or rework the first base pair part to try again. Other options include appending
closings, such as "Bye", additional talk on the topic, a topic-shift, or the initiation of a new sequence. In natural dialog,
these new sequences are often of the same type by the same initiator, such as in question-series, reciprocating the main
action, or following a larger action plan for the dialog. This description of sequence organization based on Schegloff’s
work on Conversation Analysis [17] has empirical support and seems to be language-universal [19].
Properties of sequence organization largely generalize to text-based chats as well [ 20], even though conversations
may follow slightly different patterns, which may be investigated using digital conversation analysis [ 21]. However,
humanizing chatbots to allow for more natural interaction is a popular way of striving for less friction in human-chatbot
interaction, and bears major benefits [22]. One way of achieving this is to explicitly anthropomorphize features of the
interaction [23]. Aligning digital with natural dialog patterns fits nicely in this tradition.
Since in most applications, the users steer conversation, adjacency pair expansions of the second speaker are of
primary interest for augmented language models, especially if used as conversational agents. Insert-expansions are
such expansions, and potentially reduce friction in interaction as well as divergence during reasoning. They either
support the first base pair part of an adjacency pair, or aim to bring about the second. Instances relevant to text-based
conversations include clarifying intent, scoping responses, and enhancing appeal.
2.2 Augmented Language Models
Language models can generate textual inputs to arbitrary functions, which themselves may produce other text. This
output can then be used as context to generate the next step. If the tool is one of information retrieval, for example, it
can produce a final, grounded answer to a user query. This is how tool-augmentation of language models functions at
its most basic [14].
Unaugmented Large Language Models display so-called formal linguistic competence, i.e., they can handle language in
itself. Where they are still lacking is in functional linguistic competence, which means that they cannot do everything
humans do with language. This includes formal reasoning like logic or math, using world knowledge, situation modeling
in long narratives or discourses, and being able to use communicative intent as in pragmatics or establishing common
ground [ 24]. Popular early tools address these issues, and therefore include information retrieval from documents,
search engines, and code interpreters including calculators [ 15]. Smaller, more specialized or more easily updated
models can also be used as tools [ 25]. Considering the modular make-up of the human brain [ 26], this indicates that
tooling may not only provide a capability for imitating mental activities that are deliberative in humans, but one for
imitating functional brain architecture more generally, thus reaching back to a biological exemplar again.
The main advantage of augmented language models, however, is not that they are good imitators of actual biological,
social, and cognitive processes. They may be. But their usefulness extends to economically more interesting opportuni-
ties as well, namely to a deepening of automation. For example, augmented language models have been applied to
chemical tasks such as drug discovery [27].
Such opportunities incentivize the further development of augmented language models. Consequently, we can expect
them to become more common. This is reinforced by the fact that the setup of tools is already being automated
[28], as is the chaining of model calls [ 29]. As many tools are simply python functions or calls to well-documented
APIs, the code-generating capabilities of even unaugmented language models [ 8] will likely enable future augmented
language models to extend their capabilities by themselves. While these advances are spearheaded by closed-sourced
state-of-the-art models, augmentation is feasible with open-source models as well [ 30], even if some suggest better
performance by first generating instructions on how to use the tools with such models [ 31]. Such instructions or function
metadata may be embedded as well to make the process more efficient [32].
Besides automating the augmentation process, and tweaking the efficiency of augmented language models, much focus
is also put into keeping models running for longer, so that they can solve more complex tasks. More long-lasting runs
may be enabled by extending regular chain-of-thought prompting by plan-and-solve prompting, which can generate
more structured reasoning paths that otherwise would have needed to be hard-coded [ 33]. For these longer-running
calls, decoupling observations from reasoning may serve to lessen the impact of divergence from the user intent due
to misfitting observations [ 34]. However, if supervision by humans is feasible, insert expansion may add additional
benefits even here.
In addition to cognitive automation, tool-enabled language models may also serve to more closely align with social
exemplars. As we have seen in Section 2.1, humans think and talk in iterative fashion to one-another. This interspersing
of mutual information gathering and thinking is not idle, it serves to achieve communicative success [ 17]. Besides the
benefits of humanizing chatbots [ 22], imitating this iterative fashion by introducing insert-expansions to tool-enabled
conversational agents therefore is expected to have additional inherent value in terms of improved chat outcomes.
3Göldi and Rietsche
clarify_intent.description "useful if you do not understand what the appropriate type of response would be"
scope_response.description = "useful if you need more information on the human to tailor your answer to their needs"
enhance_appeal.description = "useful if you want to make your forthcoming response more appealing"
Table 1: Prompts used for insert expansion tools in Study 1; in Study 2, only scope_response was used.
This short overview should give an idea of why we believe it is important to study augmented language models and
tool-enabled conversational agents, and provide the necessary background to understand our empirical approach.
3 Methodology
There have been efforts to make benchmarks available for tool-augmented language models [ 35]. However, because
insert expansions require user input, they are not readily automated. Furthermore, as insert expansions help to shape
final responses, these responses will depend on the idiosyncratic user input and cannot be easily standardized. This is
why we decided to use human evaluations instead of benchmarking for this paper.
For this, we used fluent English speakers from prolific3. I our pilot, we recruited n=10 participants (age m=31.50,
SD=7.53; 60% female); in Study 1, n=71 (age m=31.45, SD=10.05; 27% female); and in Study 2 n=364(age m=32.22,
SD=12.83; 31% female, 1 other). Sample sizes were based on power estimations; 80% power for a medium effect size
in a one-sample, one-tailed t-test for Study 1, and 90% for a large effect for Study 2, which was an attempt at replication
with a more suitable scenario.
We will now briefly discuss the study artifact, namely a chat interface to two differently configured conversational
agents, then discuss how direct comparison of the two works using this artifact, and finally, we will discuss pilot, Study
1, and Study 2 in sequence.
3.1 Artifact
To explore whether insert expansions would have an impact on tool-enabled chatbot interaction, we created two
augmented language model conversational agents, one vanilla, and one with additional user-as-a-tool tools. We are
going to call them vanilla and enabled bot from now on.
The agents used the python library langchain to prompt the OpenAI model gpt-3.5-turbo-03015. We used existing tools;
in Study 1, the tools could query wikipedia and do simple mathematical operations; in Study 2, an embedding-based
PDF-reader was included.
The user-as-a-tool tools modified the human-as-a-tool tool6from the langchain library, and steered queries via websocket
to the user interface. To prompt insert expansions, we simply changed name and description according to some of the
purposes of insert expansions delineated in Section 2.1. See Figure 1 for the tool definitions.
In Figure 1a, the chat interface is shown before distinguishing between the two bots. Participants were instructed on
the specific scenario via the placeholder in the input field. This was to ensure that they had to transfer the scenario to
working memory before starting it. Having done so, they sent their initial query to both bots, which then appeared on
each side of the screen. This stage is depicted in Figure 1b. They were instructed to use the two bots and compare them
directly. After at least 3 bot messages per bot in Study 1 (2 in Study 2), participants could decide to finish the scenario
and evaluate the bots (See evaluation modal in Figure 1c).
3.2 Direct comparison
The aim of the studies reported herein was an initial comparison of tool-enabled conversational agents with and without
insert-expansion capacity. As there was no prior literature on the specific effect, we opted to assess the differences
directly, i.e., by making participants aware of both options and asking them to compare. As presenting the bots
sequentially would lead to a repeated conversation, we made them available concurrently. To restrain divergence of use,
we kept the scenarios short, with only a few turns. This approach allows for relatively small sample sizes and a simple
3prolific.com
4In Study 2, we excluded two additional participants, one because their ratings had no variance while contradicting their qualitative
feedback, indicating a misunderstanding in the evaluation; and the other because one of the conversational agents unaccountably
asked them for their name, which the participant reported to have put them off using it.
5https://platform.openai.com/docs/models/model-endpoint-compatibility
6https://python.langchain.com/docs/modules/agents/tools/integrations/human_tools
4Insert-expansions for Conversational Agents
Figure 1a: Before seeing the two separate chat interfaces,
participants sent one query to both chatbots.
Figure 1b: Example chat (scenario 1 is depicted). On the
left side, insert expansion is enabled, the right side is a
vanilla chat. This was held constant throughout the study.
Figure 1c: Bipolar rating scales after each scenario
(Study 1: 7 points; Study 2: 6 points).
study design that does not necessitate, e.g., balancing. However, any results are only of a correlational nature, as we did
not randomize. We see this as appropriate for this stage of research, which is about guiding an emerging phenomenon,
and not yet about establishing causal explanations.
For the comparison, we used bipolar rating scales, with the placement (left, right) of the two bots as anchors (See
Figure 1c). We deemed this appropriate as the placement constitutes a definite reference without biasing results by
naming the bots. We visually indicated the middle of the scale with a small vertical line; however, we did not label
intermediary steps so as to not jeopardize equidistance [ 36]. In Study 1, we allowed no-preference, in Study 2, we used
a forced-choice procedure by eliminating the midpoint. The former approach does not presume a preference, and may
thus be more appropriate as a measure of spontaneous preference; the latter forces participants to decide, and thus
evokes cognition about p