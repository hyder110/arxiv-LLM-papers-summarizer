Embodied Task Planning with Large Language Models
Zhenyu Wu1, Ziwei Wang2,3, Xiuwei Xu2,3, Jiwen Lu2,3, Haibin Yan1∗
1School of Automation, Beijing University of Posts and Telecommunications, China
2Department of Automation, Tsinghua University, China
3Beijing National Research Center for Information Science and Technology, China
{wuzhenyu, eyanhaibin }@bupt.edu.cn; yali110136@gmail.com;
xxw21@mails.tsinghua.edu.cn; lujiwen@tsinghua.edu.cn
https://gary3410.github.io/TaPA
Abstract: Equipping embodied agents with commonsense is important for robots to
successfully complete complex human instructions in general environments. Recent
large language models (LLM) can embed rich semantic knowledge for agents in
plan generation of complex tasks, while they lack the information about the realistic
world and usually yield infeasible action sequences. In this paper, we propose a
TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical
scene constraint, where the agent generates executable plans according to the
existed objects in the scene by aligning LLMs with the visual perception models.
Specifically, we first construct a multimodal dataset containing triplets of indoor
scenes, instructions and action plans, where we provide the designed prompts and
the list of existing objects in the scene for GPT-3.5 to generate a large number of
instructions and corresponding planned actions. The generated data is leveraged
for grounded plan tuning of pre-trained LLMs. During inference, we discover the
objects in the scene by extending open-vocabulary object detectors to multi-view
RGB images collected in different achievable locations. Experimental results show
that the generated plan from our TaPA framework can achieve higher success rate
than LLaV A and GPT-3.5 by a sizable margin, which indicates the practicality of
embodied task planning in general and complex environments.
Keywords: Embodied task planning, large language models, open-vocabulary
detection
1 Introduction
Equipping embodied agents with general commonsense knowledge to accomplish complex tasks
based on the natural language commands is desirable in many applications such as domestic service
[1], medical treatment [ 2,3,4] and agricultural picking [ 5,6]. Due to the limited training samples
and diverse tasks in downstream applications, directly training an embodied agent across different
deployment scenarios is infeasible. Recent progress in large language models (LLMs) [ 7,8,9,10]
acquires rich commonsense knowledge from the vast web data, whose knowledge can be potentially
leveraged by embodied agents to generate action plans for human requirements represented in natural
language.
However, LLMs cannot perceive the surrounding scenes and may generate inexecutable actions due
to the requirement of interacting with non-existed objects. For example, given the human command
”Give me some wine”, the generated action steps from GPT-3.5 are ”pouring wine from the bottle
to the glass”. There may be only mugs instead of glasses in the realistic scenes, and the executable
actions should be ”pouring wine from the bottle to the mug”. Therefore, grounding the task plan
generated by LLMs to the physical world is necessary to construct embodied agents for complex task
accomplishment.
∗Corresponding author.arXiv:2307.01848v1  [cs.CV]  4 Jul 2023Figure 1: Our embodied task planning framework collects multiple RGB images from various
standing points and viewpoints. Utilizing an open vocabulary detector generates a list of objects
existed in the scene. Combining human instructions and the predicted object list, our TaPA generates
executable action plans for navigation or manipulation tasks.
To acquire executable task plans in the given physical scenes, many previous works filter or align the
generated actions by considering the visual clues in the scene for the task of general manipulation of
tabletop objects [ 11,12,13]. In order to further diversify tasks in house-level environments, SayCan
[14] and LLM-Planner [ 15] employ visual navigation to collect information in the house for the
challenging grounded plan generation. Nevertheless, SayCan can only accomplish tasks in the kitchen
scenarios and LLM-Planner performs planning in the ALFRED simulator [ 16] where most tasks are
simple such as putting and placing. They both fail to satisfy the requirement of numerous complex
tasks and diverse deployment scenarios in our daily life.
In this paper, we present a task planning agent called TaPA for embodied task plan grounding in
physical scenes. The unreleased SayCan cannot be applied in diverse indoor scenarios, and the LLM-
Planner in the ALFRED benchmark fails to generate plans for complex tasks due to the pre-defined
simple instructions in the simulator. On the contrary, our agent can generate grounded plans without
constraining task types and target objects. Therefore, Our agent acquires general commonsense
knowledge to yield action steps for complex household tasks such as making sandwiches and setting
tables, which provides the foundational instructions for the downstream navigation and manipulation
process to deal with high-level requirements from humans. Figure 1 demonstrates the overall pipeline
of our TaPA that generates the executable action steps by considering the scene information and the
human instructions. Figure 2 shows the statistical difference between our TaPA and conventional
ALFRED benchmark, where our tasks are much more complex with longer steps for accomplishment.
More specifically, we first construct a multimodal dataset where each sample is a triplet of visual
scenes, instructions, and corresponding plans. By leveraging the generated dataset, we finetune
the pre-trained LLaMA [ 7] network by predicting the action steps based on the object list of the
scene, which is employed as our task planner. For the acquisition of the object list during inference,
the embodied agent effectively visits standing points to collect RGB images providing sufficient
information in different views, and generalizes the open-vocabulary detector for multi-view images
to acquire the list of existed objects. Our TaPA agent achieves higher success rate of the generated
action plans compared with the state-of-the-art LLMs including LLaMA and GPT-3.5 and large
multimodal models (LMMs) such as LLaV A [ 17]. Our contributions can be summarized as follows:
•To the best of our knowledge, we propose the first benchmark for complex embodied task planning
that is practical in realistic indoor deployment scenarios.
•We design a framework for large-scale multimodal dataset generation in order to train the task
planner from pre-trained LLMs and construct a multimodal dataset containing 80 indoor scenes
with 15K instructions and corresponding action plans.
•We evaluate different LLMs and LMMs for complex embodied task planning in our benchmark,
and conduct the ablation study to select the optimal representation of visual scenes for executable
action generation.
2Figure 2: Statistical comparison of TaPA and ALFRED dataset. The pie chart shows the top 20
frequently appearing verbs (inner circle) and the corresponding top 4 nouns (outer circle) for each
verb. The bar chart shows the percentage of instructions with different numbers of implementation
actions, where TaPA contains more complex instructions compared to ALFRED.
2 Related Work
Large pre-trained models: Large-scale pre-trained models have revolutionized the natural language
processing (NLP) [ 18,19,20] and the computer vision [ 21,22,23] communities in recent years.
Benefiting from the vast training data and numerous parameters, the large pre-trained models acquire
strong generalization ability across different deployment scenarios. For large language models, recent
studies show that they not only perform well in NLP tasks, but also emerge the ability to master the
rich knowledge about the realistic world with factual answers. Therefore, LLMs such as LLaMA [ 7],
GPT-3 [ 24] are widely adopted to diverse tasks by interacting with input from other modalities such
as visual feature learning [ 25,26], pseudo-code generation [ 27], tool usage [ 28] and math problem
solving [ 29]. For large vision models, objects in the open environments can be detected [ 23,30]
or segmented [ 31] for scene understanding, where bounding boxes and masks are generated for all
scenarios and visual features are aligned with text embedding for category assignment. To learn the
joint embedding space of language and vision for multimodal tasks, CLIP [ 32] leverages contrastive
learning to minimize the distance between similar image-text pairs. LLaV A [ 17] synthesized a
multimodal dataset with images, captions and bounding boxes in the tasks of conversation, detailed
description and complex reasoning, so that the instructional tuning of LLMs acquires general-purpose
instruction-following visual agent. In this paper, we leverage LLMs to generate executable plans for
embodied tasks with the visual information acquired from the open-vocabulary detection models.
Language model grounding for embodied tasks: An embodied agent not only requires active
exploration [ 33], manipulation [ 34], and scene perception [ 35,36] as well as embodied task planning
ability. Embodied task planning aims to generate executable action steps in the given environments,
where action plans are generated from grounded LLMs by receiving information from the surrounding
environments [ 37,38,39] or prompt engineering [ 40]. For the former, agents acquire the feedback
from environments by interacting with the objects to ground the task plan. Li et al. [41] employed
LLMs as a general scaffold for interactive decision-making in complex tasks, where the generated
policies were grounded to the given environments for executable implementation according to the
action feedback. For prompt engineering, researchers carefully design the language prompts for
LLMs to guide them to ground the generated content. Huang et al. [40] prompted simple examples
of task instructions and corresponding actions for LLMs to produce plausible task plans, and filtered
out executable subsets by constructing mapping with semantic similarity. To enable the LLMs to be
aware of the surrounding scenes with boosted plan plausibility, Brohan et al. [14] and Song et al. [15]
extracted the visual information of the scene by latent features or object names for LLMs, where the
generated plans were limited to the one with the highest success rate for task completion. However,
these works can only accomplish simple tasks such as placing and putting in the VirtualHome [ 42] or
ALFRED simulators, which fail to be applied to practical deployment scenarios with diverse complex
tasks.
33 Approach
In this section, we first describe the construction of the multimodal instruction dataset that is leveraged
to tune our TaPA task planner, and then describe the details of grounding embodied task plans to the
visual scene with image collection and open-vocabulary detection.
3.1 Data Generation of Embodied Task Planning
Although large vision-language models (VLM) [ 17,43] and large multimodal models [ 44,45,46,47,
48] have achieved surprising performance on a wide range of complex perception tasks, embodied
task planning that is grounded to the realistic indoor scenes still remains challenging due to the lack
of the large-scale multimodal dataset to train the planning agent. Considering the recent success
of GPT models on high-level human-like reasoning, we leverage GPT-3.5 with the presented scene
representation and designed prompt to generate the large-scale multimodal dataset for our planning
agent tuning.
Figure 3: The pipeline of embedding the scene
information for the LLM to generate executable
actions. The image collection strategies to ac-
quire the list of all existed objects in the scene
include random sampling, traversal sampling,
the overall center point and block-wise cen-
ter points, where the object list is leveraged as
the condition for action planning. The dashed
circles in different colors represent grids in var-
ious blocks for block-wise center point selec-
tion.Given an embodied 3D scene Xs, we directly
utilize the class names of all objects as the rep-
resentation of the scene which is denoted as
Xl. All duplicate names are removed to provide
scene information for the LLM such as Xl=
[table, chair, keyboard, ... ]. Based on the above
scene information, a simple approach used in AL-
FRED benchmark [ 16] to generate the multimodal
instruction following the dataset for embodied task
plans is to artificially design a series of instructions
with corresponding step-by-step actions. However,
the hand-crafted design requires extremely high an-
notation cost to generate complex task plans that
are practical for realistic service robots such as tidy-
ing up the bathroom and making sandwiches. To
efficiently generate the large-scale complex instruc-
tions Xqand executable corresponding plans Xa
for the given 3D scene, we design a prompt to sim-
ulate the scenarios of embodied task planning for
GPT-3.5 to automatically synthesize data based on
the object name list Xl. As shown in Table 5 of the
supplementary materials, our prompt describes the
definition of embodied task planning, the require-
ments and several examples of generated instructions and corresponding action plans. Specifically,
the prompt designs a conversation between the service robot and humans to generate executable
instructions and actions, which simulates the exploration of robots in the embodied environments and
provides the requirements from humans. The generated instructions are diverse including requests,
commands and queries, where only instructions with explicitly executable actions are added to our
dataset. Meanwhile, we emphasize that the target object of the generated action should be constrained
within the object list Xlto mitigate the object hallucination that leads to inexecutable plans. For the
object list leveraged in the prompt for dataset generation, we directly utilize the groundtruth label
of existed instances in the scene. In Table 1, we show examples of the generated sample containing
the object name list of the scene, the instruction and the executable action steps. In embodied task
planning, the agent can only get access to the visual scene containing all interactive objects without
the groundtruth object list. Therefore, we construct the multimodal dataset by defining triplets for
each sample as X= (Xv, Xq, Xa). For the training stage of the task planner, we directly leverage
the groundtruth object list for each scene to avoid the influence of inaccurate visual perception. For
the inference phase, the extended open-vocabulary object detector predicts the list of all existed
objects in the scene.
4Instruction: Instruction:
Can you clean the sink and the toilet, please? Can you make me a sandwich?
Task planning: Task planning:
Step 1: Grasp a sponge Step 1. Grasp a plate
Step 2: Move to the sink Step 2. Grasp the knife
Step 3: Wet the sponge Step 3. Grasp a piece of bread
Step 4: Scrub the sink Step 4. Move the knife to the bread and slice it
Step 5: Rinse the sponge Step 5. Grasp another piece of bread
Step 6: Grasp a towel Step 6. Move the knife to the bread and slice it
Step 7: Dry the sink Step 7. Grasp a lettuce
Step 8: Move to the toilet Step 8. Tear the lettuce and place it on the plate
Step 9. Grasp a scrub brush Step 9. Grasp a tomato
Step 10. Scrub the toilet bowl Step 10. Slice the tomato and place it on the plate
Step 11. Place the scrub brush back in its place Step 11. Move the two slices of bread to the plate
Table 1: Examples of the generated multimodal triplet data including visual scenes, instructions and
the corresponding plans.
We employ the AI2-THOR simulator [ 49] as the embodied environment for our agent, where we split
the scenes with 80 for training and 20 for evaluation. To enlarge the scale and diversity of instructions
and action steps in training samples for effective task planner finetuning, we expand the original
80 training scenes to 6400 training scenes by directly modifying the groundtruth object list. For
each scene type, we initially acquire the list of objects that possibly appear in this type of scene by
enumerating all rooms in the same room type. Then we randomly substitute existed objects with
other ones that possibly exist in the same room type and are not observed. The plausibility constraint
aims to prevent generating counterintuitive objects for given scene types. We collected 15K samples
for training and leverages another 60 triplets for evaluation with our multimodal data generation
framework.
3.2 Grounding Task Plans to Surrounding Scenes
In order to ground the embodied task plan to the physical world with feasibility constraints, it is
necessary to accurately obtain the object list in the scene without instance missing or false positives.
We generalize the open-vocabulary object detector for object list acquisition since novel objects
unseen in detector training may appear in the deployment scenarios. As shown in Figure 1, the
agent collects RGB images in different locations to perceive the visual scenes to discover existed
objects. We design several image collection strategies to explore the surrounding 3D scenes. The
location selection criteria contains traversal positions, random positions, the overall center point
and block-wise center points, and the agent rotates the camera to obtain multi-view images for each
5location selection criteria. Therefore, we formally write the image collection strategies Sin the
following:
S={(x, y, θ )|(x, y)∈L(λ,A), θ=kθ0} (1)
where (x, y, θ )represents the location and camera orientation. L(λ,A)means the location selection
criteria with the hyperparameter λand all sampled locations are required within the achievable area
A. The unit angle for camera rotation is set to θ0, andkis an integer so that the agent collects visual
clues in different directions of the scene. The hyperparameter that all location selection criteria
share is the grid side length, where we divide the achievable area into grids. Traversal positions
choose all grid points for RGB image collection. Random positions only randomly selected part of
the grid points for visual information perception, and the hyperparameters also contain the ratio of
sampled grid points. The overall center point stands for the center of the whole scene without any
hyperparameters. The block-wise center points aim to choose the center of each division in the scene
to efficiently acquire fine-grained visual information. Inspired by [ 50,51], clustering methods can
effectively divide the entire scene into several sub-regions to improve the performance of perception,
so that the prior information of the room layout is embedded into the image collection strategy with
the K-means clustering method. Meanwhile, we employ within cluster sum of squared errors (WCSS)
principle to select the optimal number of clusters for each scene. Compared to the images collection
strategy of traversal points, the block-wise center point only traverses centroids of the subregions to
acquire sufficient visual information.
The embodied task planner requires the information of all existed objects in the scene to generate
executable action steps, where we generalize the open-vocabulary object detector to the collected
multi-view RGB images for the object list acquisition. The predicted object list ˆXlfor the scene is
acquired by removing the duplicated object names in the detection results of multi-view images:
ˆXl= Rd [
iD(Ii)
(2)
where Rdis the operation of removing duplicate object names and D(Ii)represent the detected
object names for the ithRGB image collected in the scene. With our inference prompt Pinshown in
Table 5 of the supplementary material, the human instruction Xqand the predicted object list Xlare
considered in our TaPA to generate the executable action plans Xa:
Xa= TaPA( Pin,ˆXl, Xq) (3)
By combining the perception results of existed objects ˆXlwith the instructions Xq, TaPA will give the
executable action sequence Xato complete the requirements of Xqaccording to the realistic scene
constraint. According to our empirical study, we chose the block-wise center point for multi-view
RGB image collection. The grid size in our location selection criteria is set to 0.75 and the unit angle
for camera rotation is 2π/3.
4 Experiment
In this section, we conduct extensive experiments with our generated multimodal dataset where the
visual scenes come from the simulator AI2-THOR. We first introduce the evaluation metric of the
generated action plans. Then we compare our TaPA with the state-of-the-art LLMs and LMMs to
show our superiority in embodied task planning. To further explore the effectiveness of different
scene information embedding approaches, we evaluate various image collection strategies in our
ablation study. We employ the LLaMA-7B pre-trained language model as the backbone of our task
planner, which is finetuned with our generated multimodal dataset. The maximum token number of
our task planner is set to 512, and we leverage the Detic open-vocabulary object detection framework
to collect the information of existed objects. All experiments were accelerated by 8 GTX 3090 GPUs.
4.1 Evaluation Metrics
For the deployment of our TaPA, we feed the instructions and the predicted object list in the scene for
the task planner to generate the action steps. We hired 30 researchers in large multimodal models
6Table 2: Comparison of different LLMs and LMMs on the task of embodied task planning. For the
prompt of baseline methods, LLaMA and LLaV A both employ the same prompt in the their original
finetuning phase, while GPT-3.5 adopts the same prompt of TaPA for multimodal data generation.
Method Kit. Living. Bed. Bath. Avg.
LLaV A 14.29 42.11 33.33 0.00 22.43
GPT-3.5 28.57 73.68 66.67 50.00 54.73
LLaMA 0.00 10.52 13.33 0.00 5.96
TaPA 28.57 84.21 73.33 58.33 61.11
Table 3: The average execution success rate of generated action steps for different RGB image
collection strategies in scene perception. Grepresents the side length of grids in location selection.
∆θrepresents the unit angle of camera rotation. Nrepresents the ratio of randomly selected points
compared to all grid points in achievable area.
Strategy and Parameters #Images Kit. Living. Bed. Bath. Avg.
TraversalG=0.25, D=60 782.4 14.29 73.68 46.67 33.33 41.99
G=0.25, D=120 391.2 14.29 73.68 53.33 50.00 47.83
G=0.75, D=60 80.7 28.57 73.68 46.67 33.33 45.56
G=0.75, D=120 40.4 14.29 63.16 60.00 41.67 44.78
Random
(G=0.75)N=1%, D=60 6.0 28.57 78.95 26.67 50.00 46.05
N=1%, D=120 3.0 21.43 73.68 46.67 50.00 47.95
N=75%, D=60 63.0 35.71 73.68 53.33 25.00 46.93
N=75%, D=120 31.5 28.57 73.68 53.33 33.33 47.23
Layout Priori
(G=0.75,D=60)Overall Center 6.0 28.57 68.42 33.33 58.33 47.16
Partial Center 23.1 28.57 84.21 73.33 58.33 61.11
as volunteers to vote for the success of the generated action plans, and each generated action plan
is evaluated by three volunteers. The volunteers are shown with the groundtruth object list of each
scene, the instruction and the generated action plans, where the volunteers should judge whether
implementing the action steps can successfully completes the instructions. There are two types failure
cases including counterfactuals and hallucination. Counterfactuals indicate that the plans violate
the physical rules in the real world (e.g. grasping the doorknob before moving to the door), and
hallucination means the action plans require the agent to interact with objects that do not exist in the
scene. An exceptional case is that the interacting objects can be part of the object existed in the scene
(e.g. trash can lid and trash can) or a synonym of an object (e.g. mug and cup). The generated action
plans are considered to be successful if at least two volunteers out of three regarding the steps can be
implemented to satisfy the human instruction. The volunteers were also requested to annotate the
type of failure for the unsuccessful cases. We report the ratio of successful cases for different scene
types and plan generation models.
4.2 Experimental Results
In this section, we compare our TaPA method with the state-of-the-art LLMs including LLaMA
and GPT-3.5 and LMMs containing LLaMA on 60 validation samples, and the success rate of
the generated action steps from different methods are shown in Table 2. TaPA achieves optimal
performance among all large models on all four scenes including kitchen, living room, bedroom and
bathroom, and the average success rate of TaPA is 6.38% (61.11% vs. 54.73%) higher than GPT-3.5
on the task of embodied task planning after instruction finetuning.
Since agents in kitchen scenes usually deal with complex cooking instructions in more steps, the
performance of current large models is lower than in other room types. Meanwhile, the poor
performance of LLaV A reflects the fact that the overall scene information cannot be represented by a
single image in the visual question answering task, and the insufficient scene information leads to a
7Qualitative results, Living room:
User I want to watch TV User I want to watch TV
TaPA Step 1. Move closer to the television set
Step 2. Grasp the TV remote control
Step 3. Turn on the TV
Step 4. Adjust the volume to a suitable level
Step 5. Select a TV show or movie
Step 6. Watch TVGPT-3.5 Step 1. Move closer to the TV
Step 2. Grasp the remote control
Step 3. Rotate the remote control to point at the TV
Step 4. Press the power button to turn on the remote control
User I want to watch TV
LLaV A 1.Locate the TV remote control, which is typically placed on a table or mounted on the wall near the TV .
2.Point the remote control at the TV and press the power button. The power button is often located on
the top or bottom of the remote control.
3.If the TV doesn’t power on immediately, press the channel or input button to select the correct input source.
The input button is usually located on the right side of the remote control or may be a menu button that allows
you to access the settings.
4.The TV should now turn on and display the channel or content you have selected.
User I want to watch TV
LLaMA Sit in a comfortable chair, place a pillow on the floor, and turn on the TV .
Open up the drawer to get a remote control and place the TV on a table. Adjust the volume on the speakers.
Locate the plug for the television and plug it in. Place the bowl of objects on the table next to the TV .
(Objects: chair, sofa, pen, figurine, table, dog, lampshade, doorknob, bed, toy, drawer, person, statue (sculpture),
flowerpot, stool, monitor, (computer equipment), speaker, computer monitor, . . . . . . )
Table 4: Visualization of generated action plans by different large models. The inputs of LLaMA and
GPT-3.5 are the predicted object list by our visual perception module, while LLaV A generates the
action steps only based on one single image.
low success rate of task planning. The success rate of LLaMA is far below other methods, which
even cannot succeed in completing tasks in the kitchen and bathroom scenes without instruction
finetuning. Figure 2 illustrates the percentage of failure cases in embodied task planning for different
large models. Counterfactuals represent that the generated actions violet the physical rule in the
realistic world, and hallucinations mean the actions aim to interact with objects that are not in the
scene. TaPA is embedded with more expert knowledge in embodied task planning after instruction
finetuning, which has the lowest percentage of counterfactual occurrences. Moreover, TaPA can better
understand the list of input objects, with a 26.7% (40.0% vs. 13.3%) and 5.0% (18.3% vs. 13.3%)
decrease in the percentage of hallucination cases compared to LLaV A and GPT-3.5 respectively.
We also investigate the effectiveness of different image collection strategies that perceive the scene
information by acquiring the list of existed objects. Specifically, we employ location selection criteria
including random positions, traversal positions, the overall center point and block-wise center points
with various hyperparameters containing the grid size and the sampling ratio in random positions,
and we also change the unit angle for camera rotation. The success rate of different image collection
strategies is demonstrated in Table 3. We also show the number of collected images for various
criteria to reveal the collection and computational cost. For the traversal positions, reducing the grid
size significantly increases the image collection and the computational cost due to the numerous RGB
images, while the average success rate remains similar (47.83 vs. 44.78) because the large grid size
can collect images with sufficient information of the small-scale scenes from AI2-THOR. Similar
reasons result in the phenomenon for random positions that increasing the sampling ratio and reducing
8the unit angle for camera rotation by collecting images in more locations cannot boost the success
rate (47.95 vs. 47.23, 46.93 vs. 47.23). Since the traversal positions with small grid sizes (G=0.25)
collects extremely large number of images, decreasing the unit angle for camera rotation significantly
decreases the success rate because the redundant object list degrades the planning capacity of LLMs.
Figure 4: The percentage of different failure
cases in embodied task planning for various
large models.Comparing all location selection criteria, block-
wise center points achieve the highest success rate
because of the effective representation of the ex-
isted objects in the scene. Block-wise center points
observe the scene with the high coverage rate, while
only a few RGB images are collected for scene rep-
resentation. Therefore, sufficient scene information
is captured by the acquired object list without re-
dundancy. The performance of random positions
and the overall center point is similar because the
scale of scenes in AI2-THOR is small and one im-
age collection location can also receive sufficient
information. The traversal positions obtain the low-
est success rate since collecting excess images lead
to the higher probability of false positives in open-
vocabulary object detection, which degrades the
success rate because of the redundant object list.
Among all room types, the success rate in the
kitchen scenes is the lowest since the instruction
for kitchen tasks (e.g. sandwich making) usually requires long plans with much more action steps.
With the increase of the interacted objects in the task plan, the probability of hallucination is higher so
that the plans are more likely to fail. On the contrary, the success rate of tasks in the living rooms is
high due to the simple instructions (e.g. turning off lights). By observing the success rate of kitchen
tasks across different location selection criteria, false positives in object detection that usually appear
in traversal location selection criteria degrade the performance most significantly. Since the object
list is redundant, the complex tasks in kitchen scenarios are more prone to the noise in the object list.
We also show an example of generated action steps from different large models for the given scene in
Table 4. The scene is demonstrated in the top-down view, and we also provide the groundtruth object
list for reference. The content from LLaMA is irrelevant to the human instructions, while LLaV A
provides plans that are not executable due to the non-existed objects. Although GPT-3.5 can also
yield plausible embodied task plans, the action steps from our TaPA are more complete and more
consistent with human values.
5 Conclusion
In this paper, we have presented a task planning agent called TaPA for embodied task planning,
where the executable action steps are generated for subsequent robot navigation and manipulation to
complete human instructions. We first construct a multimodal dataset where each sample is a triplet
including the visual scenes, the instructions and the corresponding plans. The dataset is generated
with GPT-3.5 by considering the list of all objects in the scene and the designed text prompt, which
is leveraged to tune the instruction model to generate executable actions. For inference, we collect
multi-view RGB images in different achievable locations, and leverage an open-vocabulary object
detection framework to discover the object list of the scene for the finetuned instruction model. The
statistics of our collected multimodal dataset indicate that our tasks are much more complex than
conventional benchmarks on instruction-following tasks with longer implementation steps, and the
extensive evaluation results show that our TaPA outperforms the state-of-the-art LLMs and LMMs on
the plausibility of generated action plans.
9